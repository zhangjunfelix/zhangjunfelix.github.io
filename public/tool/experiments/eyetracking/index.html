<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Eyetracking | Jun Zhang</title>
<meta name="keywords" content="">
<meta name="description" content="Learning About Eye-Tracking Technique: creation, design, data analysis

Learning about the eye-tracking technique is motivated by my interest in discovering cognitive processes involved in interpreting scalar quantifiers in social contexts.
My first experiments (hopefully) are about perspective-taking and interpreting scalars in social contexts, a project emerging from my discussions with Prof. Gerry Altmann.
In this post, I share my experience in learning about this technique in five areas:

Reviews
Classical studies
tools to create VWP experiments
Experimental design
Data analysis


Part I: Reviews
video tutorials:">
<meta name="author" content="">
<link rel="canonical" href="https://zhangjunfelix.github.io/tool/experiments/eyetracking/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk&#43;4bvpN&#43;sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://zhangjunfelix.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://zhangjunfelix.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://zhangjunfelix.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://zhangjunfelix.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://zhangjunfelix.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://zhangjunfelix.github.io/tool/experiments/eyetracking/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://zhangjunfelix.github.io/tool/experiments/eyetracking/">
  <meta property="og:site_name" content="Jun Zhang">
  <meta property="og:title" content="Eyetracking">
  <meta property="og:description" content="Learning About Eye-Tracking Technique: creation, design, data analysis Learning about the eye-tracking technique is motivated by my interest in discovering cognitive processes involved in interpreting scalar quantifiers in social contexts.
My first experiments (hopefully) are about perspective-taking and interpreting scalars in social contexts, a project emerging from my discussions with Prof. Gerry Altmann.
In this post, I share my experience in learning about this technique in five areas:
Reviews Classical studies tools to create VWP experiments Experimental design Data analysis Part I: Reviews video tutorials:">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="tool">
    <meta property="article:published_time" content="2023-06-17T00:00:00+00:00">
    <meta property="article:modified_time" content="2023-06-17T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Eyetracking">
<meta name="twitter:description" content="Learning About Eye-Tracking Technique: creation, design, data analysis

Learning about the eye-tracking technique is motivated by my interest in discovering cognitive processes involved in interpreting scalar quantifiers in social contexts.
My first experiments (hopefully) are about perspective-taking and interpreting scalars in social contexts, a project emerging from my discussions with Prof. Gerry Altmann.
In this post, I share my experience in learning about this technique in five areas:

Reviews
Classical studies
tools to create VWP experiments
Experimental design
Data analysis


Part I: Reviews
video tutorials:">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Toolbox",
      "item": "https://zhangjunfelix.github.io/tool/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Psycholinguistics Experiments",
      "item": "https://zhangjunfelix.github.io/tool/experiments/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Eyetracking",
      "item": "https://zhangjunfelix.github.io/tool/experiments/eyetracking/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Eyetracking",
  "name": "Eyetracking",
  "description": "Learning About Eye-Tracking Technique: creation, design, data analysis Learning about the eye-tracking technique is motivated by my interest in discovering cognitive processes involved in interpreting scalar quantifiers in social contexts.\nMy first experiments (hopefully) are about perspective-taking and interpreting scalars in social contexts, a project emerging from my discussions with Prof. Gerry Altmann.\nIn this post, I share my experience in learning about this technique in five areas:\nReviews Classical studies tools to create VWP experiments Experimental design Data analysis Part I: Reviews video tutorials:\n",
  "keywords": [
    
  ],
  "articleBody": "Learning About Eye-Tracking Technique: creation, design, data analysis Learning about the eye-tracking technique is motivated by my interest in discovering cognitive processes involved in interpreting scalar quantifiers in social contexts.\nMy first experiments (hopefully) are about perspective-taking and interpreting scalars in social contexts, a project emerging from my discussions with Prof. Gerry Altmann.\nIn this post, I share my experience in learning about this technique in five areas:\nReviews Classical studies tools to create VWP experiments Experimental design Data analysis Part I: Reviews video tutorials:\nZhan L. Using Eye Movements Recorded in the Visual World Paradigm to Explore the Online Processing of Spoken Language. J Vis Exp. 2018 Oct 13;(140):58086. doi: 10.3791/58086. PMID: 30371678; PMCID: PMC6235534. link to video\nTutorials\nIntroduction to Eye Tracking: A Hands-On Tutorial for Students and Practitioners Link Part II: Classical Studies The following is a list of classical VWP studies:\nPart III: tools to create VWP experiments Lab-Based Eye-Tracking SR EyeLink\nExperiment Builder Tutorial videos\nOpensource tools jsPsych, pychoPy, OpenSesame\njsPsych Tutorial jsPsych Official Tutorial Online Book by Winson Yang: jsPsychTutorial YouTube Tutorial Playlist PyGaze (Eye Tracking) on OpenSesame PyGaze in OpenSesame Documentation PyGaze Documentation Eyetracking Data Analyses in R eyetrackingR Package Tutorial by Marissa Barlaz: Portfolio Example YouTube Tutorial Online Webcam-Based Eye-Tracking Here are some of the resources I use to learn about designing online eye-tracking experiments:\nWebGazer WebGazer Official Site WebGazer on GitHub Tutorial by BROWNHCI: WebGazer Tutorial WebGazer Tutorial on jsPsych Tutorial by Xiaozhi Zhao: GitHub Repository WebGazer on OpenSesame WebGazer in OpenSesame Documentation Part IV Experimental Design There are several ways to build an eye-tracking experiment.\nExperiment Builder: Provided with the EyeLink machine. Open-source tools: Tools like OpenSesame and PsychoPy. OpenSesame OpenSesame (MathÃ´t et al., 2012), used together with the PyGaze plugin (Dalmaijer et al., 2014).\nResources for using PyGaze on OpenSesame: OpenSesame and PyGaze Workshop in Potsdam PyGaze (Eye Tracking) - OpenSesame Documentation Visual World Experiment with PyGaze and OpenSesame PsychoPy PsychoPy is another open-source tool that supports creating eye-tracking experiments. These experiments are compatible with EyeLink systems.\nResources for PsychoPy: Communicating with an Eye Tracker YouTube Tutorial: PsychoPy Integration for EyeLink Eye Trackers Gorilla Gorilla is also an option for creating eye-tracking experiments.\nResources for Gorilla: Eye Tracking in Task Builder 1 Eye Tracking in Task Builder 2 Note: While Gorilla is a great tool, I chose not to use it because there is a fee for recruiting participants.\nMy Path Since I am proficient with OpenSesame (Iâ€™ve used it for creating SPR tasks and mouse-tracking tasks), I designed a Visual World (VW) task using PyGaze on OpenSesame. Part V: Data Analysis Ito, A., Knoeferle, P. Analysing data from the psycholinguistic visual-world paradigm: Comparison of different analysis methods. Behav Res 55, 3461â€“3493 (2023). https://doi.org/10.3758/s13428-022-01969-3 Link to file two datasets are from the following two studies and are available at this OSF repository\nIto et al. (2018b); Knoeferle and Crocker (2006; Experiment 1):\nStep 1 Data preparation how to treat (very short and very long) fixations and blinks\ntools:\nSR Research Data Viewer;\nR packages: gazeR (Geller et al., 2020), eyetrackingR (Dink \u0026 Ferguson, 2015), VWPre (Porretta et al., 2020)\nSR Research Data Viewer tutorial videos\npreprocessind data Data Viewer Video Tutorial: 07 - Filtering Cleaning and Adjusting Data\nThe data analysis focuses on experiments designed with PyGaze. References Dalmaijer, E., MathÃ´t, S., \u0026 Van der Stigchel, S. (2014). PyGaze: An open-source, cross-platform toolbox for minimal-effort programming of eye-tracking experiments. Behavior Research Methods. DOI MathÃ´t, S., Schreij, D., \u0026 Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. Behavior Research Methods, 44(2), 314â€“324. DOI Peirce, J. W., Gray, J. R., Simpson, S., MacAskill, M. R., HÃ¶chenberger, R., Sogo, H., Kastman, E., LindelÃ¸v, J. (2019). PsychoPy2: Experiments in behavior made easy. Behavior Research Methods. DOI Part VI How I Create an Eyetracking experiment from beginning EB user manual 2024 version\nThe following tutorial are generated based on this user manual.\ncan you format the following content by using hugo markdown syntax?\nCreating EyeLink Experiments: A Beginnerâ€™s Tutorial In this tutorial, weâ€™ll guide you through creating a simple EyeLink experiment using SR Research Experiment Builder. This experiment will display a single word in the center of the screen in each trial, similar to the â€œSIMPLEâ€ template of the EyeLink C Programming API.\n1. Prerequisites Before you start, make sure you have the SR Research Experiment Builder software installed. The examples in this tutorial assume a screen resolution of 1920 x 1080 at 100% display scale.\n2. Creating the Experiment 2.1 Creating a New Experiment Project Open Experiment Builder: On Windows, click Start \u003e All Applications \u003e SR Research and choose â€œExperiment Builderâ€. On macOS, go to the â€œApplications/Experiment Builder/â€ folder and open the â€œExperimentBuilderâ€ application. Create a new project: Click â€œFile \u003e Newâ€ on the application menu bar. In the â€œNew Projectâ€ dialog box: Enter â€œSimpleâ€ in the â€œProject Nameâ€ edit box. Click the button on the right end of the â€œProject Locationâ€ to browse to the directory where you want to save the experiment project. Make sure the directory exists if you enter it manually. Leave the â€œTemplatesâ€ field as â€œNoneâ€. Check the â€œEyeLink Experimentâ€ box. Select the eye tracker version from the dropdown menu (e.g., EyeLink 1000 Plus) or select â€œCurrentâ€ to allow any EyeLink eye tracker. 2.2 Configuring Experiment Preference Settings Access preferences: On Windows, select â€œEdit \u003e Preferencesâ€ or press â€œF4â€. On macOS, click â€œExperimentBuilder \u003e Preferencesâ€ or press Command âŒ˜+â€œ,â€. Check display settings: Click â€œPreferences \u003e Experiment \u003e Devices \u003e Displayâ€. Ensure the settings (Width, Height, Bits per Pixel, and Refresh Rate) are supported by your video card and monitor. The default values 1920 Ã— 1080 Ã— 32 Ã— 60 Hz are used in this example. Configure screen settings: Click â€œPreferences \u003e Screenâ€. Set the Location Type as â€œCenter Positionâ€ and check the â€œAntialis Drawingâ€ box. Set tracker version: Ensure the â€œTracker Versionâ€ setting in the â€œPreferences -\u003e Experiment -\u003e Devices -\u003e EyeLinkâ€ preferences is set correctly for your eye tracker. If you use EyeLink 1000, 1000 Plus, or Portable Duo, configure the correct â€œCamera Mountâ€ and â€œMouse Usage setting. Set file encoding: If you plan to use non - ASCII characters, check the â€œEncode Files as UTF - 8â€ box in the Build/Deploy node. 2.3 The Topmost Experiment Layer Add nodes to the workspace: Open the â€œActionâ€ Tab of the component toolbox and drag a â€œDisplay Screenâ€ action into the work area. Open the â€œTriggerâ€ Tab and drag a â€œKeyboardâ€ trigger, an â€œEyeLink Buttonâ€ trigger, and a â€œTimerâ€ trigger into the work space. Set the Timer trigger duration to 120000 msec. Open the â€œActionâ€ Tab again and add a â€œCamera Setupâ€ action and a â€œSequenceâ€ node to the work space. Connect the nodes: Draw connections from the START node to the DISPLAY_SCREEN node. Connect the DISPLAY_SCREEN action to the KEYBOARD, EL_BUTTON, and TIMER triggers. Connect each of the three triggers to the EL_CAMERA_SETUP node, then from EL_CAMERA_SETUP to the SEQUENCE node. Right - click any blank area in the work window and select â€œArrange Layoutâ€ to organize the nodes. 2.4 Creating the Instructions Screen Open the Screen Builder: Double - click the DISPLAY_SCREEN node in the workspace. Add a multi - line text resource: Click the multiline text resource button on the screen builder toolbar and click anywhere on the screen to add the resource. Edit the text: Set the text margins to 100 in all fields. Enter the instruction text, such as instructions for camera setup, calibration, and starting recordings. Select all text (Ctrl + A on Windows or Command âŒ˜ + A on macOS) and set the text appearance (font name, size, style, alignment, line spacing, and color). Click the â€œCloseâ€ button to finish. 2.5 Editing the Trial Sequence: Data Source Select the trial sequence node: Select the last â€œSEQUENCEâ€ node in the structure list and enter a new value for the Label, e.g., â€œTRIALâ€. Create the data source: Click the value cell of the â€œData Sourceâ€ property to bring up the Data Source Editor. Click â€œAdd Columnâ€ to create two columns: â€œtrialâ€ of type â€œNumberâ€ and â€œwordâ€ of type â€œStringâ€. Click â€œAdd Rowâ€ and set the â€œNumber of Rowsâ€ to 12. Enter values for the â€œtrialâ€ column (1 - 12) and the â€œwordâ€ column (e.g., â€œOneâ€, â€œTwoâ€, â€¦, â€œTwelveâ€). 2.6 Editing the Trial Sequence: Preparing Sequence and Drift Correction Add actions to the trial sequence: Open the â€œActionâ€ Tab and drag a â€œPrepare Sequenceâ€ action, a â€œDrift Correctionâ€ action, and a â€œSequenceâ€ node into the workspace. Connect the actions: Draw connections from the â€œSTARTâ€ node to â€œPREPARE_SEQUENCEâ€, from â€œPREPARE_SEQUENCEâ€ to â€œDRIFT_CORRECTIONâ€, and from â€œDRIFT_CORRECTâ€ to the â€œSEQUENCEâ€ node. Right - click any blank area in the work window and select â€œArrange Layoutâ€. 2.7 Editing the Recording Sequence Set sequence properties: Select the newly added â€œSequenceâ€ node, enter a new label like â€œRECORDINGâ€, and check the â€œRecordâ€ and â€œIs Real Timeâ€ checkboxes. Add actions and triggers: Drag a â€œDisplay Screenâ€ action, a â€œTimerâ€ trigger, an â€œEyeLink Buttonâ€ trigger, and a second â€œDisplay Screenâ€ action (rename it as â€œDISPLAY_BLANKâ€) into the workspace. Set the Timer trigger duration to 10000 msec and its â€œMessageâ€ to â€œTime outâ€. Uncheck the â€œSend EyeLink DV Messagesâ€ box for the â€œDISPLAY_BLANKâ€ action. Connect the nodes: Draw connections from the â€œSTARTâ€ node to â€œDISPLAY_SCREENâ€, from â€œDISPLAY_SCREENâ€ to both â€œTIMERâ€ and â€œEL_BUTTONâ€, and from both â€œTIMERâ€ and â€œEL_BUTTONâ€ to â€œDISPLAY_BLANKâ€. Right - click and select â€œArrange Layoutâ€. 2.8 Modifying the Properties of a Display Screen Set message for the first display screen: Select the first DISPLAY_SCREEN node. In the properties window, double - click the value field of the â€œMessageâ€ property and enter a message like â€œSYNCTIMEâ€ to mark the screen presentation. Check the â€œSend EyeLink DV Messagesâ€ and â€œUse for Host Displayâ€ properties. Set message for the blank display screen: Select the â€œDISPLAY_BLANKâ€ action. Double - click the value field of the â€œMessageâ€ property, enter a message like â€œblank_screenâ€, and uncheck the â€œSend EyeLink DV Messagesâ€ and â€œUse for Host Displayâ€ checkboxes. 2.9 Creating the Display Screen Add a text resource: Double - click the â€œDISPLAY_SCREENâ€ object in the work space to open the Screen Builder. Click the â€œInsert Text Resourceâ€ button and click on the screen to add the resource. Modify text properties: Double - click the Font Name property to select the desired font (e.g., Arial). Double - click the Font Size property and enter the desired size (e.g., 40). Set the text to load from the data source by clicking the â€œTextâ€ property value field and using the Attribute Editor to select the â€œwordâ€ attribute from the data source. Check the â€œUse Runtime Word Segmentâ€ box. Align the text in the center of the screen and lock the selection. 2.10 Writing Trial Condition Variables to EDF file Access variable configuration: Click the Experiment node in the structure list. In the properties table, click the value field of the â€œEyeLink DV Variablesâ€ property. Select variables to write: In the dialog box, make sure both the â€œTrialâ€ and â€œWordâ€ columns are included in the â€œSelected Variablesâ€ list. Click â€œOKâ€ to finish. 2.11 Showing Experiment Progress Message on Tracker Screen Set the status message: Select the â€œRECORDINGâ€ sequence node in the structure list. In the properties panel, click the value field of the â€œEyeLink Record Status Messageâ€ property and use the Attribute Editor to enter an equation like â€œTrial \" + str(@parent.iteration@) + â€œ/ \" + str(@parent.iterationCount@) + \" \" + str(@TRIAL_DataSource.word@) to display the trial number and the word on the tracker screen. 3. Building the Experiment Save the project: Click the Save button on the application tool bar if you havenâ€™t saved your experiment project yet. Build the experiment: Click â€œExperiment \u003e Buildâ€. Check the â€œOutputâ€ tab in the Graph Editor Window for error and warning messages. If there are issues, double - click the messages to highlight the problem nodes or screen resources. Test run the experiment: After a successful build, click â€œExperiment \u003e Test Runâ€ to test the experiment. Note that â€œTest Runâ€ is for testing and debugging only, not for data collection. 4. Deploying the Experiment Deploy the experiment: Click â€œExperiment \u003e Deployâ€ to create an executable version of the experiment in a new folder. If a data source is used, a â€œdatasetsâ€ subdirectory with a copy of the data set file will be created. 5. Running the Experiment Prepare to run: Make sure the EyeLink host software is running and the network connection between the host and display computers is established. Start the experiment: Go to the directory where the experiment was deployed and click â€œsimple.exeâ€ to start. Enter an EDF file name (no more than 8 characters, with only letters, numbers, and underscores) when prompted. Conduct the experiment: Follow the on - screen instructions for camera setup, calibration, validation, and pre - trial drift correction. After running all the trials, an EDF file will be transferred to the display computer. Be patient during the file transfer. 6. Common Errors and Solutions 6.1 Error in Initializing Graphics If you see an â€œCould not initialize display to ***â€ error, check if the display settings (screen resolution, color bits, and refresh rate) are supported by your video card and monitor. Update the settings in â€œPreferences \u003e Experiment \u003e Devices \u003e DISPLAYâ€ if needed. Also, make sure your graphics card driver is up - to - date.\n6.2 Invalid Tracker Type If the eye tracker specified in the preferences doesnâ€™t match the one being used, Experiment Builder will show an error message. Set the correct tracker version in the â€œPreferences \u003e Experiment \u003e Devices \u003e EYELINKâ€ section.\ncreate images for eyetracking experiments organize the order of target and competitor (and/or distractor) in the image to display so that they are evenly distributed to the possible positions (e.g., top corner, left corner, right corner)(counterbalance the position effect)\ncreate each image of target/competitor/distractor use Microsoft Powerpoint to create image displays to be used in the experiment export the slides into .png files (specify the size as 800*600)\nQuestions: when to add â€œresultsâ€ components? When to add â€œvariableâ€ nodes? visual stimuli? audio stimuli? other response measures when recording eye movements? longer passages to display?\ndongle needed when: working on script, data viewer (analysis of data), deployment not needed: after deployed and collect data\nin the following I present the nuts and bolts in creating a visual world experiment using Eyelink 1000 plus.\nThe sample experiment has the following levels: (â€“\u003e means â€œconnects toâ€)\nLevel 1: Welcome and Introduction\nSTART â€“\u003e DisplayScreen action (lable: DISPLAY_EYE_TRACKING_EXPLANATION) (Welcome message \u0026 experiment introduction) â€“\u003e Keyboard action \u0026 EyelinkButton action â€“\u003e DisplayScreen action (label: CALI_INSTRUCTIONS) (explain calibration procedure) â€“\u003e Keyboard trigger AND EyelinkButton trigger (offering two ways out) â€“\u003e Sequence action (label: BLOCK)\nLevel 2: BLOCK sequence (iteration = 8) START â€“\u003e EyeLinkCameraSetup action (label: EL_CAMERA_SETUP_EXP) â€“\u003e NullAction action (label: Null_Action) â€“\u003e Conditional trigger (label: DETERMINE_PRAC_OR_EXP) (there are two connections further down)\nconnection #1: if iteration in the Conditional trigger equals 1, then it connects to a DisplayScreen (label: PRAC_INSTR) (practice instructions) â€“\u003e Keyboard trigger (label: KEYBOARD_START) AND EyeLinkButton trigger (label: EL_BUTTON_START) â€“\u003e TRIAL sequence\nconnection #2: if iteration in the Conditional trigger NOT equals 1, then it connects to another Conditional trigger (label: CONDITIONAL), there are two connections further down\nconnection #2.1: if iteration in CONDITIONAL equals 2, it connects to DisplayScreen action (label: EXP_INSTR) (experiment instruction) â€“\u003e Keyboard trigger (label: KEYBOARD_START) AND EyeLinkButton trigger (label: EL_BUTTON_START) â€“\u003e TRIAL sequence\nconnetion #2.2: if iteration in CONDITIONAL NOT equals 2, it connects to a DisplayScreen action (label: BREAK) (take a break) â€“\u003e timer trigger (label: BREAK_TIMER) â€“\u003e TRIAL sequence\nLevel 3: TRIAL sequence (iteration = 356; split by = [5, 12, 12, 12, 12, 12, 12, 12]; Data source: column = 9, row = 356) START â€“\u003e PrepareSquence action (label: PREPARE_SEQUENCE) â€“\u003e DriftCorrection action (label: DRIFT_CORRECT) â€“\u003e RECORDING sequency\nLevel 4: RECORDING sequence (Record = check; Recording Pause Time = 20; Eyelink Record Status Message = = str(@self.iteration@) + â€œ/89â€; Trial Result = 0; Is Real Time = check; iteration Count = 1; Data Source = not specified;) START â€“\u003e DisplayScreen action (label: PIC_ONSET)\n.ias files Generate images Experimental setting: Here is an example stimuli:\nTom is a school teacher who values kindness and empathy in his communication with students. Today, Tom is going to announce test results to two students, Mary and John. Mary failed each of the six tests, while John failed four tests but managed to pass two. Tom approached one of them and said, â€œYou failed some of the tests.â€ After hearing this, Mary found out she had failed all six tests.\nIn each trial, I present a pre-generated image (via PowerPoint) together with an audio input.\nOn each slide, there are three separate pictures. Each slide is converted into an image.\nSet slide size in PowerPoint to match 16:9 (e.g., 33 cm Ã— 18.5 cm)(The resolution of the screen of the display computer is 1920 * 1080; it is close to 16:9)\nEach image consists of three separate pictures, which are organized in a triangle shape. Thus, one picture in a top-center position, one in the left-bottom position, and the last one in the right-bottom position.\nThe PPT-generated images are ABOUT 1280** * 720 in size. (The actual width and height of PowerPoint-generated images are 1280 * 718; or you can set them as 1284 * 720) (The resolution of the screen of the display computer is 1920 * 1080; it is close to 16:9) The images will be centered both horizontally and vertically.\nwhy not 1920 * 1080 but 1280 * 720? This gives a nice large visible area, while leaving small margins on all sides. Exporting a 1920Ã—1080 image would stretch everything, misaligning your interest area calculations unless you rescale manually.\nMeasurements of the three pictures in each image are as follows:\nTop-center picture: width 6 cm; height 6 cm; X (from top) 13.5 cm; Y (from top) 0.5 cm (X = 13.5 (i.e., distance from the top-left corner to the left side of the slide): full size of a slide is 33 cm; thus, half a slide is 16.5cm; size of the picture is 6 cm; thus, half a picture is 3 cm; thus the X of the top-center corner of the picture is 16.5-3 = 13.5) (Y = 0.5 (i.e., distance from the top-left corner to the top side of the slide))\nLeft-bottom picture: width 6 cm; height 6 cm; X (from left) 4 cm; Y (from top) 12 cm (X = 4 (i.e., distance from top-left corner to the left side of the slide)) (12 (i.e., distance from top-left corner to the top side of the slide) = 18.5 - 0.5 (margin from bottom; same as the margin from the top) - 6 (size of the full picture))\nRight-bottom picture: width 6 cm; height 6 cm; X (from left) 23 cm; Y (from top) 12 cm (23 (i.e., distance from top-left corner to the left side of the slide) = 33 - 4 (margin from right; same as the margin from the left) - 6 (size fo the full picture)) (12 (i.e., distance from top-left corner to the top side of the slide) = 18.5 - 0.5(margin from bottom; same as the margin from the top) - 6 (size of the full picture))\nThe actual width and height of PowerPoint-generated images are 1280 * 718.\nUse Gpt to create .ias files\nPROMPT As the roles of the pictures are counter-balanced across images, I will set up the ias metrics for each image separately. The first thing to do is to set up ias metrics for three pictures in terms of the position, namely, top, left, right.\nMeasurements of the three pictures in each image are as follows: Each slide is 33 cm * 18.5 cm Each image is generated vis PowerPoint with the Width of 1280 and Height of 718.\nTop picture: width 6.5 cm; height 6.5 cm; X (from top) 13.25 cm; Y (from top) 2 cm Left picture: width 6.5 cm; height 6.5 cm; X (from left) 5 cm; Y (from top) 10 cm Right-bottom picture: width 6.5 cm; height 6.5 cm; X (from left) 21.5 cm; Y (from top) 10 cm\nPlease generate an ias file for three pictures in the image that could work in Experimental Builder?\nResulting .ias file as a text file\nLabel X Y Width Height Top 514 78 252 252 Left 194 388 252 252 Right 834 388 252 252\nSave the file as plain text file with .ias as the extension\nUse the template ias file and rename the â€œTopâ€, â€œLeftâ€ and â€œRightâ€ as the roles of the three pictures based on â€œTargetâ€, â€œCompetitorâ€, and â€œDistractorâ€. The following is an example (C1.ias) Label X Y Width Height Competitor 514 78 252 252 Target 194 388 252 252 Distractor 834 388 252 252\nwhat can ias files help in terms of data analysis? ğŸ¯ Interest Areas Define What You Care About in the Visual Scene In an eye-tracking experiment: The eye-tracker records raw gaze data â€” basically, X, Y coordinates at every time point. But by itself, a raw (X, Y) point doesnâ€™t tell you much: â€œWas the participant looking at the important object? Or just some background?â€\nâœ… Interest Areas (IAs) are regions you define ahead of time (boxes or other shapes), where: You tell the system: ğŸ‘‰ \"If the participant's gaze falls inside this region, log it specially.\" Thus: You know exactly when and for how long the participant looked at something you care about. âœ… IAs turn meaningless gaze dots into interpretable, analyzable data. ğŸ§© IAs Connect Eye Movements to Experimental Meaning In your case: You have three pictures (Target, Competitor, Distractor) on the screen.\nWith IAs: Every fixation is automatically labeled by which picture it fell into (Target, Competitor, Distractor). This allows you to analyze participantsâ€™ behavior relative to your experimental conditions. â³ IAs Make Time-Based Analysis Possible In time-sensitive designs (like yours, with audio unfolding):\nYou want to know when participants look at the Target, not just whether they eventually looked.\nWith IAs:\nYou can measure how quickly participants shift their gaze toward the Target after hearing â€œyouâ€ or â€œsome.â€\nYou can compute fixation probability over time, aligned with key moments in the stimulus.\nğŸ“ˆ IAs Enable Clean and Efficient Data Analysis Because EyeLink automatically logs: Fixation durations per IA First fixation landing site Total gaze time per IA Probability of fixating each IA over time\nâœ… Your analysis later (in R, Python, or SPSS) can directly use these logs: \"Compare mean dwell time on Target between Polite and Direct conditions.\" \"Calculate gaze bias toward Target during critical audio windows.\" ",
  "wordCount" : "3771",
  "inLanguage": "en",
  "datePublished": "2023-06-17T00:00:00Z",
  "dateModified": "2023-06-17T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://zhangjunfelix.github.io/tool/experiments/eyetracking/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jun Zhang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://zhangjunfelix.github.io/favicon.ico"
    }
  }
}
</script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" />
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ]
  });">
</script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://zhangjunfelix.github.io/" accesskey="h" title="Jun Zhang (Alt + H)">Jun Zhang</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://zhangjunfelix.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://zhangjunfelix.github.io/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://zhangjunfelix.github.io/teaching/" title="Teaching">
                    <span>Teaching</span>
                </a>
            </li>
            <li>
                <a href="https://zhangjunfelix.github.io/publications/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
            <li>
                <a href="https://zhangjunfelix.github.io/tool/" title="Toolbox">
                    <span>Toolbox</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Eyetracking
    </h1>
    <div class="post-meta"><span title='2023-06-17 00:00:00 +0000 UTC'>June 17, 2023</span>&nbsp;Â·&nbsp;18 min

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#learning-about-eye-tracking-technique-creation-design-data-analysis">Learning About Eye-Tracking Technique: creation, design, data analysis</a></li>
        <li><a href="#creating-eyelink-experiments-a-beginners-tutorial">Creating EyeLink Experiments: A Beginner&rsquo;s Tutorial</a></li>
        <li><a href="#1-prerequisites">1. Prerequisites</a></li>
        <li><a href="#2-creating-the-experiment">2. Creating the Experiment</a></li>
        <li><a href="#21-creating-a-new-experiment-project">2.1 Creating a New Experiment Project</a></li>
        <li><a href="#22-configuring-experiment-preference-settings">2.2 Configuring Experiment Preference Settings</a></li>
        <li><a href="#23-the-topmost-experiment-layer">2.3 The Topmost Experiment Layer</a></li>
        <li><a href="#24-creating-the-instructions-screen">2.4 Creating the Instructions Screen</a></li>
        <li><a href="#25-editing-the-trial-sequence-data-source">2.5 Editing the Trial Sequence: Data Source</a></li>
        <li><a href="#26-editing-the-trial-sequence-preparing-sequence-and-drift-correction">2.6 Editing the Trial Sequence: Preparing Sequence and Drift Correction</a></li>
        <li><a href="#27-editing-the-recording-sequence">2.7 Editing the Recording Sequence</a></li>
        <li><a href="#28-modifying-the-properties-of-a-display-screen">2.8 Modifying the Properties of a Display Screen</a></li>
        <li><a href="#29-creating-the-display-screen">2.9 Creating the Display Screen</a></li>
        <li><a href="#210-writing-trial-condition-variables-to-edf-file">2.10 Writing Trial Condition Variables to EDF file</a></li>
        <li><a href="#211-showing-experiment-progress-message-on-tracker-screen">2.11 Showing Experiment Progress Message on Tracker Screen</a></li>
        <li><a href="#3-building-the-experiment">3. Building the Experiment</a></li>
        <li><a href="#4-deploying-the-experiment">4. Deploying the Experiment</a></li>
      </ul>
    </li>
    <li><a href="#5-running-the-experiment">5. Running the Experiment</a></li>
    <li><a href="#6-common-errors-and-solutions">6. Common Errors and Solutions</a>
      <ul>
        <li><a href="#61-error-in-initializing-graphics">6.1 Error in Initializing Graphics</a></li>
        <li><a href="#62-invalid-tracker-type">6.2 Invalid Tracker Type</a></li>
      </ul>
    </li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="learning-about-eye-tracking-technique-creation-design-data-analysis">Learning About Eye-Tracking Technique: creation, design, data analysis<a hidden class="anchor" aria-hidden="true" href="#learning-about-eye-tracking-technique-creation-design-data-analysis">#</a></h3>
<hr>
<p>Learning about the <strong>eye-tracking technique</strong> is motivated by my interest in discovering cognitive processes involved in interpreting scalar quantifiers in social contexts.</p>
<p>My first experiments (hopefully) are about <strong>perspective-taking</strong> and <strong>interpreting scalars in social contexts</strong>, a project emerging from my discussions with Prof. Gerry Altmann.</p>
<p>In this post, I share my experience in learning about this technique in five areas:</p>
<ul>
<li>Reviews</li>
<li>Classical studies</li>
<li>tools to create VWP experiments</li>
<li>Experimental design</li>
<li>Data analysis</li>
</ul>
<hr>
<h4 id="part-i-reviews">Part I: Reviews<a hidden class="anchor" aria-hidden="true" href="#part-i-reviews">#</a></h4>
<p>video tutorials:</p>
<p>Zhan L. Using Eye Movements Recorded in the Visual World Paradigm to Explore the Online Processing of Spoken Language. J Vis Exp. 2018 Oct 13;(140):58086. doi: 10.3791/58086. PMID: 30371678; PMCID: PMC6235534.
<a href="https://app.jove.com/v/58086/using-eye-movements-recorded-visual-world-paradigm-to-explore-online">link to video</a></p>
<p>Tutorials</p>
<ol>
<li>Introduction to Eye Tracking: A Hands-On Tutorial for Students and Practitioners <a href="https://arxiv.org/html/2404.15435v1#S1">Link</a></li>
</ol>
<h4 id="part-ii-classical-studies">Part II: Classical Studies<a hidden class="anchor" aria-hidden="true" href="#part-ii-classical-studies">#</a></h4>
<p>The following is a list of classical VWP studies:</p>
<h4 id="part-iii-tools-to-create-vwp-experiments">Part III: tools to create VWP experiments<a hidden class="anchor" aria-hidden="true" href="#part-iii-tools-to-create-vwp-experiments">#</a></h4>
<h5 id="lab-based-eye-tracking">Lab-Based Eye-Tracking<a hidden class="anchor" aria-hidden="true" href="#lab-based-eye-tracking">#</a></h5>
<p>SR EyeLink<br>
<a href="https://www.youtube.com/watch?v=qCMgHiGbWN4">Experiment Builder Tutorial videos</a></p>
<h5 id="opensource-tools">Opensource tools<a hidden class="anchor" aria-hidden="true" href="#opensource-tools">#</a></h5>
<p>jsPsych, pychoPy, OpenSesame</p>
<ol start="4">
<li><strong>jsPsych Tutorial</strong></li>
</ol>
<ul>
<li><a href="https://www.jspsych.org/7.0/tutorials/hello-world/">jsPsych Official Tutorial</a></li>
<li>Online Book by Winson Yang: <a href="https://winsonfzyang.github.io/jsPsychTutorial/">jsPsychTutorial</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLtdKTIOUlb42qG962wz30fzlUMibJCGQW">YouTube Tutorial Playlist</a></li>
</ul>
<ol start="5">
<li><strong>PyGaze (Eye Tracking) on OpenSesame</strong></li>
</ol>
<ul>
<li><a href="https://osdoc.cogsci.nl/3.3/manual/eyetracking/pygaze/">PyGaze in OpenSesame Documentation</a></li>
<li><a href="http://www.pygaze.org/docs/">PyGaze Documentation</a></li>
</ul>
<ol start="6">
<li><strong>Eyetracking Data Analyses in R</strong></li>
</ol>
<ul>
<li><a href="http://www.eyetracking-r.com/">eyetrackingR Package</a></li>
<li>Tutorial by Marissa Barlaz: <a href="https://marissabarlaz.github.io/portfolio/eyetracking/">Portfolio Example</a></li>
<li><a href="https://www.youtube.com/watch?v=CkoJvnWEOxw">YouTube Tutorial</a></li>
</ul>
<h5 id="online-webcam-based-eye-tracking">Online Webcam-Based Eye-Tracking<a hidden class="anchor" aria-hidden="true" href="#online-webcam-based-eye-tracking">#</a></h5>
<p>Here are some of the resources I use to learn about designing online eye-tracking experiments:</p>
<ol>
<li><strong>WebGazer</strong></li>
</ol>
<ul>
<li><a href="https://webgazer.cs.brown.edu">WebGazer Official Site</a></li>
<li><a href="https://github.com/brownhci/WebGazer">WebGazer on GitHub</a></li>
<li>Tutorial by BROWNHCI: <a href="https://github.com/brownhci/WebGazer/wiki/Tutorial">WebGazer Tutorial</a></li>
</ul>
<ol start="2">
<li><strong>WebGazer Tutorial on jsPsych</strong></li>
</ol>
<ul>
<li>Tutorial by Xiaozhi Zhao: <a href="https://github.com/xiaozhi2/webgazertutorial">GitHub Repository</a></li>
</ul>
<ol start="3">
<li><strong>WebGazer on OpenSesame</strong></li>
</ol>
<ul>
<li><a href="https://osdoc.cogsci.nl/3.3/manual/eyetracking/webgazer/">WebGazer in OpenSesame Documentation</a></li>
</ul>
<h4 id="part-iv-experimental-design">Part IV Experimental Design<a hidden class="anchor" aria-hidden="true" href="#part-iv-experimental-design">#</a></h4>
<p>There are several ways to build an eye-tracking experiment.</p>
<ol>
<li><strong>Experiment Builder</strong>: Provided with the EyeLink machine.</li>
<li><strong>Open-source tools</strong>: Tools like <strong>OpenSesame</strong> and <strong>PsychoPy</strong>.</li>
</ol>
<h5 id="opensesame">OpenSesame<a hidden class="anchor" aria-hidden="true" href="#opensesame">#</a></h5>
<p>OpenSesame (MathÃ´t et al., 2012), used together with the <a href="http://www.pygaze.org">PyGaze</a> plugin (Dalmaijer et al., 2014).</p>
<h5 id="resources-for-using-pygaze-on-opensesame">Resources for using PyGaze on OpenSesame:<a hidden class="anchor" aria-hidden="true" href="#resources-for-using-pygaze-on-opensesame">#</a></h5>
<ol>
<li><a href="https://www.pygaze.org/2018/07/opensesame-and-pygaze-workshop-potsdam/">OpenSesame and PyGaze Workshop in Potsdam</a></li>
<li><a href="https://osdoc.cogsci.nl/3.2/manual/eyetracking/pygaze/">PyGaze (Eye Tracking) - OpenSesame Documentation</a></li>
<li><a href="https://osdoc.cogsci.nl/3.3/tutorials/visual-world/">Visual World Experiment with PyGaze and OpenSesame</a></li>
</ol>
<h5 id="psychopy">PsychoPy<a hidden class="anchor" aria-hidden="true" href="#psychopy">#</a></h5>
<p><a href="https://psychopy.org/index.html">PsychoPy</a> is another open-source tool that supports creating eye-tracking experiments. These experiments are compatible with EyeLink systems.</p>
<h5 id="resources-for-psychopy">Resources for PsychoPy:<a hidden class="anchor" aria-hidden="true" href="#resources-for-psychopy">#</a></h5>
<ol>
<li><a href="https://psychopy.org/hardware/eyeTracking.html">Communicating with an Eye Tracker</a></li>
<li><a href="https://www.youtube.com/watch?v=1tLJHVktrEk">YouTube Tutorial: PsychoPy Integration for EyeLink Eye Trackers</a></li>
</ol>
<h5 id="gorilla">Gorilla<a hidden class="anchor" aria-hidden="true" href="#gorilla">#</a></h5>
<p><a href="https://app.gorilla.sc/login">Gorilla</a> is also an option for creating eye-tracking experiments.</p>
<h5 id="resources-for-gorilla">Resources for Gorilla:<a hidden class="anchor" aria-hidden="true" href="#resources-for-gorilla">#</a></h5>
<ol>
<li><a href="https://support.gorilla.sc/support/tools/legacy-tools/task-builder-1/eye-tracking#overview">Eye Tracking in Task Builder 1</a></li>
<li><a href="https://support.gorilla.sc/support/tools/task-builder-2/eye-tracking#overview">Eye Tracking in Task Builder 2</a></li>
</ol>
<p><strong>Note</strong>: While Gorilla is a great tool, I chose not to use it because there is a fee for recruiting participants.</p>
<hr>
<h6 id="my-path">My Path<a hidden class="anchor" aria-hidden="true" href="#my-path">#</a></h6>
<ol>
<li>Since I am proficient with <strong>OpenSesame</strong> (Iâ€™ve used it for creating SPR tasks and mouse-tracking tasks), I designed a <strong>Visual World (VW) task</strong> using <strong>PyGaze on OpenSesame</strong>.</li>
</ol>
<hr>
<h4 id="part-v-data-analysis">Part V: Data Analysis<a hidden class="anchor" aria-hidden="true" href="#part-v-data-analysis">#</a></h4>
<ol>
<li>Ito, A., Knoeferle, P. Analysing data from the psycholinguistic visual-world paradigm: Comparison of different analysis methods. Behav Res 55, 3461â€“3493 (2023). <a href="https://doi.org/10.3758/s13428-022-01969-3">https://doi.org/10.3758/s13428-022-01969-3</a> <a href="https://link.springer.com/article/10.3758/s13428-022-01969-3">Link to file</a></li>
</ol>
<p>two datasets are from the following two studies and are available at this <a href="https://osf.io/tzn8u/">OSF repository</a><br>
<a href="https://www.sciencedirect.com/science/article/pii/S0749596X17300633?via%3Dihub">Ito et al. (2018b)</a>; <a href="https://onlinelibrary.wiley.com/doi/10.1207/s15516709cog0000_65">Knoeferle and Crocker (2006; Experiment 1)</a>:</p>
<p>Step 1 Data preparation
how to treat (very short and very long) fixations and blinks</p>
<p>tools:<br>
SR Research Data Viewer;<br>
R packages: gazeR (Geller et al., 2020), eyetrackingR (Dink &amp; Ferguson, 2015), VWPre (Porretta et al., 2020)</p>
<p>SR Research Data Viewer <a href="https://www.youtube.com/watch?v=pM_dxz-G_ic">tutorial videos</a><br>
preprocessind data <a href="https://www.youtube.com/watch?v=DpOeGKF9YeY">Data Viewer Video Tutorial: 07 - Filtering Cleaning and Adjusting Data</a></p>
<ol>
<li>The data analysis focuses on experiments designed with <strong>PyGaze</strong>.</li>
</ol>
<hr>
<h5 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h5>
<ul>
<li>Dalmaijer, E., MathÃ´t, S., &amp; Van der Stigchel, S. (2014). PyGaze: An open-source, cross-platform toolbox for minimal-effort programming of eye-tracking experiments. <em>Behavior Research Methods</em>. <a href="https://doi.org/10.3758/s13428-013-0422-2">DOI</a></li>
<li>MathÃ´t, S., Schreij, D., &amp; Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. <em>Behavior Research Methods, 44</em>(2), 314â€“324. <a href="https://doi.org/10.3758/s13428-011-0168-7">DOI</a></li>
<li>Peirce, J. W., Gray, J. R., Simpson, S., MacAskill, M. R., HÃ¶chenberger, R., Sogo, H., Kastman, E., LindelÃ¸v, J. (2019). PsychoPy2: Experiments in behavior made easy. <em>Behavior Research Methods</em>. <a href="https://doi.org/10.3758/s13428-018-01193-y">DOI</a></li>
</ul>
<hr>
<h4 id="part-vi-how-i-create-an-eyetracking-experiment-from-beginning">Part VI How I Create an Eyetracking experiment from beginning<a hidden class="anchor" aria-hidden="true" href="#part-vi-how-i-create-an-eyetracking-experiment-from-beginning">#</a></h4>
<p><a href="EBUserManual2024.pdf">EB user manual 2024 version</a></p>
<p>The following tutorial are generated based on this user manual.</p>
<p>can you format the following content by using hugo markdown syntax?</p>
<h3 id="creating-eyelink-experiments-a-beginners-tutorial">Creating EyeLink Experiments: A Beginner&rsquo;s Tutorial<a hidden class="anchor" aria-hidden="true" href="#creating-eyelink-experiments-a-beginners-tutorial">#</a></h3>
<p>In this tutorial, we&rsquo;ll guide you through creating a simple EyeLink experiment using SR Research Experiment Builder. This experiment will display a single word in the center of the screen in each trial, similar to the &ldquo;SIMPLE&rdquo; template of the EyeLink C Programming API.</p>
<h3 id="1-prerequisites">1. Prerequisites<a hidden class="anchor" aria-hidden="true" href="#1-prerequisites">#</a></h3>
<p>Before you start, make sure you have the SR Research Experiment Builder software installed. The examples in this tutorial assume a screen resolution of 1920 x 1080 at 100% display scale.</p>
<h3 id="2-creating-the-experiment">2. Creating the Experiment<a hidden class="anchor" aria-hidden="true" href="#2-creating-the-experiment">#</a></h3>
<h3 id="21-creating-a-new-experiment-project">2.1 Creating a New Experiment Project<a hidden class="anchor" aria-hidden="true" href="#21-creating-a-new-experiment-project">#</a></h3>
<ol>
<li><strong>Open Experiment Builder</strong>:
<ul>
<li>On Windows, click Start &gt; All Applications &gt; SR Research and choose â€œExperiment Builderâ€.</li>
<li>On macOS, go to the â€œApplications/Experiment Builder/â€ folder and open the â€œExperimentBuilderâ€ application.</li>
</ul>
</li>
<li><strong>Create a new project</strong>:
<ul>
<li>Click â€œFile &gt; Newâ€ on the application menu bar.</li>
<li>In the â€œNew Projectâ€ dialog box:
<ul>
<li>Enter â€œSimpleâ€ in the â€œProject Nameâ€ edit box.</li>
<li>Click the button on the right end of the â€œProject Locationâ€ to browse to the directory where you want to save the experiment project. Make sure the directory exists if you enter it manually.</li>
<li>Leave the â€œTemplatesâ€ field as â€œNoneâ€.</li>
<li>Check the â€œEyeLink Experimentâ€ box.</li>
<li>Select the eye tracker version from the dropdown menu (e.g., EyeLink 1000 Plus) or select â€œCurrentâ€ to allow any EyeLink eye tracker.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="22-configuring-experiment-preference-settings">2.2 Configuring Experiment Preference Settings<a hidden class="anchor" aria-hidden="true" href="#22-configuring-experiment-preference-settings">#</a></h3>
<ol>
<li><strong>Access preferences</strong>:
<ul>
<li>On Windows, select â€œEdit &gt; Preferencesâ€ or press â€œF4â€.</li>
<li>On macOS, click â€œExperimentBuilder &gt; Preferencesâ€ or press Command âŒ˜+â€œ,â€.</li>
</ul>
</li>
<li><strong>Check display settings</strong>:
<ul>
<li>Click â€œPreferences &gt; Experiment &gt; Devices &gt; Displayâ€. Ensure the settings (Width, Height, Bits per Pixel, and Refresh Rate) are supported by your video card and monitor. The default values 1920 Ã— 1080 Ã— 32 Ã— 60 Hz are used in this example.</li>
</ul>
</li>
<li><strong>Configure screen settings</strong>:
<ul>
<li>Click â€œPreferences &gt; Screenâ€. Set the Location Type as &ldquo;Center Position&rdquo; and check the &ldquo;Antialis Drawing&rdquo; box.</li>
</ul>
</li>
<li><strong>Set tracker version</strong>:
<ul>
<li>Ensure the &ldquo;Tracker Version&rdquo; setting in the &ldquo;Preferences -&gt; Experiment -&gt; Devices -&gt; EyeLink&rdquo; preferences is set correctly for your eye tracker. If you use EyeLink 1000, 1000 Plus, or Portable Duo, configure the correct â€œCamera Mountâ€ and â€œMouse Usage setting.</li>
</ul>
</li>
<li><strong>Set file encoding</strong>:
<ul>
<li>If you plan to use non - ASCII characters, check the â€œEncode Files as UTF - 8â€ box in the Build/Deploy node.</li>
</ul>
</li>
</ol>
<h3 id="23-the-topmost-experiment-layer">2.3 The Topmost Experiment Layer<a hidden class="anchor" aria-hidden="true" href="#23-the-topmost-experiment-layer">#</a></h3>
<ol>
<li><strong>Add nodes to the workspace</strong>:
<ul>
<li>Open the â€œActionâ€ Tab of the component toolbox and drag a â€œDisplay Screenâ€ action into the work area.</li>
<li>Open the â€œTriggerâ€ Tab and drag a â€œKeyboardâ€ trigger, an â€œEyeLink Buttonâ€ trigger, and a â€œTimerâ€ trigger into the work space.</li>
<li>Set the Timer trigger duration to 120000 msec.</li>
<li>Open the â€œActionâ€ Tab again and add a â€œCamera Setupâ€ action and a â€œSequenceâ€ node to the work space.</li>
</ul>
</li>
<li><strong>Connect the nodes</strong>:
<ul>
<li>Draw connections from the START node to the DISPLAY_SCREEN node.</li>
<li>Connect the DISPLAY_SCREEN action to the KEYBOARD, EL_BUTTON, and TIMER triggers.</li>
<li>Connect each of the three triggers to the EL_CAMERA_SETUP node, then from EL_CAMERA_SETUP to the SEQUENCE node.</li>
<li>Right - click any blank area in the work window and select &ldquo;Arrange Layout&rdquo; to organize the nodes.</li>
</ul>
</li>
</ol>
<h3 id="24-creating-the-instructions-screen">2.4 Creating the Instructions Screen<a hidden class="anchor" aria-hidden="true" href="#24-creating-the-instructions-screen">#</a></h3>
<ol>
<li><strong>Open the Screen Builder</strong>:
<ul>
<li>Double - click the DISPLAY_SCREEN node in the workspace.</li>
</ul>
</li>
<li><strong>Add a multi - line text resource</strong>:
<ul>
<li>Click the multiline text resource button on the screen builder toolbar and click anywhere on the screen to add the resource.</li>
</ul>
</li>
<li><strong>Edit the text</strong>:
<ul>
<li>Set the text margins to 100 in all fields.</li>
<li>Enter the instruction text, such as instructions for camera setup, calibration, and starting recordings.</li>
<li>Select all text (Ctrl + A on Windows or Command âŒ˜ + A on macOS) and set the text appearance (font name, size, style, alignment, line spacing, and color).</li>
<li>Click the â€œCloseâ€ button to finish.</li>
</ul>
</li>
</ol>
<h3 id="25-editing-the-trial-sequence-data-source">2.5 Editing the Trial Sequence: Data Source<a hidden class="anchor" aria-hidden="true" href="#25-editing-the-trial-sequence-data-source">#</a></h3>
<ol>
<li><strong>Select the trial sequence node</strong>:
<ul>
<li>Select the last â€œSEQUENCEâ€ node in the structure list and enter a new value for the Label, e.g., â€œTRIALâ€.</li>
</ul>
</li>
<li><strong>Create the data source</strong>:
<ul>
<li>Click the value cell of the â€œData Sourceâ€ property to bring up the Data Source Editor.</li>
<li>Click &ldquo;Add Column&rdquo; to create two columns: â€œtrialâ€ of type &ldquo;Number&rdquo; and â€œwordâ€ of type &ldquo;String&rdquo;.</li>
<li>Click â€œAdd Rowâ€ and set the â€œNumber of Rowsâ€ to 12.</li>
<li>Enter values for the â€œtrialâ€ column (1 - 12) and the â€œwordâ€ column (e.g., â€œOneâ€, â€œTwoâ€, â€¦, â€œTwelveâ€).</li>
</ul>
</li>
</ol>
<h3 id="26-editing-the-trial-sequence-preparing-sequence-and-drift-correction">2.6 Editing the Trial Sequence: Preparing Sequence and Drift Correction<a hidden class="anchor" aria-hidden="true" href="#26-editing-the-trial-sequence-preparing-sequence-and-drift-correction">#</a></h3>
<ol>
<li><strong>Add actions to the trial sequence</strong>:
<ul>
<li>Open the â€œActionâ€ Tab and drag a â€œPrepare Sequenceâ€ action, a &ldquo;Drift Correction&rdquo; action, and a â€œSequenceâ€ node into the workspace.</li>
</ul>
</li>
<li><strong>Connect the actions</strong>:
<ul>
<li>Draw connections from the â€œSTARTâ€ node to â€œPREPARE_SEQUENCEâ€, from â€œPREPARE_SEQUENCEâ€ to â€œDRIFT_CORRECTIONâ€, and from â€œDRIFT_CORRECTâ€ to the â€œSEQUENCEâ€ node.</li>
<li>Right - click any blank area in the work window and select &ldquo;Arrange Layout&rdquo;.</li>
</ul>
</li>
</ol>
<h3 id="27-editing-the-recording-sequence">2.7 Editing the Recording Sequence<a hidden class="anchor" aria-hidden="true" href="#27-editing-the-recording-sequence">#</a></h3>
<ol>
<li><strong>Set sequence properties</strong>:
<ul>
<li>Select the newly added â€œSequenceâ€ node, enter a new label like â€œRECORDINGâ€, and check the â€œRecordâ€ and â€œIs Real Timeâ€ checkboxes.</li>
</ul>
</li>
<li><strong>Add actions and triggers</strong>:
<ul>
<li>Drag a â€œDisplay Screenâ€ action, a â€œTimerâ€ trigger, an â€œEyeLink Buttonâ€ trigger, and a second â€œDisplay Screenâ€ action (rename it as â€œDISPLAY_BLANKâ€) into the workspace.</li>
<li>Set the Timer trigger duration to 10000 msec and its â€œMessageâ€ to â€œTime outâ€.</li>
<li>Uncheck the â€œSend EyeLink DV Messagesâ€ box for the â€œDISPLAY_BLANKâ€ action.</li>
</ul>
</li>
<li><strong>Connect the nodes</strong>:
<ul>
<li>Draw connections from the â€œSTARTâ€ node to â€œDISPLAY_SCREENâ€, from â€œDISPLAY_SCREENâ€ to both â€œTIMERâ€ and â€œEL_BUTTONâ€, and from both â€œTIMERâ€ and â€œEL_BUTTONâ€ to â€œDISPLAY_BLANKâ€.</li>
<li>Right - click and select â€œArrange Layoutâ€.</li>
</ul>
</li>
</ol>
<h3 id="28-modifying-the-properties-of-a-display-screen">2.8 Modifying the Properties of a Display Screen<a hidden class="anchor" aria-hidden="true" href="#28-modifying-the-properties-of-a-display-screen">#</a></h3>
<ol>
<li><strong>Set message for the first display screen</strong>:
<ul>
<li>Select the first DISPLAY_SCREEN node. In the properties window, double - click the value field of the â€œMessageâ€ property and enter a message like â€œSYNCTIMEâ€ to mark the screen presentation. Check the â€œSend EyeLink DV Messagesâ€ and â€œUse for Host Displayâ€ properties.</li>
</ul>
</li>
<li><strong>Set message for the blank display screen</strong>:
<ul>
<li>Select the â€œDISPLAY_BLANKâ€ action. Double - click the value field of the â€œMessageâ€ property, enter a message like â€œblank_screenâ€, and uncheck the â€œSend EyeLink DV Messagesâ€ and â€œUse for Host Displayâ€ checkboxes.</li>
</ul>
</li>
</ol>
<h3 id="29-creating-the-display-screen">2.9 Creating the Display Screen<a hidden class="anchor" aria-hidden="true" href="#29-creating-the-display-screen">#</a></h3>
<ol>
<li><strong>Add a text resource</strong>:
<ul>
<li>Double - click the â€œDISPLAY_SCREENâ€ object in the work space to open the Screen Builder. Click the â€œInsert Text Resourceâ€ button and click on the screen to add the resource.</li>
</ul>
</li>
<li><strong>Modify text properties</strong>:
<ul>
<li>Double - click the Font Name property to select the desired font (e.g., Arial).</li>
<li>Double - click the Font Size property and enter the desired size (e.g., 40).</li>
<li>Set the text to load from the data source by clicking the â€œTextâ€ property value field and using the Attribute Editor to select the â€œwordâ€ attribute from the data source.</li>
<li>Check the â€œUse Runtime Word Segmentâ€ box.</li>
<li>Align the text in the center of the screen and lock the selection.</li>
</ul>
</li>
</ol>
<h3 id="210-writing-trial-condition-variables-to-edf-file">2.10 Writing Trial Condition Variables to EDF file<a hidden class="anchor" aria-hidden="true" href="#210-writing-trial-condition-variables-to-edf-file">#</a></h3>
<ol>
<li><strong>Access variable configuration</strong>:
<ul>
<li>Click the Experiment node in the structure list.</li>
<li>In the properties table, click the value field of the â€œEyeLink DV Variablesâ€ property.</li>
</ul>
</li>
<li><strong>Select variables to write</strong>:
<ul>
<li>In the dialog box, make sure both the â€œTrialâ€ and â€œWordâ€ columns are included in the â€œSelected Variablesâ€ list. Click â€œOKâ€ to finish.</li>
</ul>
</li>
</ol>
<h3 id="211-showing-experiment-progress-message-on-tracker-screen">2.11 Showing Experiment Progress Message on Tracker Screen<a hidden class="anchor" aria-hidden="true" href="#211-showing-experiment-progress-message-on-tracker-screen">#</a></h3>
<ol>
<li><strong>Set the status message</strong>:
<ul>
<li>Select the â€œRECORDINGâ€ sequence node in the structure list.</li>
<li>In the properties panel, click the value field of the â€œEyeLink Record Status Messageâ€ property and use the Attribute Editor to enter an equation like &ldquo;Trial &quot; + str(@parent.iteration@) + &ldquo;/ &quot; + str(@parent.iterationCount@) + &quot; &quot; + str(@TRIAL_DataSource.word@) to display the trial number and the word on the tracker screen.</li>
</ul>
</li>
</ol>
<h3 id="3-building-the-experiment">3. Building the Experiment<a hidden class="anchor" aria-hidden="true" href="#3-building-the-experiment">#</a></h3>
<ol>
<li><strong>Save the project</strong>:
<ul>
<li>Click the Save button on the application tool bar if you haven&rsquo;t saved your experiment project yet.</li>
</ul>
</li>
<li><strong>Build the experiment</strong>:
<ul>
<li>Click â€œExperiment &gt; Buildâ€. Check the â€œOutputâ€ tab in the Graph Editor Window for error and warning messages. If there are issues, double - click the messages to highlight the problem nodes or screen resources.</li>
</ul>
</li>
<li><strong>Test run the experiment</strong>:
<ul>
<li>After a successful build, click â€œExperiment &gt; Test Runâ€ to test the experiment. Note that â€œTest Runâ€ is for testing and debugging only, not for data collection.</li>
</ul>
</li>
</ol>
<h3 id="4-deploying-the-experiment">4. Deploying the Experiment<a hidden class="anchor" aria-hidden="true" href="#4-deploying-the-experiment">#</a></h3>
<ol>
<li><strong>Deploy the experiment</strong>:
<ul>
<li>Click â€œExperiment &gt; Deployâ€ to create an executable version of the experiment in a new folder. If a data source is used, a â€œdatasetsâ€ subdirectory with a copy of the data set file will be created.</li>
</ul>
</li>
</ol>
<h2 id="5-running-the-experiment">5. Running the Experiment<a hidden class="anchor" aria-hidden="true" href="#5-running-the-experiment">#</a></h2>
<ol>
<li><strong>Prepare to run</strong>:
<ul>
<li>Make sure the EyeLink host software is running and the network connection between the host and display computers is established.</li>
</ul>
</li>
<li><strong>Start the experiment</strong>:
<ul>
<li>Go to the directory where the experiment was deployed and click â€œsimple.exeâ€ to start. Enter an EDF file name (no more than 8 characters, with only letters, numbers, and underscores) when prompted.</li>
</ul>
</li>
<li><strong>Conduct the experiment</strong>:
<ul>
<li>Follow the on - screen instructions for camera setup, calibration, validation, and pre - trial drift correction. After running all the trials, an EDF file will be transferred to the display computer. Be patient during the file transfer.</li>
</ul>
</li>
</ol>
<h2 id="6-common-errors-and-solutions">6. Common Errors and Solutions<a hidden class="anchor" aria-hidden="true" href="#6-common-errors-and-solutions">#</a></h2>
<h3 id="61-error-in-initializing-graphics">6.1 Error in Initializing Graphics<a hidden class="anchor" aria-hidden="true" href="#61-error-in-initializing-graphics">#</a></h3>
<p>If you see an â€œCould not initialize display to ***â€ error, check if the display settings (screen resolution, color bits, and refresh rate) are supported by your video card and monitor. Update the settings in â€œPreferences &gt; Experiment &gt; Devices &gt; DISPLAYâ€ if needed. Also, make sure your graphics card driver is up - to - date.</p>
<h3 id="62-invalid-tracker-type">6.2 Invalid Tracker Type<a hidden class="anchor" aria-hidden="true" href="#62-invalid-tracker-type">#</a></h3>
<p>If the eye tracker specified in the preferences doesn&rsquo;t match the one being used, Experiment Builder will show an error message. Set the correct tracker version in the â€œPreferences &gt; Experiment &gt; Devices &gt; EYELINKâ€ section.</p>
<p>create images for eyetracking experiments
organize the order of target and competitor (and/or distractor) in the image to display so that they are evenly distributed to the possible positions (e.g., top corner, left corner, right corner)(counterbalance the position effect)</p>
<p>create each image of target/competitor/distractor
use Microsoft Powerpoint to create image displays to be used in the experiment
export the slides into .png files (specify the size as 800*600)</p>
<p>Questions:
when to add &ldquo;results&rdquo; components?
When to add &ldquo;variable&rdquo; nodes?
visual stimuli?
audio stimuli?
other response measures when recording eye movements?
longer passages to display?</p>
<p>dongle
needed when: working on script, data viewer (analysis of data), deployment
not needed: after deployed and collect data</p>
<p>in the following I present the nuts and bolts in creating a visual world experiment using Eyelink 1000 plus.</p>
<p>The sample experiment has the following levels:
(&ndash;&gt; means &ldquo;connects to&rdquo;)</p>
<p>Level 1: Welcome and Introduction</p>
<p>START &ndash;&gt; DisplayScreen action (lable: DISPLAY_EYE_TRACKING_EXPLANATION) (Welcome message &amp; experiment introduction) &ndash;&gt; Keyboard action &amp; EyelinkButton action &ndash;&gt; DisplayScreen action (label: CALI_INSTRUCTIONS) (explain calibration procedure) &ndash;&gt; Keyboard trigger AND EyelinkButton trigger (offering two ways out) &ndash;&gt; Sequence action (label: BLOCK)</p>
<p>Level 2: BLOCK sequence (iteration = 8)
START &ndash;&gt; EyeLinkCameraSetup action (label: EL_CAMERA_SETUP_EXP) &ndash;&gt; NullAction action (label: Null_Action) &ndash;&gt; Conditional trigger (label: DETERMINE_PRAC_OR_EXP) (there are two connections further down)</p>
<p>connection #1: if iteration in the Conditional trigger equals 1, then it connects to a DisplayScreen (label: PRAC_INSTR) (practice instructions) &ndash;&gt; Keyboard trigger (label: KEYBOARD_START) AND EyeLinkButton trigger (label: EL_BUTTON_START) &ndash;&gt; TRIAL sequence</p>
<p>connection #2: if iteration in the Conditional trigger NOT equals 1, then it connects to another Conditional trigger (label: CONDITIONAL), there are two connections further down</p>
<p>connection #2.1: if iteration in CONDITIONAL equals 2, it connects to DisplayScreen action (label: EXP_INSTR) (experiment instruction) &ndash;&gt; Keyboard trigger (label: KEYBOARD_START) AND EyeLinkButton trigger (label: EL_BUTTON_START) &ndash;&gt; TRIAL sequence</p>
<p>connetion #2.2: if iteration in CONDITIONAL NOT equals 2, it connects to a DisplayScreen action (label: BREAK) (take a break) &ndash;&gt; timer trigger (label: BREAK_TIMER) &ndash;&gt; TRIAL sequence</p>
<p>Level 3: TRIAL sequence (iteration = 356; split by = [5, 12, 12, 12, 12, 12, 12, 12]; Data source: column = 9, row = 356)
START &ndash;&gt; PrepareSquence action (label: PREPARE_SEQUENCE) &ndash;&gt; DriftCorrection action (label: DRIFT_CORRECT) &ndash;&gt; RECORDING sequency</p>
<p>Level 4: RECORDING sequence
(Record = check; Recording Pause Time = 20; Eyelink Record Status Message = = str(@self.iteration@) + &ldquo;/89&rdquo;; Trial Result = 0; Is Real Time = check; iteration Count = 1; Data Source = not specified;)
START &ndash;&gt; DisplayScreen action (label: PIC_ONSET)</p>
<h4 id="ias-files">.ias files<a hidden class="anchor" aria-hidden="true" href="#ias-files">#</a></h4>
<p>Generate images
Experimental setting:
Here is an example stimuli:</p>
<blockquote>
<p>Tom is a school teacher who values kindness and empathy in his communication with students. Today, Tom is going to announce test results to two students, Mary and John. Mary failed each of the six tests, while John failed four tests but managed to pass two.
Tom approached one of them and said, â€œ<strong>You</strong> failed <strong>some of</strong> <strong>the tests</strong>.â€ After hearing this, <strong>Mary</strong> found out she had failed all six tests.</p>
</blockquote>
<ol>
<li>
<p>In each trial, I present a pre-generated image (via PowerPoint) together with an audio input.</p>
</li>
<li>
<p>On each slide, there are three separate pictures. Each slide is converted into an image.</p>
</li>
<li>
<p>Set slide size in PowerPoint to match 16:9 (e.g., 33 cm Ã— 18.5 cm)(The resolution of the screen of the display computer is 1920 * 1080; it is close to 16:9)</p>
</li>
<li>
<p>Each image consists of three separate pictures, which are organized in a triangle shape. Thus, one picture in a top-center position, one in the left-bottom position, and the last one in the right-bottom position.</p>
</li>
<li>
<p>The PPT-generated images are ABOUT 1280** * 720 in size. (The actual width and height of PowerPoint-generated images are 1280 * 718; or you can set them as 1284 * 720) (The resolution of the screen of the display computer is 1920 * 1080; it is close to 16:9) The images will be centered both horizontally and vertically.</p>
</li>
<li>
<p>why not 1920 * 1080 but 1280 * 720? This gives a nice large visible area, while leaving small margins on all sides. Exporting a 1920Ã—1080 image would stretch everything, misaligning your interest area calculations unless you rescale manually.</p>
</li>
<li>
<p>Measurements of the three pictures in each image are as follows:</p>
<p>Top-center picture: width 6 cm; height 6 cm; X (from top) 13.5 cm; Y (from top) 0.5 cm
(X = 13.5 (i.e., distance from the top-left corner to the left side of the slide): full size of a slide is 33 cm; thus, half a slide is 16.5cm; size of the picture is 6 cm; thus, half a picture is 3 cm; thus the X of the top-center corner of the picture is 16.5-3 = 13.5)
(Y = 0.5 (i.e., distance from the top-left corner to the top side of the slide))</p>
<p>Left-bottom picture: width 6 cm; height 6 cm; X (from left) 4 cm; Y (from top) 12 cm
(X = 4 (i.e., distance from top-left corner to the left side of the slide))
(12 (i.e., distance from top-left corner to the top side of the slide) = 18.5 - 0.5 (margin from bottom; same as the margin from the top) - 6 (size of the full picture))</p>
<p>Right-bottom picture: width 6 cm; height 6 cm; X (from left) 23 cm; Y (from top) 12 cm
(23 (i.e., distance from top-left corner to the left side of the slide) = 33 - 4 (margin from right; same as the margin from the left) - 6 (size fo the full picture))
(12 (i.e., distance from top-left corner to the top side of the slide) = 18.5 - 0.5(margin from bottom; same as the margin from the top) - 6 (size of the full picture))</p>
</li>
<li>
<p>The actual width and height of PowerPoint-generated images are 1280 * 718.</p>
</li>
<li>
<p>Use Gpt to create .ias files</p>
</li>
</ol>
<p>PROMPT
As the roles of the pictures are counter-balanced across images, I will set up the ias metrics for each image separately. The first thing to do is to set up ias metrics for three pictures in terms of the position, namely, top, left, right.</p>
<p>Measurements of the three pictures in each image are as follows:
Each slide is 33 cm * 18.5 cm
Each image is generated vis PowerPoint with the Width of 1280 and Height of 718.</p>
<p>Top picture: width 6.5 cm; height 6.5 cm; X (from top) 13.25 cm; Y (from top) 2 cm
Left picture: width 6.5 cm; height 6.5 cm; X (from left) 5 cm; Y (from top) 10 cm
Right-bottom picture: width 6.5 cm; height 6.5 cm; X (from left) 21.5 cm; Y (from top) 10 cm</p>
<p>Please generate an ias file for three pictures in the image that could work in Experimental Builder?</p>
<p>Resulting .ias file as a text file</p>
<h1 id="label-------x----y----width---height">Label       X    Y    Width   Height<a hidden class="anchor" aria-hidden="true" href="#label-------x----y----width---height">#</a></h1>
<p>Top          514  78   252     252
Left         194  388  252     252
Right        834  388  252     252</p>
<p>Save the file as plain text file with .ias as the extension</p>
<ol start="10">
<li>Use the template ias file and rename the &ldquo;Top&rdquo;, &ldquo;Left&rdquo; and &ldquo;Right&rdquo; as the roles of the three pictures based on &ldquo;Target&rdquo;, &ldquo;Competitor&rdquo;, and &ldquo;Distractor&rdquo;. The following is an example (C1.ias)</li>
</ol>
<h1 id="label-------x----y----width---height-1">Label       X    Y    Width   Height<a hidden class="anchor" aria-hidden="true" href="#label-------x----y----width---height-1">#</a></h1>
<p>Competitor          514  78   252     252
Target         194  388  252     252
Distractor        834  388  252     252</p>
<ol start="12">
<li>what can ias files help in terms of data analysis?</li>
</ol>
<p>ğŸ¯ Interest Areas Define What You Care About in the Visual Scene
In an eye-tracking experiment:
The eye-tracker records raw gaze data â€” basically, X, Y coordinates at every time point.
But by itself, a raw (X, Y) point doesn&rsquo;t tell you much:
&ldquo;Was the participant looking at the important object? Or just some background?&rdquo;</p>
<pre><code>  âœ… Interest Areas (IAs) are regions you define ahead of time (boxes or other shapes), where:
  You tell the system:
  ğŸ‘‰ &quot;If the participant's gaze falls inside this region, log it specially.&quot;
  Thus: You know exactly when and for how long the participant looked at something you care about.
  âœ… IAs turn meaningless gaze dots into interpretable, analyzable data.
</code></pre>
<p>ğŸ§© IAs Connect Eye Movements to Experimental Meaning
In your case:
You have three pictures (Target, Competitor, Distractor) on the screen.</p>
<pre><code>  With IAs:
  Every fixation is automatically labeled by which picture it fell into (Target, Competitor, Distractor).
  This allows you to analyze participantsâ€™ behavior relative to your experimental conditions.
</code></pre>
<p>â³ IAs Make Time-Based Analysis Possible
In time-sensitive designs (like yours, with audio unfolding):</p>
<p>You want to know when participants look at the Target, not just whether they eventually looked.</p>
<p>With IAs:</p>
<p>You can measure how quickly participants shift their gaze toward the Target after hearing &ldquo;you&rdquo; or &ldquo;some.&rdquo;</p>
<p>You can compute fixation probability over time, aligned with key moments in the stimulus.</p>
<p>ğŸ“ˆ IAs Enable Clean and Efficient Data Analysis
Because EyeLink automatically logs:
Fixation durations per IA
First fixation landing site
Total gaze time per IA
Probability of fixating each IA over time</p>
<pre><code>  âœ… Your analysis later (in R, Python, or SPSS) can directly use these logs:
  &quot;Compare mean dwell time on Target between Polite and Direct conditions.&quot;
  &quot;Calculate gaze bias toward Target during critical audio windows.&quot;
</code></pre>
<ol start="13">
<li></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://zhangjunfelix.github.io/">Jun Zhang</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
