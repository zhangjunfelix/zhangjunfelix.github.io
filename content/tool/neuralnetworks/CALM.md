---
title: "Notes to Computational Approach to Language and Mind (CALM)"
date: 2025-02-05T15:38:14+08:00
draft: false

---



# Course Schedule

## Week 1

- **Jargon checklist** [ðŸ“„ View](Jargon_checklist.pdf)
- **Reading:** *AI-pocalypse* [ðŸ“„ View](AI-pocalypse.pdf)

---

## Week 2: LSA to SRNs

In this seminar, we shall explore one perspective on "meaning" through *Latent Semantic Analysis (LSA)*. Key concepts include: 
- Words-as-vectors
- Semantic space
- Semantic distance/similarity  

We will then examine how similar semantic spaces can be induced using a *Simple Recurrent Network (SRN)*, covering:
- Distributed representation
- Spreading activation
- Weight change
- Recurrence
- Prediction task
- Emergent representation

Slides and reading materials:
- **Slides:** *SRN_1* [ðŸ“„ View](SRN_1.pdf)
- **Reading:** *Chapter 13 - The Ascent of Babel* [ðŸ“„ View](Chap13_TheAscentofBabel.pdf)

### Additional Resources:
- ðŸŽ¥ **Video Tutorials:**  
  - [Neural Network Simply Explained](https://www.youtube.com/watch?v=ER2It2mIagI)  
  - [But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk&t=0s)  
  - [What is a neuron?](https://www.youtube.com/watch?v=VhRtaziEWd4)  

---

## Week 3: From Backprop to Word2Vec  

This seminar focuses on *backpropagation*â€”adjusting network weights to achieve better predictions. We will discuss:
- Recurrence and backpropagation challenges
- Scaling issues with SRNs
- Word2Vec as a solution for large vocabularies

Slides and reading materials:
- **Slides:** *Word2Vec* [ðŸ“„ View](Word2Vec.pdf)
- **Note:** *Backpropagation through time* [ðŸ“„ View](Backpropagation_through_time.pdf)

### Additional Resources:
- ðŸŽ¥ **Video Tutorials:**  
  - [What is Word2Vec? A Simple Explanation](https://www.youtube.com/watch?v=hQwFeIupNP0&t=45s)  
  - [Recurrent Neural Networks (RNNs), Clearly Explained](https://www.youtube.com/watch?v=AsNTP8Kwu80)  

---

## Week 4: Amplify the Echoes  

This session consolidates key concepts from previous weeks. We will examine:
- Random weight initialization
- Deep learning's reliance on structured learning
- The importance of trust in model training  
- *Long Short-Term Memory (LSTM)* and *Gated Recurrent Units (GRUs)*  

### Additional Resources:
- ðŸŽ¥ **Video Tutorials:**  
  - [Simple Explanation of GRU](https://www.youtube.com/watch?v=tOuXgORsXJ4)  
  - [What is LSTM (Long Short Term Memory)?](https://www.youtube.com/watch?v=b61DPVFX03I)  
  - [Illustrated Guide to LSTMs and GRUs](https://www.youtube.com/watch?v=8HyCNIVRbSU&t=32s)  

- **Note:** *Simplified GRU Description* [ðŸ“„ View](simplistic_GRU.pdf)  

---

## Week 5: Event Representation as Comprehension  

This session is based on a computational modeling paper co-authored with *Forrest Davis*. We will explore how RNNs encode event knowledge and their role in language understanding.

- **Reading:** *Finding event structure in time* [ðŸ“„ View](Davis_Altmann_2021.pdf)

---

## Week 6: Attending to the Context  

We will focus on *Attention*, *Transformers*, and *BERT*.  

### Additional Resources:
- ðŸŽ¥ **Video Tutorials:**  
  - [What is Attention in Language Models?](https://www.youtube.com/watch?v=j10yrR6PPfg)  
  - [What are Transformer Models and How do they Work?](https://www.youtube.com/watch?v=tsbRdJbJi9U)  
  - [The Narrated Transformer Language Model](https://www.youtube.com/watch?v=-QH8fRhqFHM&t=4s)  

- **Readings:**
  - *How LLMs Work - Explained in 9 Steps* [ðŸ“„ View](How_LLMs_Work_Explained_in_9_Steps.pdf)  
  - *BERT 101* [ðŸ“„ View](BERT101.pdf)  
  - **Slides:** *Recap II and Transformers* [ðŸ“„ View](Recap_II_and_Transformers.pdf)  

---

## Week 7: How Fine-Tuning Creates Chatbots  

Having understood Transformers, we now focus on fine-tuning and chatbot development.  

### Additional Resources:
- ðŸ“– **Articles:**
  - [How Large Language Models Work: From Zero to ChatGPT](https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f) [ðŸ“„ View](How_LLMs_Work_From_Zero_to_ChatGPT.pdf)  
  - [LLM Training: A Simple 3-Step Guide](https://masteringllm.medium.com/llm-training-a-simple-3-step-guide-you-wont-find-anywhere-else-98ee218809e5) [ðŸ“„ View](LLM_Training.pdf)  
  - [What Is ChatGPT Doing â€¦ and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/) [ðŸ“„ View](What_is_ChatGPT_doing.pdf)  
  - [How does Llama 2 work?](https://www.ibm.com/think/topics/llama-2#:~:text=Llama%202%20is%20a%20family%20of%20transformer%2Dbased%20autoregressive%20causal,the%20next%20word(s).0)  

- **Slides:** *Key Concepts* [ðŸ“„ View](Key_concepts.pdf)  
  *Note: These slides are from 2024â€”updated versions will be available soon.*

---

This refined version enhances clarity and organization while adhering to Hugo Markdown syntax, ensuring consistency in formatting across headings, lists, links, and spacing. Let me know if you'd like further adjustments! ðŸš€
