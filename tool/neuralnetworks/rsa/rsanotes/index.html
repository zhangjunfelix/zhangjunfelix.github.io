<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Rational Speech Act-learning notes | Jun Zhang</title>
<meta name="keywords" content="">
<meta name="description" content="&ldquo;&hellip; one of my avowed aims is to see talking as a special case or variety of purposive, indeed rational, behavior&rdquo; (Grice, 1975: 47)

I. Background of RSA
The Rational Speech Act (RSA) framework was developed within the broader enterprise of Probabilistic Pragmatics, a rapidly growing approach in the study of meaning.
Probabilistic pragmatics integrates insights from formal and experimental semantics and pragmatics, psycholinguistics, and computational cognitive science. It offers a unified framework for modeling how speakers produce and listeners interpret language in context.">
<meta name="author" content="">
<link rel="canonical" href="https://zhangjunfelix.github.io/tool/neuralnetworks/rsa/rsanotes/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://zhangjunfelix.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://zhangjunfelix.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://zhangjunfelix.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://zhangjunfelix.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://zhangjunfelix.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://zhangjunfelix.github.io/tool/neuralnetworks/rsa/rsanotes/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://zhangjunfelix.github.io/tool/neuralnetworks/rsa/rsanotes/">
  <meta property="og:site_name" content="Jun Zhang">
  <meta property="og:title" content="Rational Speech Act-learning notes">
  <meta property="og:description" content="“… one of my avowed aims is to see talking as a special case or variety of purposive, indeed rational, behavior” (Grice, 1975: 47)
I. Background of RSA The Rational Speech Act (RSA) framework was developed within the broader enterprise of Probabilistic Pragmatics, a rapidly growing approach in the study of meaning.
Probabilistic pragmatics integrates insights from formal and experimental semantics and pragmatics, psycholinguistics, and computational cognitive science. It offers a unified framework for modeling how speakers produce and listeners interpret language in context.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="tool">
    <meta property="article:published_time" content="2025-05-06T11:41:54-04:00">
    <meta property="article:modified_time" content="2025-05-06T11:41:54-04:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Rational Speech Act-learning notes">
<meta name="twitter:description" content="&ldquo;&hellip; one of my avowed aims is to see talking as a special case or variety of purposive, indeed rational, behavior&rdquo; (Grice, 1975: 47)

I. Background of RSA
The Rational Speech Act (RSA) framework was developed within the broader enterprise of Probabilistic Pragmatics, a rapidly growing approach in the study of meaning.
Probabilistic pragmatics integrates insights from formal and experimental semantics and pragmatics, psycholinguistics, and computational cognitive science. It offers a unified framework for modeling how speakers produce and listeners interpret language in context.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Toolbox",
      "item": "https://zhangjunfelix.github.io/tool/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Neural Networks \u0026 LLMs",
      "item": "https://zhangjunfelix.github.io/tool/neuralnetworks/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Rational Speech Act-learning notes",
      "item": "https://zhangjunfelix.github.io/tool/neuralnetworks/rsa/rsanotes/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Rational Speech Act-learning notes",
  "name": "Rational Speech Act-learning notes",
  "description": "\u0026ldquo;\u0026hellip; one of my avowed aims is to see talking as a special case or variety of purposive, indeed rational, behavior\u0026rdquo; (Grice, 1975: 47)\nI. Background of RSA The Rational Speech Act (RSA) framework was developed within the broader enterprise of Probabilistic Pragmatics, a rapidly growing approach in the study of meaning.\nProbabilistic pragmatics integrates insights from formal and experimental semantics and pragmatics, psycholinguistics, and computational cognitive science. It offers a unified framework for modeling how speakers produce and listeners interpret language in context.\n",
  "keywords": [
    
  ],
  "articleBody": "“… one of my avowed aims is to see talking as a special case or variety of purposive, indeed rational, behavior” (Grice, 1975: 47)\nI. Background of RSA The Rational Speech Act (RSA) framework was developed within the broader enterprise of Probabilistic Pragmatics, a rapidly growing approach in the study of meaning.\nProbabilistic pragmatics integrates insights from formal and experimental semantics and pragmatics, psycholinguistics, and computational cognitive science. It offers a unified framework for modeling how speakers produce and listeners interpret language in context.\nKey Characteristics of Probabilistic Pragmatics Formal Framework: Provides a structure for implementing hypotheses about how speakers contextually choose among utterance alternatives and how listeners arrive at context-sensitive interpretations. Formalizing Conversational Principles: Captures principles such as relevance, brevity, and helpful informativeness (closely aligned with Gricean maxims) in a formal and testable way. Probabilistic Processes: Treats language production and interpretation as fundamentally probabilistic, allowing for gradience and variability that classic models struggle to explain. Bounded Rationality: Models speakers and listeners as boundedly rational agents, integrating information in ways consistent with general cognitive constraints. Integration of Factors: Allows linguistic knowledge to interact with communicative pressures and subjective prior beliefs (world knowledge), acknowledging their central role in interpretation. Bridging Theoretical Traditions: Seeks to unify “language-as-product” (representational structure) and “language-as-action” (context-sensitive decision-making). Methodology: Strongly computational and data-driven—models are implemented as algorithms and tested against empirical data. Contrast with Classic Views: In contrast to categorical interpretations in classic models, probabilistic pragmatics treats meaning, informativeness, and alternatives as gradient and context-dependent. Types of Theories within Probabilistic Pragmatics Game-theoretic approaches (e.g., Benz \u0026 Stevens, 2018) Probabilistic but not fully Bayesian accounts (e.g., Qing \u0026 Franke, 2014; Russell, 2012) The Rational Speech Act (RSA) framework stands out as arguably the most influential probabilistic model of pragmatic interpretation, integrating many of the features described above.\nII. classical view of meaning vs. RSA view of meaning Aspect The Classic View of Meaning The RSA View of Meaning Nature of Interpretation Meaning is treated as categorical. Implicatures either arise or not; presuppositions either project or not. No room for gradience or interpretational uncertainty. Interpretation is probabilistic. Listeners hold posterior probability distributions over meanings, capturing uncertainty. The model supports variability and gradience in interpretation. Informativeness Defined categorically using entailment-based alternatives. One statement is more informative if it entails another. No contextual modulation. Defined gradually and contextually. Informativeness reflects how much a literal listener would learn after hearing the utterance (minimizing surprisal). It varies based on the context. Alternatives Treated as static and lexicalized (e.g., scales like ). The set of alternatives is fixed and not derived from context. Treated as flexible and context-sensitive. RSA is not a theory of alternatives but a framework for testing hypotheses. Alternatives can be adjusted based on data, and costly or complex alternatives can be penalized. World Knowledge (Prior Beliefs) Not integrated. Classic models often exclude prior beliefs or world knowledge from formal reasoning, viewing them as nonlinguistic. Formally integrated. RSA uses Bayes’ rule to model how listeners use subjective prior beliefs (world knowledge) to infer intended meaning. These priors do not need to be objectively accurate. Inference Process Gricean reasoning involves fixed premises. If these are met, an implicature is derived categorically; if not, it doesn’t arise. The speaker is assumed to choose the stronger alternative if it is true. RSA models interpretation as a signaling game. The speaker selects utterances balancing informativeness and cost. The listener uses Bayesian inference over utterances, context, and prior beliefs. This allows for gradient and variable inference. III. The basic/standard/vanilla RSA model The basic Rational Speech Act (RSA) model treats language use as a signaling game. Speakers and listeners are modeled as agents who reason about:\na space of utterances: $ U $ a space of possible meanings: $ M $ What Is a Signaling Game?\nThe RSA model builds on the idea that language use is a kind of signaling game — a concept borrowed from game theory (Lewis, 1969).\nA signaling game models communication as an interaction between two rational agents:\nA speaker (or sender) who wants to convey a particular meaning. A listener (or receiver) who observes the speaker’s utterance and tries to infer what meaning was intended. Key Ingredients of a Signaling Game\nRole Description Sender Knows some private information (e.g., a meaning) and sends a signal (utterance). Signal The utterance or message the speaker chooses to send. Receiver Observes the signal and makes an inference about the speaker’s intended meaning. Goal Successful communication: the listener correctly infers the speaker’s intended meaning. 3.1 Semantic Foundation (Denotation Functions) RSA begins with a literal semantics, defined by a denotation function (see SECTION 4.1 for more explanations):\n$$ \\llbracket \\cdot \\rrbracket : U \\rightarrow M $$\nThis function specifies the set of meanings that are literally compatible with each utterance. It is assumed to be shared by both speaker and listener, grounding the recursive pragmatic reasoning that follows.\n3.2 Recursive Probabilistic Rules Building on literal semantics, RSA defines a recursive hierarchy of production and interpretation rules. These involve probabilistic reasoning at each level and encode pragmatic inference.\n3.3 Literal Listener $ P_{L_0} $ The literal listener interprets utterances based on their literal meaning (See SECTION 4.2 for its components and calculations). Their interpretation rule is:\n$$ P_{L_0}(m \\mid u) = \\delta_{m \\in \\llbracket u \\rrbracket} \\cdot P(m) $$\nThis means the listener updates their prior beliefs $P(m)$ only for meanings that are literally compatible with the utterance u. Here, $ \\delta_{m \\in \\llbracket u \\rrbracket} $ is an indicator function that returns 1 if $ m \\in \\llbracket u \\rrbracket $, and 0 otherwise.\nThis step enforces the Gricean Quality maxim, ruling out meanings that are literally false.\n3.4 Pragmatic Speaker $\\ P_{S_1} $ The pragmatic speaker reasons about the literal listener’s interpretation to choose an utterance that best conveys the intended meaning $m$ (See SECTION 4.3 for more details). The speaker is modeled as a utility-maximizing agent:\n$$ P_{S_1}(u \\mid m) \\propto \\exp\\left(\\alpha \\cdot U(u, m)\\right) $$\nThe utility function balances informativeness and cost:\n$$ U(u, m) = \\log P_{L_0}(m \\mid u) - \\text{Cost}(u) $$\nInformativeness: How well $u$ communicates $m$, i.e., how likely the literal listener is to infer $m$. Cost: A penalty for utterances that are longer, less frequent, or harder to retrieve. α (alpha): A rationality parameter controlling how strongly the speaker optimizes utility. This step reflects Gricean Quantity (informativeness) and Manner (cost) maxims.\n3.5 Pragmatic Listener $P_{L_1}$ The pragmatic listener inverts the speaker model to infer the likely intended meaning:\n$$ P_{L_1}(m \\mid u) \\propto P_{S_1}(u \\mid m) \\cdot P(m) $$\nThis is a Bayesian inference over possible meanings, integrating:\nA model of the speaker’s utterance choice $ P_{S_1} $ The listener’s prior beliefs about likely meanings $ P(m) $ These priors encode world knowledge and subjective expectations about communicative goals.\nKey Insight “RSA models replace Grice’s maxims with a single, utility-theoretic version of the cooperative principle” (Goodman \u0026 Frank, 2016: 821)\nRSA treats language understanding as a probabilistic inference problem, in contrast to classical models which assume categorical interpretation. The pragmatic listener arrives at a posterior probability distribution over meanings, capturing the inherent uncertainty in language comprehension.\nThis recursive reasoning structure links production and interpretation in a unified, probabilistic framework.\nIV. More on the mathematical notations (for those not good at mathematical formalization, like me) (you may find it a bit repetitive) To facilitate the explanation, a Scalar Implicature Game is used as an example:\nBASIC SCALAR IMPLICATURE GAME In this context, there were Alex and 4 cookies on a plate.\nMeaning space: $ M = \\{ m_0, m_1, m_2, m_3, m_4 \\} $\nUtterance space: $ U = \\{ u_{\\text{all}}, u_{\\text{some}}, u_{\\text{none}} \\} $\nSemantics: $ u_{\\text{all}} = \\{ m_4 \\}$\n$ u_{\\text{some}} = \\{ m_1, m_2, m_3, m_4 \\} $\n$ u_{\\text{none}} = \\{ m_0 \\} $\nPrior beliefs: $P(m_0)$ = $P(m_1)$ = $P(m_2)$ = $P(m_3)$ = $P(m_4)$ = $0.2$\n4.1 Denotation Function The denotation function is a foundational concept in formal semantics and the Rational Speech Act (RSA) framework. In the RSA framework, the denotation function is written as:\n$$ \\llbracket \\cdot \\rrbracket : U \\rightarrow M $$ This means:\nThe denotation function maps each utterance $ u $ from the set of possible utterances $ U $ to a set of meanings $ M $ — specifically, the meanings for which the utterance is literally true.\n4.1.1 Step-by-Step Logic of the Indicator Step 1: Evaluate Whether $m$ Is Literally Compatible with $u$\nAsk: Does this meaning make the utterance true according to its literal meaning? Step 2: Output the Result\nIf YES → Output 1 (keep this meaning for further consideration). If NO → Output 0 (eliminate this meaning from consideration). 4.1.2 Meaning of Components An utterance $ u $ is something a speaker might say.\nExample:\n“Alex ate some of the cookies.”\nA meaning $ m $ refers to a possible state of the world — what might be true or false.\nFor example: (in a context where there were four cookies)\n$ m_0 $: Alex ate 0 cookie. $ m_1 $: Alex ate 1 cookie. $ m_2 $: Alex ate 2 cookies. $ m_3 $: Alex ate 3 cookies. $ m_4 $: Alex ate 4 cookies. These meanings represent different situations the utterance might refer to.\n4.1.3 What does the denotation function do? It defines which of those meanings ($m_0$, $m_1$, $m_2$, $m_3$, $m_4$) make the utterance literally true.\nFor the utterance: $ u = \\text{“Alex ate some of the cookies.”} $\nThe denotation is: $ \\llbracket u \\rrbracket = { m_1, m_2, m_3, m_4 } $ , because the statement is true in those situations, but not in $\\ m_{0} $, where Alex ate no cookies.\n4.1.4 Why Is This Important in RSA? It directly enforces Grice’s Quality Maxim at the literal level: Only meanings that make the utterance literally true are considered. It performs an efficient filtering step before any deeper probabilistic reasoning. This grounding in literal truth ensures that pragmatic reasoning starts from a shared semantic base. It makes the RSA framework compositional and grounded in literal semantics. 4.1.5 Summary Table Concept Explanation Utterance $ u $ A sentence a speaker might say (“Some of the apples are red”) Meaning $ m $ A possible state of the world (e.g., all red, some red, none red) Denotation $ \\llbracket u \\rrbracket $ The set of meanings for which the utterance is literally true Role in RSA Helps the literal listener filter out meanings that are literally impossible 4.2 Literal Listener $L_0$ The Literal Listener — denoted as $L_0$ — interprets an utterance based solely on its literal semantics, without considering the speaker’s goals or alternative utterances.\n4.2.1 Literal Listener Equations 1. Simplified (Proportional) Equation $$ P_{L_0}(m \\mid u) \\propto \\delta_{m \\in \\llbracket u \\rrbracket} \\cdot P(m) $$\nThis means:\nThe probability that the Literal Listener $L_0$ interprets utterance $u$ as meaning $m$ is proportional to whether the meaning $m$ is compatible with the literal semantics of $u$, multiplied by the prior probability of $ m $.\nThis computes unnormalized scores. 2. Full Normalized Equation $$ P_{L_0}(m \\mid u) = \\frac{\\delta_{m \\in \\llbracket u \\rrbracket} \\cdot P(m)}{\\displaystyle \\sum_{m’} \\delta_{m’ \\in \\llbracket u \\rrbracket} \\cdot P(m’)} $$\nThe denominator ensures that final probabilities sum to 1. $\\displaystyle \\sum_{m’}$ sums over all possible meanings $m’$. NOTE:\nDenominator\n$$ \\sum_{m’} \\delta_{m’ \\in \\llbracket u \\rrbracket} \\cdot P(m’) $$\nThis computes the total weight of all and only remaining possible meanings that make the utterance true (the meanings that make the utterance false would be left out). It’s the sum over all possible meanings $ m’ $ that are compatible with the utterance. This ensures the final probabilities sum to 1 — this is the normalization step. Why Are There Two Versions? Version Purpose Proportional Computes unnormalized values. Useful for comparing relative likelihoods before normalization. Normalized Computes final, interpretable probabilities that sum to 1. Required for reporting and decision-making. “$\\propto$” vs. “=”\n“$\\propto$” is used in the unnormalized equation. “$\\propto$” This form\"\" gives unnormalized scores — only the relative values matter. You must normalize later by dividing by the total. “=” means the result is a proper probability distribution. “=” is used in the normalized equation. 4.2.2 Explanation of Each Component (1) $ P_{L_0}(m \\mid u) $\nThis is the posterior belief of the Literal Listener:\nHow likely is it that the intended meaning is $ m $, given that the utterance $ u $ was heard?\nThis represents the listener’s literal interpretation of the utterance: $ \\llbracket u \\rrbracket $\n(2) $ \\llbracket u \\rrbracket $\nThe set of meanings where $ u $ is literally true.\n(3) $ \\delta_{m \\in \\llbracket u \\rrbracket} $\nThis is an indicator/delta function. It returns 1 if the meaning $m$ is compatible with the literal meaning of the utterance; 0 otherwise\nIn effect, this acts as a truth filter — it rules out meanings that are not literally possible.\n(4) $ P(m) $\nThis is the prior probability of each possible meaning, representing the listener’s expectations about the world before hearing the utterance.\n4.2.3 How the Literal Listener Computes Interpretation In the RSA model, the Literal Listener $ L_0 $ updates beliefs about the world after hearing an utterance $ u $, based purely on literal semantics and prior expectations.\nExample 1: “Alex ate some of the cookies.” Step 1: Define the Possible Meanings $M$ Meaning Description $ m_0 $ Alex ate 0 cookies $ m_1 $ Alex ate 1 cookie $ m_2 $ Alex ate 2 cookies $ m_3 $ Alex ate 3 cookies $ m_4 $ Alex ate 4 cookies Assume a uniform prior:\n$$ P(m_i) = 0.2 \\quad \\text{for all } i \\in {0, 1, 2, 3, 4} $$\nStep 2: Determine Literal Semantics (Define $\\llbracket u \\rrbracket$) Utterance:\n$ u = $ “Alex ate some of the cookies.”\nLiteral semantics:\n$$ \\llbracket u \\rrbracket = { m_1, m_2, m_3, m_4 } $$\nStep 3 Option 1: Calculation Using the Simplified (Proportional) Equation $$ P_{L_0}(m \\mid u) \\propto \\delta_{m \\in \\llbracket u \\rrbracket} \\cdot P(m) $$\nCompute Unnormalized Values: $$ \\delta_{m \\in \\llbracket u \\rrbracket} = \\begin{cases} 0 \u0026 \\text{if } m = m_0, \\\\ 1 \u0026 \\text{if } m \\in \\{ m_1, m_2, m_3, m_4 \\}. \\end{cases} $$ $P_{L_0}(m_0 \\mid u) = \\delta_{m_0 \\in \\llbracket u \\rrbracket} \\cdot P(m_0) = 0 \\cdot 0.2 = 0$ $P_{L_0}(m_i \\mid u) = \\delta_{m_i \\in \\llbracket u \\rrbracket} \\cdot P(m_i) = 1 \\cdot 0.2 = 0.2$ for $i \\in {1, 2, 3, 4}$ At this stage, these values are unnormalized scores.\nCompute the Denominator When You Normalize The sum of unnormalized scores is the denominator: $$ \\text{Denominator} = 0.2 + 0.2 + 0.2 + 0.2 = 0.8 $$\nFinal Step: Normalize to Get Probabilities $$ P_{L_0}(m_i \\mid u) = \\frac{\\text{Unnormalized Score}}{\\text{Denominator}} = \\frac{0.2}{0.8} = 0.25 \\quad \\text{for } m_i \\in {1, 2, 3, 4} $$\nOption 2: Calculation Using the Full Normalized Equation $$ P_{L_0}(m \\mid u) = \\frac{\\delta_{m \\in \\llbracket u \\rrbracket} \\cdot P(m)}{\\displaystyle \\sum_{m’} \\delta_{m’ \\in \\llbracket u \\rrbracket} \\cdot P(m’)} $$\nCompute Numerators: Meaning $\\delta_{m \\in \\llbracket u \\rrbracket}$ $P(m)$ Numerator ($\\delta \\cdot P(m)$) $m_0$ 0 (ruled out) 0.2 0 $m_1$ 1 0.2 0.2 $m_2$ 1 0.2 0.2 $m_3$ 1 0.2 0.2 $m_4$ 1 0.2 0.2 Compute Denominator (Normalization Constant): $$ \\displaystyle \\sum_{m’} \\delta_{m’ \\in \\llbracket u \\rrbracket} \\cdot P(m’) = P(m_1) + P(m_2) + P(m_3) + P(m_4) = 0.2 + 0.2 + 0.2 + 0.2 = 0.8 $$\nCompute Final Probabilities: For $m_0$ (ruled out):\n$$ P_{L_0}(m_0 \\mid u) = 0 $$ For $m_i$ where $i \\in {1, 2, 3, 4}$:\n$$ P_{L_0}(m_i \\mid u) = \\frac{0.2}{0.8} = 0.25 $$ Final Interpretation Meaning Final Probability $P_{L_0}(m \\mid u)$ $m_0$: 0 cookies 0.0 (ruled out) $m_1$: 1 cookie 0.25 $m_2$: 2 cookies 0.25 $m_3$: 3 cookies 0.25 $m_4$: 4 cookies 0.25 Example 2: “Alex ate none of the cookies.” Step 1: Define the Possible Meanings $M$ Meaning Description $ m_0 $ Alex ate 0 cookies $ m_1 $ Alex ate 1 cookie $ m_2 $ Alex ate 2 cookies $ m_3 $ Alex ate 3 cookies $ m_4 $ Alex ate 4 cookies Assume a uniform prior:\n$$ P(m_i) = 0.2 \\quad \\text{for all } i \\in {0, 1, 2, 3, 4} $$\nStep 2: Determine Literal Semantics (Define $\\llbracket u \\rrbracket$) Utterance:\n$ u = $ “Alex ate none of the cookies.”\nLiteral semantics:\n$$ \\llbracket u \\rrbracket = { m_0 } $$\nCalculation Using the Simplified (Proportional) Equation (option 1) $$ P_{L_0}(m \\mid u) \\propto \\delta_{m \\in \\llbracket u \\rrbracket} \\cdot P(m) $$\nCompute Unnormalized Values: Meaning $\\delta_{m \\in \\llbracket u \\rrbracket}$ $P(m)$ Unnormalized Score $m_0$ 1 (kept) 0.2 0.2 $m_1$–$m_4$ 0 (ruled out) 0.2 0 At this stage, the values are unnormalized scores.\nCalculation Using the Full Normalized Equation (option 2) $$ P_{L_0}(m \\mid u) = \\frac{\\delta_{m \\in \\llbracket u \\rrbracket} \\cdot P(m)}{\\displaystyle \\sum_{m’} \\delta_{m’ \\in \\llbracket u \\rrbracket} \\cdot P(m’)} $$\nCompute the Denominator (Normalization Constant): $$ \\text{Total} = P(m_0) = 0.2 $$\nCompute Final Probabilities: For $m_0$:\n$$ P_{L_0}(m_0 \\mid u) = \\frac{0.2}{0.2} = 1.0 $$ For all other $m_i$:\n$$ P_{L_0}(m_i \\mid u) = 0 \\quad \\text{for } i \\in {1, 2, 3, 4} $$ Final Interpretation Meaning Final Probability $P_{L_0}(m \\mid u)$ $ m_0 $: 0 cookies 1.0 (certain) $ m_1 $ to $ m_4 $ 0.0 (ruled out) Example 3: “Alex ate all of the cookies.” Step 1: Define the Possible Meanings $M$ Meaning Description $ m_0 $ Alex ate 0 cookies $ m_1 $ Alex ate 1 cookie $ m_2 $ Alex ate 2 cookies $ m_3 $ Alex ate 3 cookies $ m_4 $ Alex ate 4 cookies Assume a uniform prior:\n$$ P(m_i) = 0.2 \\quad \\text{for all } i \\in {0, 1, 2, 3, 4} $$\nStep 2: Determine Literal Semantics (Define $\\llbracket u \\rrbracket$) Utterance:\n$ u = $ “Alex ate all of the cookies.”\nLiteral semantics:\n$$ \\llbracket u \\rrbracket = { m_4 } $$\nCalculation Using the Simplified (Proportional) Equation (option 1) $$ P_{L_0}(m \\mid u) \\propto \\delta_{m \\in \\llbracket u \\rrbracket} \\cdot P(m) $$\nCompute Unnormalized Values Meaning $\\delta_{m \\in \\llbracket u \\rrbracket}$ $P(m)$ Unnormalized Score $m_4$ 1 (kept) 0.2 0.2 $m_0$–$m_3$ 0 (ruled out) 0.2 0 At this stage, the values are unnormalized scores.\nCalculation Using the Full Normalized Equation (option 2) $$ P_{L_0}(m \\mid u) = \\frac{\\delta_{m \\in \\llbracket u \\rrbracket} \\cdot P(m)}{\\displaystyle \\sum_{m’} \\delta_{m’ \\in \\llbracket u \\rrbracket} \\cdot P(m’)} $$\nCompute the Denominator (Normalization Constant): $$ \\text{Total} = P(m_4) = 0.2 $$\nCompute Final Probabilities: For $m_4$:\n$$ P_{L_0}(m_4 \\mid u) = \\frac{0.2}{0.2} = 1.0 $$ For all other $m_i$:\n$$ P_{L_0}(m_i \\mid u) = 0 \\quad \\text{for } i \\in {0, 1, 2, 3} $$ Final Interpretation Meaning Final Probability $P_{L_0}(m \\mid u)$ $ m_4 $: 4 cookies 1.0 (certain) $ m_0 $ to $ m_3 $ 0.0 (ruled out) The simplified equation identifies that only $m_4$ is possible but doesn’t directly provide the final, normalized probability. The full equation confirms that after normalization, the listener is completely certain the intended meaning is $m_4$. What These Examples Show Utterance Remaining Possible Meanings Final Distribution “Some” $ m_1 $ to $ m_4 $ Evenly distributed (0.25 each) “None” $ m_0 $ only Certain (prob = 1) “All” $ m_4 $ only Certain (prob = 1) 4.2.3 A more detailed example based on the full equation (for “Alex ate some of the cookies.”) What Are $m$ and $m’$ in the Literal Listener Formula? $$ P_{L_0}(m \\mid u) = \\frac{\\delta_{m \\in \\llbracket u \\rrbracket} \\cdot P(m)}{\\displaystyle \\sum_{m’} \\delta_{m’ \\in \\llbracket u \\rrbracket} \\cdot P(m’)} $$\nSymbol Meaning $m$ A specific possible meaning the listener is considering. Example: “Alex ate 2 cookies.” $m'$ A placeholder variable for summing over all possible meanings. The numerator computes the contribution of one particular meaning $m$. The denominator sums over all meanings $m’$ to calculate the normalization constant. Example: “Alex ate some of the cookies” Step 1: Define Possible Meanings Meaning Description Prior $P(m)$ $m_0$ Alex ate 0 cookies 0.2 $m_1$ Alex ate 1 cookie 0.2 $m_2$ Alex ate 2 cookies 0.2 $m_3$ Alex ate 3 cookies 0.2 $m_4$ Alex ate 4 cookies 0.2 Step 2: Determine Literal Semantics $$ \\llbracket \\text{“some”} \\rrbracket = { m_1, m_2, m_3, m_4 } $$\n“Some” means at least one cookie was eaten, so $m_0$ is ruled out. Step 3: Compute the Denominator (Normalization) $$ \\sum_{m’} \\delta_{m’ \\in \\llbracket u \\rrbracket} \\cdot P(m’) = 1 * P(m_1) + 1 * P(m_2) + 1 * P(m_3) + 1 * P(m_4) $$\n$$ \\text{Total} = 1 * 0.2 + 1 * 0.2 + 1 * 0.2 + 1 * 0.2 = 0.8 $$\nStep 4: Compute Final Probabilities for Each Meaning $$ P_{L_0}(m \\mid u) = \\frac{P(m)}{0.8} \\quad \\text{if } m \\in \\llbracket u \\rrbracket $$\nMeaning $P_{L_0}(m \\mid u)$ Calculation Final Probability $P_{L_0}(m \\mid u)$ $m_0$ Ruled out by literal meaning 0.0 $m_1$ $\\frac{0.2}{0.8}$ 0.25 $m_2$ $\\frac{0.2}{0.8}$ 0.25 $m_3$ $\\frac{0.2}{0.8}$ 0.25 $m_4$ $\\frac{0.2}{0.8}$ 0.25 $m$: The specific meaning you are evaluating. $m’$: The variable used to sum over all meanings during normalization. The literal listener rules out incompatible meanings and redistributes prior beliefs over the remaining possibilities. 4.2.4 Why normalize? In the RSA model (and probabilistic modeling more broadly), normalization is the process of turning raw or unnormalized scores into a valid probability distribution — one where all values are between 0 and 1 and sum to exactly 1.\nProbabilities must satisfy two conditions: (1) They must be non-negative; (2) They must sum to 1.\nHowever, RSA agents (like the Literal Listener or Pragmatic Speaker) often compute values that are proportional to probabilities, not properly normalized. So we apply normalization to make them valid.\nNormalization is needed whenever probabilities are defined up to proportionality.\nNormalization:\n(1) Converts raw scores into valid probabilities\n(2) Filters and redistributes prior beliefs based on literal truth\n(3) Is essential for computing meaningful results in RSA and Bayesian models\nIt is the final — and often implicit — step in probabilistic interpretation.\n4.2.5 Summary The Literal Listener does not reason about why the speaker chose one utterance over another. It simply filters meanings based on:\nLiteral truth conditions (defined by $ \\llbracket u \\rrbracket $) Prior beliefs about what is likely This provides the foundation for higher levels of reasoning in RSA, where more sophisticated listeners and speakers reason about each other recursively.\n4.3 Pragmatic Speaker $ S_1 $ The Pragmatic Speaker (denoted as $ S_1 $) models the speaker as a rational agent who chooses utterances strategically. The speaker’s goal is to:\nCommunicate the intended meaning effectively (informativeness). Minimize production effort or cost (cost). This balances the Gricean Quantity Maxim (be informative) and the Manner Maxim (avoid unnecessary effort).\n4.3.1 The Pragmatic Speaker Formula 1. Simplified (Proportional) Equation $$ P_{S_1}(u \\mid m) \\propto \\exp \\left( \\alpha \\cdot U(u, m) \\right) $$\nwhere $ U(u, m) = \\log P_{L_0}(m \\mid u) - \\text{cost}(u) $\nThis means:\n“The probability that the Pragmatic Speaker $ S_1 $ will produce utterance $ u $ given that they want to communicate meaning $ m $, is proportional to the exponential of $ \\alpha $ times the utility of utterance $ u $ for meaning $ m $.”\nThis formula defines the speaker’s production $ u $ as softmax optimizing $ u $’s utility for communicating $ m $, $ U (u, m) $.\nMeaning of its components\n$ P_{S_1}(u \\mid m) $: Probability the speaker chooses utterance $ u $ to communicate meaning $ m $. $ \\alpha $: Rationality parameter controlling how strongly the speaker optimizes utility. $ U(u, m) $: Utility of utterance $ u $ for communicating meaning $ m $. $ \\propto $ is read as “proportional to”. It is used in equations to express that one quantity scales with another, but the exact value is not yet determined — because we still need to compute a normalization step. This computes unnormalized scores for each utterance $u$. 2. Full Normalized Equation (Softmax Function) $$ P_{S_1}(u \\mid m) = \\frac{\\exp \\left( \\alpha \\cdot U(u, m) \\right)}{\\displaystyle \\sum_{u’} \\exp \\left( \\alpha \\cdot U(u’, m) \\right)} $$\nThis computes the final probabilities by normalizing the unnormalized scores. The denominator ensures that probabilities sum to 1. 4.3.2 Understanding Utility $U(u, m)$ The utility function balances informativeness and cost:\nAn utterance’s utility $ U (u, m) $ is defined as a trade-off between the utterance’s informativeness as characterized by $ P_{L_0}(m \\mid u) $ –how likely it is that a literal listener will corectly infer $ m $ from $ u $’s literal semantics alone– and its cost, as defined in the utility function:\n$$ U(u, m) = \\log P_{L_0}(m \\mid u) - \\text{cost}(u) $$\n$ \\log P_{L_0}(m \\mid u) $: Informativeness — how well the literal listener would infer $ m $ from $ u $. $ \\text{cost}(u) $: Cost of producing utterance $ u $ (e.g., effort, complexity, length). How Does This Work? (1) Informativeness:\nThe informativeness term captures the spirit of the Gricean Quantity Maxims (even Relation). If the utterance makes the intended meaning very likely for the literal listener, its utility is high. Example: “All the cookies were eaten” perfectly conveys the state where all cookies are gone. (2) Cost:\nThe cost term captures the spirit of part of the Gricean Maxim of Mnner: the cheaper (e.g., shorter) the utterance, the better. Longer or more complex utterances may have higher cost. Example: “Alex ate all four cookies” might be costlier than simply saying “Alex ate all.” (3) Balancing:\nThe pragmatic speaker prefers utterances that are informative but low in cost. If two utterances are equally informative, the speaker prefers the cheaper one. 4.3.3 The Rationality Parameter $\\alpha$ $ \\alpha $ is a utlity-scaling parameter, which governs the extent to which the speaker is a utility-maximizing agent. If $ \\alpha = 0 $: The speaker chooses utterances randomly. If $ \\alpha = 1 $: The speaker chooses utterances proportionally to utility (probability matching). If $ \\alpha \\to \\infty $: The speaker ceases to choose utterances probabilistically and always chooses the utterance with the highest utility (fully rational). This softmax function makes the model flexible, allowing for varying levels of rationality in speaker behavior.\nTo compute pragmatic speaker probabilities, we must set a value for $ \\alpha $ and define the cost of utterances.\n$\\alpha$ Value Speaker Behavior 0 Random utterance choice. 1 Probability matches utility. $\\to \\infty$ Always chooses the utterance with the highest utility. 4.3.4 Understanding $\\propto$ and Why Normalize? $\\propto$ means “proportional to”.\nIt indicates the values are relative but not yet normalized.\nThe probability of choosing utterance $ u $ to express meaning $ m $ is proportional to the exponentiated utility, but this is not yet the final probability.\nWhy Normalize? Probabilities must sum to 1. The full normalized equation ensures this by dividing unnormalized scores by their total sum. 4.3.5 Worked Example: Cookie Scenario Suppose the speaker wants to communicate that “Alex ate all the cookies”, i.e., $m_4$.\nStep 1: Compute Literal Listener Beliefs $$ P_{L_0}(m_4 \\mid u_{\\text{all}}) = 1.0, \\quad P_{L_0}(m_4 \\mid u_{\\text{some}}) = 0.25 $$\ne.g.: Computing $P_{L_0}(m_4 \\mid u_{\\text{some}}) = 0.25$\nDefine the Possible Meanings $M$: $ m_0 $ = 0 cookies; $ m_1 $ = 1 cookies; $ m_2 $ = 2 cookies; $ m_3 $ = 3 cookies; $ m_4 $ = 4 cookies;\nuniform priors: $ P(m_i) = 0.2 \\quad \\text{for all } i \\in {0, 1, 2, 3, 4} $\nStep 2: Determine Literal Semantics (Define $\\llbracket u \\rrbracket$) Utterance:\n$ u = $ “Alex ate some of the cookies.”\nLiteral semantics:\n$$ \\llbracket u \\rrbracket = { m_1, m_2, m_3, m_4 } $$\nStep 3: Calculation Using the Simplified (Proportional) Equation $$ P_{L_0}(m \\mid u) \\propto \\delta_{m \\in \\llbracket u \\rrbracket} \\cdot P(m) $$\nCompute Unnormalized Values: $$ \\delta_{m \\in \\llbracket u \\rrbracket} = \\begin{cases} 0 \u0026 \\text{if } m = m_0, \\\\ 1 \u0026 \\text{if } m \\in \\{ m_1, m_2, m_3, m_4 \\}. \\end{cases} $$ $P_{L_0}(m_i \\mid u) = \\delta_{m_i \\in \\llbracket u \\rrbracket} \\cdot P(m_i) = 1 \\cdot 0.2 = 0.2$ for $i \\in {1, 2, 3, 4}$\n$P_{L_0}(m_4 \\mid u_{some}) = \\delta_{m_4 \\in \\llbracket u_{some} \\rrbracket} \\cdot P(m_4) = 1 \\cdot 0.2 = 0.2$\nAt this stage, these values are unnormalized scores.\nCompute the Denominator When You Normalize The sum of unnormalized scores is the denominator (all meanings $m_i$ that make $u_{some}$ true): $$ \\text{Denominator} = 0.2 + 0.2 + 0.2 + 0.2 = 0.8 $$\nFinal Step: Normalize to Get Probabilities $$ P_{L_0}(m_4 \\mid u) = \\frac{\\text{Unnormalized Score}}{\\text{Denominator}} = \\frac{0.2}{0.8} = 0.25 $$\nStep 2: Compute Utilities Assume $\\text{cost}(u) = 0$ for simplicity.\n$U(u_{\\text{all}}, m_4) = \\log P_{L_0}(m_4 \\mid u_{all}) - \\text{cost}(u_{all}) = \\log(1.0) - 0 = 0 - 0 = 0$ $U(u_{\\text{some}}, m_4) = \\log P_{L_0}(m_4 \\mid u_{some}) - \\text{cost}(u_{some}) = \\log(0.25) - 0 \\approx -1.386 -0 \\approx -1.386 $ Step 3: Compute Unnormalized Scores (Assume $\\alpha = 1$) $P_{S_1}(u_{all} \\mid m_4) \\propto \\exp \\left( \\alpha \\cdot U(u_{all}, m_4) \\right) \\propto \\exp(1 \\cdot 0) \\propto \\exp(0) = 1$ $P_{S_1}(u_{some} \\mid m_4) \\propto \\exp \\left( \\alpha \\cdot U(u_{some}, m_4) \\right) \\propto \\exp(1 \\cdot -1.386) \\propto \\exp(-1.386) \\approx 0.25$ Step 4: Normalize $$ \\text{Total} = 1 + 0.25 = 1.25 $$\n$P_{S_1}(u_{\\text{all}} \\mid m_4) = \\frac{1}{1.25} = 0.8$ $P_{S_1}(u_{\\text{some}} \\mid m_4) = \\frac{0.25}{1.25} = 0.2$ Final Interpretation The speaker is four times more likely to say “all” than “some” when Alex ate all the cookies. 4.3.6 A Complete Example: Cookies Scenario Possible Meanings (M):\n$ m_1 $: Alex ate all the cookies. $ m_2 $: Alex ate some but not all cookies. $ m_3 $: Alex ate none of the cookies. Possible Utterances (U):\n$ u_{\\text{some}} $: “Alex ate some cookies.” $ u_{\\text{all}} $: “Alex ate all the cookies.” Step 1: Compute Literal Listener’s Belief Assume:\n$$ P_{L_0}(m_1 \\mid u_{\\text{all}}) = 1.0,\\quad P_{L_0}(m_1 \\mid u_{\\text{some}}) = 0.25 $$\nThis means:\n“All” perfectly communicates that Alex ate all cookies. “Some” leaves uncertainty. Step 2: Compute Utility Assume cost is zero for simplicity.\n$$ U(u_{\\text{all}}, m_1) = \\log(1.0) = 0 $$\n$$ U(u_{\\text{some}}, m_1) = \\log(0.25) = -1.386 $$\nStep 3: Compute Production Probabilities (Assume $ \\alpha = 1 $) $$ P_{S_1}(u_{\\text{all}} \\mid m_1) \\propto \\exp(1 \\cdot 0) = 1 $$\n$$ P_{S_1}(u_{\\text{some}} \\mid m_1) \\propto \\exp(1 \\cdot -1.386) \\approx 0.25 $$\nNormalize:\n$$ P_{S_1}(u_{\\text{all}} \\mid m_1) = \\frac{1}{1 + 0.25} = 0.8 $$ $$ P_{S_1}(u_{\\text{some}} \\mid m_1) = \\frac{0.25}{1 + 0.25} = 0.2 $$\nInterpretation:\nThe speaker is four times more likely to say “all” than “some” when Alex ate all cookies. 4.3.7 How to understand Figure 2? Figure 2: RSA Speaker Production Probabilities Figure 2 in Degen (2023) visualizes how the Pragmatic Speaker’s production probabilities change as a function of the rationality parameter $\\alpha$, assuming the intended meaning is that Alex ate all the cookies ($m_4$).\nRSA Model Equation for the Pragmatic Speaker $$ P_{S_1}(u \\mid m) \\propto \\exp \\left( \\alpha \\cdot U(u, m) \\right) $$\n$U(u, m)$ is the utility of utterance $u$ for expressing meaning $m$. $\\alpha$ controls how strongly the speaker favors high-utility utterances. Utility Calculation $$ U(u, m) = \\log P_{L_0}(m \\mid u) - \\text{cost}(u) $$\nFor the meaning $m_4$ (Alex ate all cookies): $P_{L_0}(m_4 \\mid u_{\\text{all}}) = 1$ (perfectly informative) $P_{L_0}(m_4 \\mid u_{\\text{some}}) = 0.25$ (less informative) How to Read the Figure Y-Axis: Probability of choosing each utterance ($P_{S_1}(u \\mid m_4)$). X-Axis: Value of the rationality parameter $\\alpha$. Low $\\alpha$: The speaker is less rational and chooses utterances almost randomly. High $\\alpha$: The speaker strongly prefers the more informative utterance (“all”). As $\\alpha$ increases, the model predicts the speaker will overwhelmingly prefer to say “all” rather than “some”.\nWhat This Demonstrates The figure shows the effect of the softmax function:\nHigher utilities lead to higher probabilities, but this relationship is smoothed and modulated by $\\alpha$. This accounts for graded, probabilistic behavior rather than hard, deterministic choices. The RSA model predicts that humans adjust their speech behavior depending on context, goals, and cognitive resources (modeled by $\\alpha$). 4.3.8 Understanding $\\exp()$ in the Pragmatic Speaker Equation In the Pragmatic Speaker formula:\n$$ P_{S_1}(u \\mid m) \\propto \\exp\\left( \\alpha \\cdot U(u, m) \\right) $$\nThe $\\exp()$ function plays a critical role in transforming utilities into positive values suitable for computing probabilities.\nWhat Does $\\exp()$ Mean? $\\exp(x)$ is the exponential function, equivalent to $e^x$, where $e \\approx 2.718$. It converts utility values (which can be negative or positive) into positive scores. This ensures that all computed scores for probabilities remain non-negative, which is required for valid probability calculations. Why Use $\\exp()$? Transforms Utilities into Positive Scores\nSince utilities can be negative, exponentiation ensures that all scores used for probability calculations are positive. Amplifies Differences Between Utilities\nLarger utilities result in exponentially larger scores, making higher-utility utterances significantly more likely. Creates Smooth, Graded Probabilities (Softmax Function)\nEven utterances with lower utility still have a non-zero chance of being selected, depending on the value of $\\alpha$. Example Calculations Utility $U(u, m)$ $\\exp(U(u, m))$ Interpretation 0 1.0 Baseline value. -1.386 0.25 Lower utility, lower score. 2 $\\exp(2) \\approx 7.389$ High utility, very large score. Key Takeaways $\\exp()$ ensures that probabilities are positive and differentiable, enabling the use of softmax for probabilistic choices. It reflects the intuition that the speaker is exponentially more likely to choose high-utility utterances, but not exclusively. This function, combined with $\\alpha$, controls how deterministic or probabilistic the speaker’s choices are. 4.3.9 Summary Table Component Meaning $ P_{S_1}(u \\mid m) $ Speaker’s probability of choosing $ u $ given $ m $ $ U(u, m) $ Utility of utterance $ u $ for meaning $ m $ $ \\alpha $ Rationality parameter controlling how deterministic the speaker is Cost $ \\text{cost}(u) $ Production effort or complexity of the utterance In sum, the pragmatic speaker chooses utterances strategically, balancing informativeness and cost. This formalization explains why speakers might sometimes avoid perfectly informative utterances when they are costly, or why they might choose simpler alternatives when the difference in informativeness is\n4.4 Pragmatic Listener $ L_1 $ The Pragmatic Listener in RSA, often denoted as $L_1$, performs Bayesian inference to reason about what meaning $m$ the speaker likely intended after hearing the utterance $u$.\nThis listener doesn’t just rely on literal meaning; it also considers how likely the speaker would have chosen the utterance $u$ under each possible meaning $m$.\n4.4.1 Equation for Pragmatic Listener $ L_1 $ 1. Simplified (Proportional) Form $$ P_{L_1}(m \\mid u) \\propto P_{S_1}(u \\mid m) \\cdot P(m) $$\nThis tells us that the posterior belief about meaning $m$ is proportional to:\nThe probability that the pragmatic speaker would choose $u$ to express $m$. The prior probability of meaning $m$ before hearing the utterance. This equation computes unnormalized scores. To turn them into probabilities, we need to normalize.\n2. Full Normalized Equation $$ P_{L_1}(m \\mid u) = \\frac{P_{S_1}(u \\mid m) \\cdot P(m)}{\\sum_{m’} P_{S_1}(u \\mid m’) \\cdot P(m’)} $$\nThe denominator ensures that the final probabilities sum to 1. $\\sum_{m’}$ sums over all possible meanings $m’$. Explanation of Components Component Explanation $P_{L_1}(m \\mid u)$ The posterior probability that the intended meaning is $m$ after hearing utterance $u$. $P_{S_1}(u \\mid m)$ The pragmatic speaker’s probability of choosing utterance $u$ when intending meaning $m$. Computed via softmax over utilities. $P(m)$ The prior probability of meaning $m$ before hearing any utterance (reflects world knowledge or expectations). Denominator (Normalization) Ensures the final probabilities sum to 1 by dividing by the total unnormalized probability mass. How Does This Work Conceptually? The listener hears $u$ and asks:\n“How likely would a rational speaker have said this if they intended each possible meaning?”\nThe listener also considers how likely each meaning was before hearing $u$ (using the prior $P(m)$).\nCombining these two factors, the listener updates their beliefs and computes the final probabilities over meanings.\nWhy Are There Two Versions of the Equation? Version Purpose Proportional Used to calculate relative likelihoods before normalization. Efficient for intermediate steps. Normalized Produces valid probabilities that sum to 1. Required for final interpretation and reporting results. 4.4.2 Case Study: Scalar Implicature for “Some” (Likelihood of $m_4$ being true when $u_{\\text{some}}$ is heard) Utterance space:\n$U = { u_{\\text{all}}, u_{\\text{some}}, u_{\\text{none}} }$\nPossible meanings:\n$M = { m_0, m_1, m_2, m_3, m_4 }$\n$m_0$: Alex ate 0 cookies $m_1$: Alex ate 1 cookie $m_2$: Alex ate 2 cookies $m_3$: Alex ate 3 cookies $m_4$: Alex ate all 4 cookies Assume a uniform prior:\n$P(m_i) = 0.2$ for all $i$.\nUtterance heard:\n$u = u_{\\text{some}}$ (“Alex ate some of the cookies.”)\nStep 1: Compute the Pragmatic Speaker’s Probabilities of choosing an utterance when $m_4$ is true [$P_{S_1}(u \\mid m_4)$] Literal Listener’s Interpretations: For $u_{\\text{all}}$:\n$P_{L_0}(m_4 \\mid u_{\\text{all}}) = \\delta_{m_4 \\in \\llbracket u_{\\text{all}} \\rrbracket} \\cdot P(m_4) = 1 \\cdot 0.2 = 0.2 $ (This is Unnormalized probability)\nNormalization Step\n$$ \\sum_{m’} \\delta_{m’ \\in \\llbracket u_{\\text{all}} \\rrbracket} \\cdot P(m’) = P(m_4) = 0.2 $$\nFinal probability:\n$$ P_{L_0}(m_4 \\mid u_{\\text{all}}) = \\frac{0.2}{0.2} = 1.0 $$\nStep-by-Step Expansion\nRecall that $\\llbracket u_{\\text{all}} \\rrbracket = { m_4 }$, because the utterance “all” is only literally true if Alex ate all the cookies.\nSo the indicator function $\\delta_{m’ \\in \\llbracket u_{\\text{all}} \\rrbracket}$ evaluates as:\n$m'$ $\\delta_{m’ \\in \\llbracket u_{\\text{all}} \\rrbracket}$ $P(m’)$ Contribution $m_0$ 0 (ruled out) 0.2 $0 \\cdot 0.2 = 0$ $m_1$ 0 (ruled out) 0.2 $0 \\cdot 0.2 = 0$ $m_2$ 0 (ruled out) 0.2 $0 \\cdot 0.2 = 0$ $m_3$ 0 (ruled out) 0.2 $0 \\cdot 0.2 = 0$ $m_4$ 1 (kept) 0.2 $1 \\cdot 0.2 = 0.2$ Summing all contributions: $$ \\text{Total} = 0 + 0 + 0 + 0 + 0.2 = 0.2 $$\nFor $u_{\\text{some}}$\n$P_{L_0}(m_4 \\mid u_{some}) = \\delta_{m_4 \\in \\llbracket u_{some} \\rrbracket} \\cdot P(m_4) = 1 \\cdot 0.2 = 0.2 $ (This is Unnormalized probability) Normalization Step\n$$ \\sum_{m’} \\delta_{m’ \\in \\llbracket u_{\\text{all}} \\rrbracket} \\cdot P(m’) = P(m_1)+P(m_2)+P(m_3) +P(m_4) = 0.2 + 0.2 + 0.2 + 0.2 = 0.8 $$\nFinal probability:\n$$ P_{L_0}(m_4 \\mid u_{\\text{some}}) = \\frac{0.2}{0.8} = 0.25 $$\nUtility Calculation: Using $U(u, m) = \\log P_{L_0}(m \\mid u) - cost(u)$ and assuming no cost:\n$U(u_{all}, m_4) = \\log P_{L_0}(m_4 \\mid u_{all}) - cost(u_{all})= \\log(1) - 0 = 0$ $U(u_{some}, m_4) = \\log P_{L_0}(m_4 \\mid u_{some}) - cost(u_{some})= \\log(0.25) - 0 \\approx -1.386 - 0 \\approx -1.386$ Compute Pragmatic Speaker $P_{S_1}(u \\mid m_4)$: Unnormalized:\n$P_{S_1}(u_{all} \\mid m_4) \\propto \\exp \\left( \\alpha \\cdot U(u_{all}, m_4) \\right) \\propto \\exp \\left( 1 \\cdot 0 \\right) \\propto \\exp(0) = 1$ $P_{S_1}(u_{\\text{some}} \\mid m_4) \\propto \\exp \\left( \\alpha \\cdot U(u_{some}, m_4) \\right) \\propto \\exp \\left( 1 \\cdot -1.386 \\right) \\propto \\exp(-1.386) \\approx 0.25$ Normalize:\nTotal = $1 + 0.25 = 1.25$ $P_{S_1}(u_{\\text{all}} \\mid m_4) = \\frac{1}{1.25} = 0.8$ $P_{S_1}(u_{\\text{some}} \\mid m_4) = \\frac{0.25}{1.25} = 0.2$ Step 2: Compute the Pragmatic Listener’s Posterior $P_{L_1}(m \\mid u_{\\text{some}})$ Apply Bayes’ Rule:\n$$ P_{L_1}(m \\mid u_{\\text{some}}) \\propto P_{S_1}(u_{\\text{some}} \\mid m) \\cdot P(m) $$\nCompute unnormalized scores for all meanings:\nMeaning $P_{S_1}(u_{\\text{some}} \\mid m)$ $P(m)$ Product $m_0$ 0 (ruled out by literal meaning) 0.2 0.0 $m_1$ Assume 0.3 0.2 0.06 $m_2$ Assume 0.3 0.2 0.06 $m_3$ Assume 0.3 0.2 0.06 $m_4$ 0.2 (computed above) 0.2 0.04 Total sum = $0.06 + 0.06 + 0.06 + 0.04 = 0.22$\nFinal Posterior Probabilities: Meaning Final $P_{L_1}(m \\mid u_{\\text{some}})$ $m_0$ 0.0 (ruled out) $m_1$ $\\frac{0.06}{0.22} \\approx 0.273$ $m_2$ $\\frac{0.06}{0.22} \\approx 0.273$ $m_3$ $\\frac{0.06}{0.22} \\approx 0.273$ $m_4$ $\\frac{0.04}{0.22} \\approx 0.182$ Interpretation After hearing “some of the cookies were eaten,” the listener assigns the highest probability to $m_1$ (Alex ate only 1 cookie). Although $m_4$ is possible, it’s less likely because if Alex had eaten all the cookies, the speaker would have likely said “all” instead. This demonstrates how the RSA model formally derives the scalar implicature that “some” often implies “not all.” V. A complete illustration of pragmatic reasoning in RSA This section, building on the previous sections, offers a complete demonstration of how listensers interpret scalar utterances, like “Alex ate some of the cookies.”\nCommon Setup and Assumptions Meaning Description Prior $P(m)$ $m_0$ Alex ate 0 cookies 0.2 $m_1$ Alex ate 1 cookie 0.2 $m_2$ Alex ate 2 cookies 0.2 $m_3$ Alex ate 3 cookies 0.2 $m_4$ Alex ate 4 cookies 0.2 Utterance space:\n$$ U = { u_{\\text{none}}, u_{\\text{some}}, u_{\\text{all}} } $$\nLiteral Semantics:\n$\\llbracket u_{\\text{none}} \\rrbracket = { m_0 }$ $\\llbracket u_{\\text{some}} \\rrbracket = { m_1, m_2, m_3, m_4 }$ $\\llbracket u_{\\text{all}} \\rrbracket = { m_4 }$ Assume $\\alpha = 1$ and $\\text{cost}(u) = 0$ for all utterances.\nREASONING LEVEL #1 Literal Listener ($L_0$) Step 1: Apply the Full Equation $$ P_{L_0}(m \\mid u_{\\text{some}}) = \\frac{\\delta_{m \\in \\llbracket u_{\\text{some}} \\rrbracket} \\cdot P(m)}{\\displaystyle \\sum_{m’} \\delta_{m’ \\in \\llbracket u_{\\text{some}} \\rrbracket} \\cdot P(m’)} $$\nCompute Denominator:\n$$ \\text{Total} = 0.2 + 0.2 + 0.2 + 0.2 = 0.8 $$\nCompute Final Probabilities:\n$P_{L_0}(m_0 \\mid u_{\\text{some}}) = 0$ $P_{L_0}(m_i \\mid u_{\\text{some}}) = \\frac{0.2}{0.8} = 0.25$ for $i \\in {1, 2, 3, 4}$ $L_0$ Final Belief Distribution Meaning $P_{L_0}(m \\mid u_{\\text{some}})$ $m_0$ 0.0 (ruled out) $m_1$ 0.25 $m_2$ 0.25 $m_3$ 0.25 $m_4$ 0.25 REASONING LEVEL #2 Pragmatic Speaker ($S_1$) The speaker reasons about how the Literal Listener interprets utterances to decide which utterance to produce.\nStep 1: Compute Utility for Each Utterance When Meaning = $m_4$ Using:\n$$ U(u, m) = \\log P_{L_0}(m \\mid u) - \\text{cost}(u) $$\nUtterance $P_{L_0}(m_4 \\mid u)$ $U(u, m_4)$ $u_{\\text{all}}$ 1.0 $\\log(1) = 0$ $u_{\\text{some}}$ 0.25 $\\log(0.25) = -1.386$ $u_{\\text{none}}$ 0 (ruled out) $-\\infty$ Step 2: Compute Unnormalized Scores $$ P_{S_1}(u \\mid m_4) \\propto \\exp\\left( \\alpha \\cdot U(u, m_4) \\right) $$\nUtterance $\\exp(\\alpha \\cdot U)$ Unnormalized Score $u_{\\text{all}}$ $\\exp(0) = 1$ 1.0 $u_{\\text{some}}$ $\\exp(-1.386) \\approx 0.25$ 0.25 $u_{\\text{none}}$ 0 0 Total Sum = $1 + 0.25 = 1.25$\nStep 3: Normalize to Get Final Speaker Probabilities $P_{S_1}(u_{\\text{all}} \\mid m_4) = \\frac{1}{1.25} = 0.8$ $P_{S_1}(u_{\\text{some}} \\mid m_4) = \\frac{0.25}{1.25} = 0.2$ $S_1$ Final Production Probabilities (Given $m_4$) Utterance $P_{S_1}(u \\mid m_4)$ “All” 0.8 “Some” 0.2 “None” 0 REASONING LEVEL #3 Pragmatic Listener ($L_1$) The listener now reasons about what meaning the speaker most likely intended after hearing the utterance.\nStep 1: Apply the Full Equation $$ P_{L_1}(m \\mid u_{\\text{some}}) = \\frac{P_{S_1}(u_{\\text{some}} \\mid m) \\cdot P(m)}{\\displaystyle \\sum_{m’} P_{S_1}(u_{\\text{some}} \\mid m’) \\cdot P(m’)} $$\nStep 2: Compute $P_{S_1}(u_{\\text{some}} \\mid m)$ for All Meanings We already computed this for $m_4$:\n$P_{S_1}(u_{\\text{some}} \\mid m_4) = 0.2$ Assume for this example:\nMeaning $P_{S_1}(u_{\\text{some}} \\mid m)$ $m_0$ 0 (ruled out by literal semantics) $m_1$ 0.4 $m_2$ 0.3 $m_3$ 0.2 $m_4$ 0.2 Step 3: Compute Numerator and Denominator Meaning $P_{S_1}(u_{\\text{some}} \\mid m)$ $P(m)$ Product $m_0$ 0 0.2 0 $m_1$ 0.4 0.2 0.08 $m_2$ 0.3 0.2 0.06 $m_3$ 0.2 0.2 0.04 $m_4$ 0.2 0.2 0.04 Denominator (Normalization Constant):\n$$ \\text{Total} = 0.08 + 0.06 + 0.04 + 0.04 = 0.22 $$\nStep 4: Compute Final $L_1$ Posterior Beliefs $P_{L_1}(m_1 \\mid u_{\\text{some}}) = \\frac{0.08}{0.22} \\approx 0.364$ $P_{L_1}(m_2 \\mid u_{\\text{some}}) = \\frac{0.06}{0.22} \\approx 0.273$ $P_{L_1}(m_3 \\mid u_{\\text{some}}) = \\frac{0.04}{0.22} \\approx 0.182$ $P_{L_1}(m_4 \\mid u_{\\text{some}}) = \\frac{0.04}{0.22} \\approx 0.182$ $P_{L_1}(m_0 \\mid u_{\\text{some}}) = 0$ $L_1$ Final Interpretation Meaning $P_{L_1}(m \\mid u_{\\text{some}})$ $m_0$: 0 cookies 0.0 (ruled out) $m_1$: 1 cookie 0.364 $m_2$: 2 cookies 0.273 $m_3$: 3 cookies 0.182 $m_4$: 4 cookies 0.182 Overall Takeaways Agent Main Process $L_0$ Filters meanings based on literal truth. $S_1$ Chooses utterances to maximize utility. $L_1$ Infers intended meaning based on the utterance and speaker model. Each level of reasoning in the RSA model refines the interpretation process using both prior knowledge and rational inference. VI. Reference Games This section presents how RSA explains language production and comprehension in Reference Games based on Franke \u0026 Jäger (2016).\n6.1 What Are Reference Games? Reference Games are simplified experimental tasks designed to study how speakers choose expressions to refer to objects and how listeners interpret those expressions.\nKey Features of Reference Games Interactive Setting: Involves a speaker and a listener. Goal: The speaker must communicate a specific referent (target object) to the listener. The listener must infer which object the speaker is referring to based on the utterance. Typical Experimental Setup Here is an example:\nObjects in Context:\nExample:\nGreen Square Green Circle Blue Circle Possible Utterances:\n“green” “square” “circle” “blue” Task:\nIf the target is the Green Square, should the speaker say “green” or “square”?\n- “Square” is more informative because it uniquely identifies the referent.\nConnection to Gricean Maxims Maxim of Quantity: Be as informative as required.\n→ Speakers prefer utterances that best help the listener identify the referent.\nMaxim of Manner: Avoid unnecessary effort.\n→ Speakers also tend to avoid overly complex or costly expressions.\nWhat Makes Reference Games Powerful? They provide a simple, controlled environment for testing: How speakers balance informativeness and production cost. How listeners reason about speaker choices (pragmatic inference). Reference games form the foundation for formal models of language use like the RSA model. 6.2 Reference Games — Formal Modeling In this section, we explore how the RSA model accounts for reasoning in reference games, following the mathematical formalizations used in Franke \u0026 Jäger (2016).\n1. Literal Listener ($L_0$) $$ P_{\\text{literal}}(r \\mid p) = \\begin{cases} \\frac{1}{|{r’ : p \\text{ is true of } r’}|} \u0026 \\text{if } p \\text{ is true of } r \\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$\nThis assigns uniform probability over all referents that literally satisfy the utterance. The Literal Listener does not consider why the speaker chose this utterance—only whether it’s true of a referent. Explanation of Formula Components $r$: Referent (target object)\n$p$: Property or utterance\n${ r’ : p \\text{ is true of } r’ }$\n→ This is the set of all referents for which the property $p$ holds true.\nExample: If $p$ is “green”, this set contains all green objects.\n$|{ r’ : p \\text{ is true of } r’ }|$\n→ This is the size of that set, i.e., how many referents have the property $p$.\n$\\frac{1}{|{ \\dots }|}$\n→ This means that each compatible referent is assigned equal probability, based on the total number of compatible referents.\n$\\text{if } p \\text{ is true of } r$\n→ This part of the case condition says that the formula only applies if $r$ is actually compatible with the utterance $p$.\nWhat Does $r’$ Represent? Symbol Meaning $r$ The specific referent the listener is considering. $r'$ A variable representing all possible referents in the context. Used for counting how many referents are compatible with the utterance. The notation ${ r’ : p \\text{ is true of } r’ }$ defines the set of all referents where the utterance $p$ is literally true. The term $| \\dots |$ counts how many referents are in that set. Example: Listener Hears “green” Referent Properties $r_1$ Green, Square $r_2$ Green, Circle $r_3$ Blue, Circle ${ r’ : p = \\text{“green”} \\text{ is true of } r’ } = { r_1, r_2 }$ $|{ \\dots }| = 2$ Compute Probabilities: $P_{\\text{literal}}(r_1 \\mid \\text{“green”}) = \\frac{1}{2} = 0.5$ $P_{\\text{literal}}(r_2 \\mid \\text{“green”}) = 0.5$ $P_{\\text{literal}}(r_3 \\mid \\text{“green”}) = 0$ 2. Pragmatic Speaker ($S_1$) $$ P_{prod}(p \\mid r; \\lambda, f) = \\frac{\\exp\\left(\\lambda \\cdot EU_{speaker}(r, p; f)\\right)}{\\sum_{p’} \\exp\\left(\\lambda \\cdot EU_{speaker}(r, p’; f)\\right)} $$\n$ EU_{speaker}(r, p; f) = P_{literal}(r \\mid p) + f(p) $ $f(p)$: Utterance bias or cost $\\lambda$: Rationality parameter (inverse temperature) 3. Pragmatic Listener ($L_1$) $$ P_{\\text{comp}}(r \\mid p) = \\frac{P(r) \\cdot P_{\\text{prod}}(p \\mid r)}{\\sum_{r’} P(r’) \\cdot P_{\\text{prod}}(p \\mid r’)} $$\n6.3 Further break-downs of the mathematical notations in Franke \u0026 Jager (2016) and Degen (2023) Literal Listener: Notation Comparison Concept Franke \u0026 Jäger (2016) Degen (2023) Agent $P_{\\text{literal}}(r \\mid p)$ $P_{L_0}(m \\mid u)$ Meaning / Referent $r$ = referent $m$ = meaning Utterance $p$ = property $u$ = utterance Literal Semantics $p$ is true of $r$ $m \\in \\llbracket u \\rrbracket$ Truth Filter Verbal condition Indicator: $\\delta_{m \\in \\llbracket u \\rrbracket}$ Uniformity Uniform over compatible referents Prior $P(m)$ allows non-uniformity Normalization Divide by number of compatible referents Sum over priors of compatible meanings Key Takeaways Both formulations implement literal semantics: they restrict attention to meanings that are literally compatible with the utterance. Degen’s version allows more Bayesian flexibility and incorporates world knowledge via priors. Franke \u0026 Jäger’s approach is useful for simple reference tasks with uniform assumptions. 6.4 Literal Listener (Based on Franke \u0026 Jäger, 2016)\nStep 1: Understand the Task Setup Object Color Shape $r_1$ Green Square $r_2$ Green Cirle $r_3$ Blue Circle Possible Utterances (Properties):\n$p \\in \\{ “green”, “circle”, “square”, “blue” \\}$\nHow Does the Literal Listener Interpret an Utterance? The Literal Listener reasons purely based on literal semantics, without considering why the speaker chose a particular utterance.\nFormula: $$ P_{\\text{literal}}(r \\mid p) = \\begin{cases} \\frac{1}{|{ r’ : p \\text{ is true of } r’ }|} \u0026 \\text{if } p \\text{ is true of } r \\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$\nIf the utterance $p$ is true of object $r$, assign equal probability among all such objects. Otherwise, assign probability $0$. Example 1: Listener Hears “green” Determine which objects that are compatible with the property “green”:\n→ $r_1$ (Green Square), $r_2$ (Green Circle)\nAssign Probabilities:\nReferent Compatible with “green”? $P_{\\text{literal}}(r \\mid p = \\text{“green”})$ $r_1$ Yes 0.5 $r_2$ Yes 0.5 $r_3$ No 0 ${ r’ : p = \\text{“green”} \\text{ is true of } r’ } = { r_1, r_2 }$ $|{ \\dots }| = 2$ Compute Probabilities: $P_{\\text{literal}}(r_1 \\mid \\text{“green”}) = \\frac{1}{2} = 0.5$ $P_{\\text{literal}}(r_2 \\mid \\text{“green”}) = \\frac{1}{2} = 0.5$ $P_{\\text{literal}}(r_3 \\mid \\text{“green”}) = 0$ Example 2: Listener Hears “square” Determine which objects are compatible with the property “square”:\n→ Only $r_1$.\nAssign Probabilities:\nReferent Compatible with “square”? $P_{\\text{literal}}(r \\mid p = \\text{“square”})$ $r_1$ Yes $1.0$ $r_2$ No $0$ $r_3$ No $0$ ${ r’ : p = \\text{“square”} \\text{ is true of } r’ } = { r_1 }$ $ |{ \\dots }| = 1 $ Compute Probabilities: $P_{\\text{literal}}(r_1 \\mid \\text{“square”}) = \\frac{1}{1} = 1.0$ $P_{\\text{literal}}(r_2 \\mid \\text{“square”}) = 0$ $P_{\\text{literal}}(r_3 \\mid \\text{“square”}) = 0$ Example 3: Listener Hears “blue” Determine which objects are compatible with the property “square”:\n→ Only $r_1$.\nAssign Probabilities:\nReferent Compatible with “square”? $P_{\\text{literal}}(r \\mid p = \\text{“square”})$ $r_1$ No $0.0$ $r_2$ No $0$ $r_3$ Yes $1.0$ ${ r’ : p = \\text{“blue”} \\text{ is true of } r’ } = { r_3 }$ $ |{ \\dots }| = 1 $ Compute Probabilities: $P_{\\text{literal}}(r_1 \\mid \\text{“square”}) = 0$ $P_{\\text{literal}}(r_2 \\mid \\text{“square”}) = 0$ $P_{\\text{literal}}(r_3 \\mid \\text{“square”}) = \\frac{1}{1} = 1.0$ 6.5 Pragmatic Speaker $$ P_{prod}(p \\mid r; \\lambda, f) = \\frac{\\exp\\left( \\lambda \\cdot EU_{speaker}(r, p; f) \\right)}{\\displaystyle \\sum_{p’} \\exp\\left( \\lambda \\cdot EU_{speaker}(r, p’; f) \\right)} $$\nwhere $EU_{speaker}(r, p; f)$ is Utility Function:\n$$ EU_{speaker}(r, p; f) = P_{literal}(r \\mid p) + f(p) $$\n$P_{literal}(r \\mid p)$: How likely is it that a literal listener will pick referent $r$ after hearing $p$? $f(p)$: Cost or bias term, penalizing utterances that are long, complex, or dispreferred. Explanation of Components Symbol Explanation $p$ Property (utterance). $r$ Referent (target object). $P_{prod} (p \\mid r; \\lambda, f)$ Probability of Pragmatic Speaker choosing a property/utterance $\\lambda$ Rationality parameter (speaker’s sensitivity to utility differences). $f(p)$ Cost or bias associated with utterance $p$. $ EU_{speaker}$ Expected utility of utterance $p$ for referent $r$. Combine the two\n$$ P_{prod}(p \\mid r; \\lambda, f) = \\frac{\\exp\\left( \\lambda \\cdot EU_{speaker}(r, p; f) \\right)}{\\displaystyle \\sum_{p’} \\exp\\left( \\lambda \\cdot EU_{speaker}(r, p’; f) \\right)} = \\frac{\\exp\\left( \\lambda \\cdot (P_{literal}(r \\mid p) + f(p)) \\right)}{\\displaystyle \\sum_{p’} \\exp\\left( \\lambda \\cdot (P_{literal}(r \\mid p’) + f(p’)) \\right)} $$\nWorked Example Scenario:\nObject Color Shape $r_1$ Green Square $r_2$ Green Circle $r_3$ Blue Circle Target referent $r = r_1$ (Green Square). Utterance/Property space: “green”, “square”. (Note that we limit the utterance choices to be of only “green” and “square” which are literally true of “Green Square”) Note that the set of utterances to choose from matters in final Speaker’s probability in choosing which utterance to use\n$u_1 \\in \\{ “green”, “square” \\}$ $u_2 \\in \\{ \\text{“green”}, \\text{“square”}, \\text{“circle”}, \\text{“blue”} \\}$ why the two $u$ sets? They lead to different Speaker’s probabilities in choosing utterances to refer to a particular referent. in the $u_2$ option, those semantically incompatible/literally false utterances/properties were also assigned probabilities.\nShould False Utterances Be Included? Option 1: Full Utterance Set (Even Non-True Ones) Assumption:\nSpeakers can, and sometimes do, use suboptimal or false utterances.\nModel Characteristics: All utterances in the property space are included, regardless of literal truth. Even non-true properties like \"blue\" (for a green object) receive non-zero probability, though down-weighted. Matches the default RSA formulation as used in: Franke \u0026 Jäger (2016) Goodman \u0026 Frank (2016) When to Use: You want to model noisy, probabilistic human behavior. You’re studying psycholinguistic behavior, overinformativeness, or slips. You’re interested in graded inference and soft competition. Option 2: Restricted Utterance Set (Only True Properties) Assumption: Speakers are always semantically competent and never use false utterances.\nModel Characteristics: The utterance set is filtered to include only properties that are literally true of the referent. Softmax normalization is applied only over true utterances. Produces sharper speaker preferences and downstream listener inference. When to Use: You’re modeling idealized agents in formal semantics or decision theory. Your research assumes strict informativeness. You’re simulating maximally rational communication. Speaker production probabilities calculation\nStep 1: Assume:\n$\\lambda = 1$ (moderate rationality). $f(p) = 0$ (no production cost). Step 2: Compute $P_{literal}(r \\mid p)$\nLiteral Listener for $“green”$:\nCompatible referents: $r_1$, $r_2$. $P_{\\text{literal}}(r_1 \\mid \\text{“green”}) = \\frac{1}{2} = 0.5$ Literal Listener for $“square”$:\nOnly $r_1$ is a square. $P_{literal}(r_1 \\mid {“square”}) = 1.0$ Step 3: Compute Utility for Each Utterance\n$EU_{speaker}(r_1, “green”; f) = P_{literal}(r_1 \\mid “green”) + f(“green”) = 0.5 + 0 = 0.5$\n$EU_{speaker}(r_1, “square”; f) = P_{literal}(r_1 \\mid “square”) + f(“square”) = 1.0 + 0 = 1.0$\nStep 4: Compute Unnormalized Scores Using $\\exp\\left(\\lambda \\cdot EU_{speaker}(r, p; f)\\right) $:\n$ \\exp\\left(\\lambda \\cdot EU_{speaker}(r_1, “green”; f) \\right) = \\exp(1 \\cdot 0.5) = \\exp(0.5) \\approx 1.649$ $\\exp\\left(\\lambda \\cdot EU_{speaker}(r_1, “square”; f) \\right) = \\exp(1 \\cdot 1.0) = \\exp(1) \\approx 2.718$ What Does $\\exp(0.5)$ Mean?\n$\\exp(x)$ is the exponential function, defined as: $ \\exp(x) = e^x $\nWhere $e \\approx 2.71828$ is $Euler’s$ number, a fundamental constant in mathematics. Why Is This Important in RSA?\nThe exponential function transforms utility values into positive scores. It ensures that higher-utility utterances lead to higher scores in the softmax calculation for final probabilities. Without exponentiation, negative utilities could produce invalid (negative) probabilities. Quick Reference Table Utility $EU(p, r)$ $\\exp(EU)$ Interpretation 0 1.0 Baseline utility. 0.5 1.649 Moderately high utility. 1.0 2.718 High utility. -1.386 0.25 Low utility (as in $\\log(0.25)$). Step 5: Normalize to Get Final Probabilities\n$ Total = \\exp\\left( \\lambda \\cdot EU_{speaker}(r_1, green’; f) \\right) + \\exp\\left( \\lambda \\cdot EU_{speaker}(r_1, square’; f) \\right) =1.649 + 2.718 = 4.367 $\n$P_{prod}({“green”} \\mid r_1) = \\frac{\\exp\\left( \\lambda \\cdot EU_{speaker}(r_1, “green”; f) \\right)}{\\sum_{p’} \\exp\\left( \\lambda \\cdot EU_{speaker}(r, p’; f) \\right)} = \\frac{1.649}{4.367} \\approx 0.378$ $P_{\\text{prod}}(\\text{“square”} \\mid r_1) = \\frac{\\exp\\left( \\lambda \\cdot EU_{speaker}(r_1, “square”; f) \\right)}{\\sum_{p’} \\exp\\left( \\lambda \\cdot EU_{speaker}(r, p’; f) \\right)} = \\frac{2.718}{4.367} \\approx 0.622$ Final Interpretation Utterance $P_{\\text{prod}}(p \\mid r_1)$ “green” 0.378 (38%) “square” 0.622 (62%) These are the Pragmatic Speaker’s probabilities in choosing “green” or “square” to refer to $r_1$ (Green Square) The speaker prefers “square” but still sometimes chooses “green”. This reflects probabilistic, graded behavior rather than deterministic selection. Key Insights Higher Utility → Higher Probability\nBut not necessarily 100%, depending on $\\lambda$.\nEffect of $\\lambda$:\nIf $\\lambda = 0$, the speaker would choose “green” and “square” randomly. If $\\lambda \\to \\infty$, the speaker would always choose “square”. Effect of Cost $f(p)$:\nIf “square” had a higher cost, the speaker might prefer “green” despite its lower informativeness. 6.6 Pragmatic Listener In the RSA model, the pragmatic listener ($L_1$) updates beliefs about the speaker’s intended referent based on the utterance they receive. This is done using Bayes’ Rule and the listener’s model of the speaker.\n$$ P_{comp}(\\text{choose } r \\mid \\text{receive } p;\\ \\lambda, f) = \\frac{P(r) \\cdot P_{prod}(p \\mid r;\\ \\lambda, f)}{\\displaystyle \\sum_{r’} P(r’) \\cdot P_{prod}(p \\mid r’;\\ \\lambda, f)} $$\nExplanation of Components Symbol Meaning $r$ A possible referent (object) $p$ The utterance (property) received from the speaker $P(r)$ Prior probability of referent $r$ $P_{prod}(p \\mid r;\\ \\lambda, f)$ Probability that a speaker with parameters $(\\lambda, f)$ would produce $p$ when referring to $r$ $\\lambda$ Rationality parameter (how optimally the speaker chooses) $f(p)$ Cost or bias associated with producing utterance $p$ $r'$ Variable over all referents used to normalize the distribution Intuition The listener reasons backward:\n“If the speaker chose to say $p$, which referent $r$ would make that utterance most likely—assuming they were optimizing utility using $\\lambda$ and $f$?”\nThis is a form of Bayesian social reasoning:\nThe listener assumes the speaker is rational (to some degree), and uses this model to infer likely referents. Example Referents\nCode Description $r_1$ Green Square $r_2$ Green Circle $r_3$ Blue Circle Possible utterances (utterance space): “green”, “square”, “circle”, “blue”\nStep 1: Assumed Prior\nUniform prior: $ P(r_1) = P(r_2) = P(r_3) = \\frac{1}{3} $\nStep 2: calculate Pragmatic Speaker’s Probability of choosing “green” for each of three referent (using λ = 1, no cost bias $f(p) = 0$):\n1. probability of intending $r_1$ (green square) for each option in the utterance/property space\nStep 1: Literal Listener Probabilities $P_{literal}(r_1 \\mid \\text{“green”}) = \\frac{1}{2} = 0.5$ $P_{literal}(r_1 \\mid \\text{“square”}) = 1.0$ $P_{literal}(r_1 \\mid \\text{“circle”}) = 0$ $P_{literal}(r_1 \\mid \\text{“blue”}) = 0$ Step 2: Pragmatic Speaker Probabilities The formula of Pragmatic Speaker is: $$ P_{prod}(p \\mid r; \\lambda, f) = \\frac{\\exp\\left( \\lambda \\cdot EU_{speaker}(r, p; f) \\right)}{\\sum_{p’} \\exp\\left( \\lambda \\cdot EU_{speaker}(r, p’; f) \\right)} = \\frac{\\exp\\left( \\lambda \\cdot (P_{literal}(r \\mid p) + f(p)) \\right)}{\\sum_{p’} \\exp\\left( \\lambda \\cdot (P_{literal}(r \\mid p’) + f(p’)) \\right)} $$\nstep 2.1: Compute $EU$ Values $EU(r_1, \\text{“green”; f}) = P_{literal}(r_1 \\mid “green”) + f(“green”) = 0.5 + 0 = 0.5$ $EU(r_1, \\text{“square”}; f) = P_{literal}(r_1 \\mid “green”) + f(p) = 1.0 + 0 = 1.0$ $EU(r_1, \\text{“circle”}; f) = P_{literal}(r_1 \\mid “green”) + f(p) = 0 + 0 = 0$ $EU(r_1, \\text{“blue”}; f) = P_{literal}(r_1 \\mid “blue”) + f(p) = 0 + 0 = 0$ Step 2.2: Exponentiate and Normalize Exponentiate:\n$\\exp\\left( \\lambda \\cdot EU_{speaker}(r_1, “green”; f) \\right) = \\exp(1 \\cdot 0.5) = \\exp(0.5)\\approx 1.649$ $\\exp\\left( \\lambda \\cdot EU_{speaker}(r_1, “square”; f) \\right) = \\exp(1 \\cdot 1.0) = \\exp(1)\\approx 2.718$ $\\exp\\left( \\lambda \\cdot EU_{speaker}(r_1, “circle”; f) \\right) = \\exp(1 \\cdot 0) = \\exp(0) = 1.0$ $\\exp\\left( \\lambda \\cdot EU_{speaker}(r_1, “blue”; f) \\right) = \\exp(1 \\cdot 0) = \\exp(0) = 1.0$ Total sum:\n$$ 1.649 + 2.718 + 1.0 + 1.0 = 6.367 $$\nFinal probabilities:\n$P_{prod}(\\text{“green”} \\mid r_1) = \\frac{1.649}{6.367} \\approx 0.2589$ $P_{prod}(\\text{“square”} \\mid r_1) = \\frac{2.718}{6.367} \\approx 0.4269$ $P_{prod}(\\text{“circle”} \\mid r_1) = \\frac{1.0}{6.367} \\approx 0.1571$ $P_{prod}(\\text{“blue”} \\mid r_1) = \\frac{1.0}{6.367} \\approx 0.1571$ 2. probability of intending $r_2$ (green circle) for each option in the utterance/property space\nLiteral Listener Probabilities $P_{literal}(r_2 \\mid “green”) = \\frac{1}{2} = 0.5$ $P_{literal}(r_2 \\mid “square”) = 0$ $P_{literal}(r_2 \\mid “circle”) = \\frac{1}{2} = 0.5$ $P_{literal}(r_2 \\mid “blue”) = 0$ Step 2: Pragmatic Speaker Probabilities step 2.1: Compute $EU$ Values $EU(r_2, \\text{“green”; f}) = P_{literal}(r_1 \\mid “green”) + f(“green”) = 0.5 + 0 = 0.5$ $EU(r_2, \\text{“square”}; f) = P_{literal}(r_1 \\mid “green”) + f(p) = 0 + 0 = 0$ $EU(r_2, \\text{“circle”}; f) = P_{literal}(r_1 \\mid “green”) + f(p) = 0.5 + 0 = 0.5 $ $EU(r_2, \\text{“blue”}; f) = P_{literal}(r_1 \\mid “blue”) + f(p) = 0 + 0 = 0 $ Step 2.2: Exponentiate and Normalize Exponentiate:\n$\\exp\\left( \\lambda \\cdot EU_{speaker}(r_2, “green”; f) \\right) = \\exp(1 \\cdot 0.5) = \\exp(0.5)\\approx 1.649$ $\\exp\\left( \\lambda \\cdot EU_{speaker}(r_2, “square”; f) \\right) = \\exp(1 \\cdot 0) = \\exp(0) = 1$ $\\exp\\left( \\lambda \\cdot EU_{speaker}(r_2, “circle”; f) \\right) = \\exp(1 \\cdot 0.5) = \\exp(0.5) \\approx 1.649$ $\\exp\\left( \\lambda \\cdot EU_{speaker}(r_2, “blue”; f) \\right) = \\exp(1 \\cdot 0) = \\exp(0) = 1$ Total:\n$$ 1.649 + 1 + 1.649 + 1 = 5.298 $$\nFinal probabilities:\n$P_{prod}(\\text{“green”} \\mid r_2) = \\frac{1.649}{4.298} \\approx 0.3112$ $P_{prod}(\\text{“square”} \\mid r_2) = \\frac{1}{4.298} \\approx 0.1888$ $P_{prod}(\\text{“circle”} \\mid r_2) = \\frac{1.649}{4.298} \\approx 0.3112$ $P_{prod}(\\text{“blue”} \\mid r_2) = \\frac{1}{4.298} \\approx 0.1888$ 3. probability of intending $r_3$ (blue circle) for each option in the utterance/property space\nLiteral Listener Probabilities $P_{literal}(r_2 \\mid “green”) = 0$ $P_{literal}(r_2 \\mid “square”) = 0$ $P_{literal}(r_2 \\mid “circle”) = \\frac{1}{2} = 0.5$ $P_{literal}(r_2 \\mid “blue”) = \\frac{1}{1} = 1$ Step 2: Pragmatic Speaker Probabilities step 2.1: Compute $EU$ Values $EU(r_3, \\text{“green”; f}) = P_{literal}(r_3 \\mid “green”) + f(“green”) = 0 + 0 = 0$ $EU(r_3, \\text{“square”}; f) = P_{literal}(r_3 \\mid “green”) + f(p) = 0 + 0 = 0$ $EU(r_3, \\text{“circle”}; f) = P_{literal}(r_3 \\mid “green”) + f(p) = 0.5 + 0 = 0.5 $ $EU(r_3, \\text{“blue”}; f) = P_{literal}(r_3 \\mid “blue”) + f(p) = 1 + 0 = 1 $ Step 2.2: Exponentiate and Normalize Exponentiate:\n$\\exp\\left( \\lambda \\cdot EU_{speaker}(r_3, “green”; f) \\right) = \\exp(1 \\cdot 0) = \\exp(0) = 1$ $\\exp\\left( \\lambda \\cdot EU_{speaker}(r_3, “square”; f) \\right) = \\exp(1 \\cdot 0) = \\exp(0) = 1$ $\\exp\\left( \\lambda \\cdot EU_{speaker}(r_3, “circle”; f) \\right) = \\exp(1 \\cdot 0.5) = \\exp(0.5) \\approx 1.649$ $\\exp\\left( \\lambda \\cdot EU_{speaker}(r_3, “blue”; f) \\right) = \\exp(1 \\cdot 1) = \\exp(1) \\approx 2.718$ Total: $ 1 + 1 + 1.649 + 2.718 = 6.367 $\nFinal probabilities:\n$P_{prod}(\\text{“green”} \\mid r_3) = \\frac{1}{6.367} \\approx 0.1571$ $P_{prod}(\\text{“square”} \\mid r_3) = \\frac{1}{6.367} \\approx 0.1571$ $P_{prod}(\\text{“circle”} \\mid r_3) = \\frac{1.64}{6.367} \\approx 0.2589$ $P_{prod}(\\text{“blue”} \\mid r_3) = \\frac{2.718}{6.367} \\approx 0.4269$ VII. Indirect Speech Acts in RSA Based on Franke \u0026 Jäger (2016), Section 6\n7.1 What Are Indirect Speech Acts? An indirect speech act occurs when a speaker utters something whose literal meaning differs from their intended communicative goal. For example:\n“Can you pass the salt?”\nLiteral interpretation: A question about your ability. Pragmatic interpretation: A polite request to pass the salt. These types of speech acts are pervasive in everyday communication and raise a key challenge for formal models of meaning. How can a listener derive an intended meaning that diverges from the literal form?\nThe Rational Speech Act (RSA) model provides a solution by modeling speakers and listeners as rational agents engaging in recursive social reasoning — not just about what was said, but why it was said in a given context.\nCertainly! Below is the revised section “6.1 The Problem” formatted in Hugo markdown syntax with clear bullet points, examples, and beginner-friendly explanation. This version is tailored for your RSA learning notes website.\n7.2 The Problem: Why Indirect Speech Acts Seem Irrational—But Aren’t Indirect speech acts pose a foundational puzzle for rational models of communication. At first glance, they appear to violate classic Gricean principles:\nQuantity: Be as informative as necessary. Manner: Avoid obscurity and ambiguity. Relation: Be relevant. Quality: Say what you believe to be true. Yet in daily life, people routinely use—and understand—indirect speech effectively. So why do speakers often choose less direct expressions, especially when their intent is clear?\nWhat’s the Puzzle? According to Gricean maxims, indirect utterances seem:\nLess informative than needed (Quantity), More ambiguous than necessary (Manner), Sometimes even irrelevant on the surface (Relation). This suggests that indirect speech is irrational—and yet, it’s ubiquitous and interpretable. To resolve this contradiction, Franke and Jäger (2016) draw from the broader field of rationalist pragmatics, especially ideas from Brown \u0026 Levinson (1987) and Steven Pinker (2008).\nTheir key insight:\nIndirect speech is rational when we consider social context, strategic goals, and reputational concerns.\nThree Social-Pragmatic Motivations (after Pinker, 2008) 1. Plausible Deniability Example:\nThe veiled bribe is another recognizable plot device, as when the kidnapper in Fargo shows a police officer his drivers’s license in a wallet with a fifty-dollar bill protruding from it and suggests, ‘So maybe the best thing would be to take care of that here in Brainerd.’ (Pinker 2008: 374)\nThe literal meaning is vague. The implied meaning (a bribe) is understood—but not explicit. If challenged, the speaker can deny the implicature. 👉 Why? Indirectness helps the speaker avoid social or legal consequences if the implicature is rejected or punished.\n2. Establishing Shared Knowledge Without Common Knowledge Example: “Would you like to come up for coffee?” (instead of “Do you want to have sex?”)\nThe utterance signals intent. Both speaker and hearer recognize the implicature. But they avoid making it common knowledge—publicly acknowledged by both parties. Why? Maintaining deniability or politeness can protect both parties from embarrassment or rejection.\n3. Preserving Social Relationships and Roles Example: A customer tells the maître d’, “Is there any way to get a table sooner?” instead of “Here’s $50 to seat me now.”\nThe direct request would frame the situation as a market transaction. The indirect request preserves the social role of a courteous guest. 👉 Why? Indirect speech protects the relational frame and avoids signaling an inappropriate relationship type (e.g., transactional vs. respectful).\nshared knowledge vs. common knowledge Key Distinction Understanding indirect speech acts often requires distinguishing between two types of mutual understanding:\nShared Knowledge Both speaker and listener know a fact. However, there is no public acknowledgment that both know it. This creates a zone of plausible deniability. Common Knowledge Not only do both know it, but they know that the other knows it—and this fact is mutually acknowledged. Once a fact becomes common knowledge, it’s out in the open and cannot be easily denied. Example 1: Romantic Context Indirect speech: “Would you like to come up for coffee?”\nDirect speech: “Do you want to have sex?”\nBoth interlocutors may know the intent behind “coffee” (shared knowledge). But they avoid stating it directly, which would make it common knowledge. This preserves social harmony and allows either party to save face. Example 2: Workplace Context Indirect: “It’s a little warm in here, isn’t it?”\nDirect: “Please turn on the air conditioning.”\nThe boss’s intent is understood (shared knowledge), But it’s not framed as a direct order, keeping it from becoming common knowledge. This helps preserve hierarchical boundaries without confrontation. Indirect speech often works by deliberately avoiding the shift from shared to common knowledge.\nAvoiding the Mixing of Relationship Types One reason for using indirect speech acts is to preserve the boundaries between different social relationship types, such as professional vs. romantic, or hierarchical vs. egalitarian.\nWhen people communicate across multiple social dimensions, they must signal which frame of relationship they are currently invoking. Direct speech might trigger an unwanted interpretation associated with another relationship type.\nExample: Professor and Student Indirect: “Would you be interested in attending the conference with me?”\nDirect: “Would you like to go on a date with me?”\nThe indirect question may preserve the professional relationship, while opening space for a social connection. A direct question risks re-framing the interaction as romantic, which may be inappropriate or problematic. Example: Manager and Subordinate Indirect: “If you have time later, maybe you could take a look at this?”\nDirect: “I’m assigning this task to you.”\nA manager might avoid sounding too authoritarian, maintaining a more collegial tone. Mixing directive and collaborative frames could create discomfort. Indirectness helps the speaker frame the interaction in a way that suits the current social dynamic. 7.3 The Deeper Problem The real puzzle isn’t “why do indirect speech acts work?” but rather:\nWhen and why is it rational to use them?\nWhat seems irrational in terms of raw information transfer becomes rational when:\nSocial dynamics, Strategic ambiguity, and Relationship management are taken into account.\nFranke and Jäger argue that game-theoretic models, especially from the RSA/IBR family, can model these choices:\nAgents (speakers and listeners) have utilities that include social and reputational costs. Speakers reason about how listeners will interpret them. Listeners reason about why a speaker chose a particular form over another. 7.4 Why Do Indirect Speech Acts Work? (RSA Explanation) Indirect speech acts—where speakers imply meaning rather than state it directly—can seem puzzling. RSA offers a clear game-theoretic approach to explaining why these acts work rationally, despite their apparent indirectness.\nStep-by-Step Explanation (Stalnaker’s Example) The authors illustrate their RSA approach using a real-world scenario involving an indirect speech act by the US Treasury Secretary:\nIn May, 2003, the US Treasury Secretary, John Snow, in response to a question, made some remarks that caused the dollar to drop precipitously in value. The Wall Street Journal sharply criticized him for ‘playing with fire,’ and characterized his remarks as ‘dumping on his own currency,’ ‘bashing the dollar,’ and ‘talking the dollar down.’ What he in fact said was this: ‘When the dollar is at a lower level it helps exports, and I think exports are getting stronger as a result.’ This was an uncontroversial factual claim that everyone, whatever his or her views about what US government currency policy is or should be, would agree with. Why did it have such an impact? (Stalnaker 2005: 82)\nUtterance $s$ (indirect):\n“When the dollar is at a lower level it helps exports, and I think exports are getting stronger as a result.”\nImplied meaning (direct):\n“The US Treasury will take measures to lower the dollar’s exchange rate.”\nA rationalist account based on decision and game theory\nStep 1: Define Speaker’s Possible Intentions (“Types”)\nWe consider two possible “types” of speaker (S):\nType $t_1$: The speaker plans to reduce the dollar’s value. Type $t_2$: The speaker has no plan to reduce the dollar’s value. prior assumptions about the relative likelihood of $t_1$ and $t_2$:\nprior probability distribution of $t_1$: $P(t_1)$ prior probability distribution of $t_1$: $P(t_2)$ The listener $L$ initially has uncertainty regarding these two possibilities:\n$$ P(t_1) + P(t_2) = 1,\\quad 0 \u003c P(t_1) \u003c 1 $$\nStep 2: Determine Likelihood of Utterance Given Speaker’s Intentions\nHow likely is it that $S$ would utter $s$ in $t_1$, and in $t_2$?\nNext, consider the likelihood that each speaker type would produce the indirect utterance $s$:\n$P(s|t_1)$: Probability the speaker would say $s$ if planning to lower the dollar ($t_1$). $P(s|t_2)$: Probability the speaker would say $s$ if not planning any action ($t_2$). Because the statement made is generally an economic truism, it’s plausible for both types to utter it.\nfor $t_1$ it would be a useful argument to justify his intentions; thus, the statement is significantly more meaningful for a speaker who plans to act (type $t_1$) for $t_2$, t2 utters this sentence, just to say something meaningless during a public hearing. As there myriads of meaningless statements to choose from, this likelihood is small. Thus: $$ P(s|t_1) \u003e P(s|t_2) $$\nIntuitively:\nA speaker planning to act ($t_1$) would choose such a statement intentionally (making their intent clear indirectly). A speaker not planning to act ($t_2$) would rarely choose this specific statement, given many equally trivial statements available. Step 3: Listener Updates Beliefs (Bayesian Reasoning)\nthe listener $L$ will use Bayes’ rule to compute the posterior probability distribution over S’s types $t$, given the signal observed $s$\n$$ P(t_1|s) = \\frac{P(s|t_1)P(t_1)}{\\sum_{(t’)}P(s|t’)P(t’)} $$\nalso:\n$$ P(t_1|s) = \\frac{P(s|t_1)P(t_1)}{P(s|t_1)P(t_1) + P(s|t_2)P(t_2)} $$\nSince $P(s|t_1) \u003e P(s|t_2)$, we have:\n$$ P(t_1|s) \u003e P(t_1) $$\nThis means hearing the utterance increases the listener’s belief that the speaker plans to lower the dollar.\nKey Insight The indirect utterance (economic truism) raises the listener’s belief in the speaker’s intention without explicitly confirming it. If the utterance were completely direct (e.g., “The US Treasury will lower the dollar”), it would remove all uncertainty—potentially creating commitment or unwanted accountability for the speaker. Thus, indirectness maintains plausible deniability while effectively communicating intent.\nWhy does $ P(s \\mid t_1) \u003e P(s \\mid t_2) $ imply $ P(t_1 \\mid s) \u003e P(t_1) $?\n$$P(s \\mid t_1) \u003e P(s \\mid t_2)$$ $$ \\frac{P(s \\mid t_1)}{P(s \\mid t_2)} \u003e 1 $$ $$ \\frac{P(s \\mid t_1)P(t_1)}{P(s \\mid t_2)(P(t_2))} \u003e \\frac{P(t_1)}{P(t_2)}$$ $$ \\frac{ \\frac{P(s \\mid t_1)P(t_1)}{P(s)}}{\\frac{P(s \\mid t_2)(P(t_2))}{P(s)}} \u003e \\frac{P(t_1)}{P(t_2)}$$ as Bayesian Theorem stipulates: $\\frac{P(s \\mid t)P(t)}{P(s)} = P(t \\mid s)$ $$ \\frac{P(t_1 \\mid s)}{P(t_2 \\mid s)} \u003e \\frac{P(t_1)}{P(t_2)} $$ as in this setting: $P(t_1) + P(t_2) = 1$ and $P(t_1 \\mid s) + P(t_2 \\mid s) = 1$ $$\\frac{P(t_1 \\mid s)}{1 - P(t_1 \\mid s)} \u003e \\frac{P(t_1)}{1 - P(t_1)}$$ $$P(t_1 \\mid s) - P(t_1)P(t_1 \\mid s) \u003e P(t_1) - P(t_1)P(t_1 \\ mid s)$$ $$P(t_1 \\mid s) \u003e P(t_1)$$\n7.5 Why RSA Models Capture This Rationality: RSA rationalizes indirect speech acts as:\nStrategic choices made by speakers to influence listener beliefs without explicit commitment. Optimally balancing social cost (being explicit can be risky) and communicative effectiveness (the listener still infers intent clearly). RSA Summary (Stalnaker’s Example): Component Explanation (in RSA terms) $t_1, t_2$ Possible speaker intentions $P(t)$ Listener’s prior beliefs $P(s|t)$ Likelihood speaker utters sentence given intention $P(t|s) $ Listener’s updated belief after hearing utterance Indirectness Rational strategy for influencing listener beliefs without full accountability Intuition Check: Indirectness is rational because it strategically affects listener beliefs without explicitly committing the speaker. It shifts probabilities, making some interpretations highly likely—but not guaranteed—thus preserving crucial social and strategic flexibility.\nStep 3: Bayesian Reasoning and Listener Belief Updating\nWhen a listener hears an utterance — especially an indirect one — they update their beliefs about what the speaker intends.\nRSA models this belief updating using Bayes’ rule, a core principle in probabilistic reasoning.\nExample (Franke \u0026 Jäger, 2016) Utterance (indirect):\n“When the dollar is at a lower level, it helps exports, and I think exports are getting stronger as a result.”\nPossible speaker intentions:\n• t₁: The speaker intends to lower the dollar.\n• t₂: The speaker does not intend any action.\nBayes’ Rule (Formal) $$ P(t_1 \\mid s) = \\frac{P(s \\mid t_1) \\cdot P(t_1)}{P(s)} $$\n• $P(t_1 \\mid s)$: Updated belief about speaker’s intention t₁ after hearing utterance s.\n• $P(s \\mid t_1)$: Likelihood of producing utterance s given intention t₁.\n• $P(t_1)$: Prior belief about intention t₁.\n• $P(s)$: Total probability of s, marginalizing over all possible intentions.\nSample Calculation: Prior: $P(t_1) = 0.5$, $P(t_2) = 0.5$ Likelihood: $P(s \\mid t_1) = 0.8$, $P(s \\mid t_2) = 0.2$ Marginal:\n$$ P(s) = (0.8 \\cdot 0.5) + (0.2 \\cdot 0.5) = 0.5 $$ Posterior:\n$$ P(t_1 \\mid s) = \\frac{0.8 \\cdot 0.5}{0.5} = 0.8 $$ The listener now assigns 80% probability to the speaker intending to lower the dollar — a strong update from the original 50%.\nWhy It Matters RSA shows that even vague or indirect utterances can shift beliefs dramatically through rational inference.\nThis explains why indirect speech acts are effective, despite their surface ambiguity.\n7.4 RSA Perspective on Indirectness In the RSA framework, interpretation goes beyond truth conditions. Speakers are modeled as optimizing communicative goals, and listeners reason backward from utterances to infer those goals.\nFranke \u0026 Jäger (2016) formalize this with the notion of goal-based RSA:\nEach utterance $u$ can serve multiple communicative goals $g$. Listeners compute the most likely goal the speaker intended, given the utterance. Communicative Goals as Targets of Inference Let:\n$U$ = utterance space\n$G$ = space of possible communicative goals\nExample goals:\n$g_1$: Request (e.g., “please pass the salt”) $g_2$: Information inquiry (e.g., “are you able to pass the salt?”) RSA Inference for Indirect Speech Step 1: Pragmatic Speaker The speaker chooses an utterance $u$ to realize goal $g$ by maximizing its utility:\n$$ P_{S_1}(u \\mid g) \\propto \\exp\\left( \\lambda \\cdot U(u, g) \\right) $$\n$U(u, g)$ is the utility of using utterance $u$ to achieve goal $g$ $\\lambda$ is the rationality parameter, controlling how deterministic the speaker is Step 2: Pragmatic Listener The listener infers the likely goal $g$ that motivated $u$:\n$$ P_{L_1}(g \\mid u) \\propto P_{S_1}(u \\mid g) \\cdot P(g) $$\n$P(g)$: prior probability of goal $g$ (based on context/world knowledge) $P_{S_1}(u \\mid g)$: speaker’s production probability This equation is a direct application of Bayes’ Rule.\nIllustration: “Can you pass the salt?” Scenario Speaker utters: “Can you pass the salt?” Literal content = question about ability ($g_2$) Context: speaker and listener are seated at dinner Listener’s reasoning: Literal listener interprets $u$ as ability question.\nPragmatic listener reasons:\n“If the speaker truly wanted information, would they have said this? Or is it more likely they’re politely requesting action?”\nGiven the contextual prior $P(g_1) \u003e P(g_2)$, and assuming $P_{S_1}(u \\mid g_1)$ is high, the listener concludes the intended goal is $g_1$ (request).\nWhy This Works RSA formalizes a long-standing idea in pragmatics:\nUtterances are goal-directed Indirectness arises from optimization under social and contextual constraints Rather than assuming fixed speech act types, RSA derives them as the outcome of:\nRational choice by the speaker, balancing informativeness, politeness, and cost Rational inference by the listener, integrating form, context, and expectations Summary Table Component Explanation $g$ Communicative goal (e.g., request, information-seeking) $u$ Observed utterance (e.g., “Can you pass the salt?”) $P(g)$ Prior plausibility of goal $g$ $P_{S_1}(u \\mid g)$ Probability speaker would say $u$ to achieve $g$ $P_{L_1}(g \\mid u)$ Listener’s inferred probability that $g$ is the intended goal Takeaways RSA treats speech acts as probabilistic goal inferences. Indirect speech acts are explained as intentional ambiguity for reasons like politeness or plausible deniability. By using recursive reasoning about goals, RSA naturally derives indirect meaning as an emergent property of rational communication. Extra Extra #1 Lambda $\\lambda$ In the Rational Speech Act (RSA) model, λ (lambda) appears in the Pragmatic Speaker equation, where it controls how strongly the speaker favors higher-utility utterances.\nCore Intuition: λ determines how deterministic or probabilistic the speaker’s utterance choices are.\nWhere Does λ Appear? In Franke \u0026 Jäger (2016), the Pragmatic Speaker is modeled as:\n$$ P_{prod}(p \\mid r; \\lambda, f) = \\frac{\\exp\\left( \\lambda \\cdot EU(r, p; f) \\right)}{\\sum_{p’} \\exp\\left( \\lambda \\cdot EU(r, p’; f) \\right)} $$\n$p$: utterance (property) $r$: intended referent $f(p)$: cost or bias of utterance $EU(r, p; f)$: utility of utterance $p$ for referent $r$ λ: rationality parameter What Does λ Do? λ scales the utility before exponentiation. It determines how much more likely the speaker is to choose high-utility utterances over others. Effects of Different λ Values λ Value Speaker Behavior 0 Completely random (uniform choice) ~0.5 Weak preference for better options 1 Moderate, probabilistic choice \u003e5 Strong preference for best option → ∞ Always chooses highest-utility utterance Mathematical Background Softmax Function The softmax is a smooth version of the argmax:\n$$ \\text{softmax}_i(x) = \\frac{\\exp(\\lambda x_i)}{\\sum_j \\exp(\\lambda x_j)} $$\nλ controls the sharpness of the preference. High λ → output resembles a hard decision (argmax). Low λ → output is closer to uniform probability. Connection to Statistical Physics In the Boltzmann distribution:\n$$ P_i = \\frac{\\exp(-E_i / kT)}{\\sum_j \\exp(-E_j / kT)} $$\n$T$ is temperature. RSA’s λ is analogous to inverse temperature: $$ \\lambda = \\frac{1}{T} $$\nLow temperature (high λ) → more deterministic. High temperature (low λ) → more randomness. Link to Decision Theory Softmax choice reflects bounded rationality. Agents aren’t fully deterministic, but biased toward better options. λ represents decision sharpness or confidence. Numerical Example Suppose:\n$EU(\\text{“square”}) = 1.0$ $EU(\\text{“green”}) = 0.5$ Then:\nWith λ = 1:\n$$ \\exp(1 \\cdot 1.0) = 2.718,\\quad \\exp(1 \\cdot 0.5) = 1.649 $$\nNormalize:\n$$ \\text{Total} = 2.718 + 1.649 = 4.367 $$\n$$ P(\\text{“square”}) = \\frac{2.718}{4.367} \\approx 0.622,\\quad P(\\text{“green”}) = \\frac{1.649}{4.367} \\approx 0.378 $$\nThe speaker prefers “square” but still sometimes says “green”.\nSummary Table Concept Interpretation λ (lambda) Rationality or sensitivity to utility Low λ Speaker behaves more randomly High λ Speaker consistently chooses best utterance λ = 0 Speaker chooses uniformly λ → ∞ Speaker chooses deterministically Mathematical Function Appears inside exponential in softmax Final Takeaway λ gives the RSA model flexibility to represent real human speakers who are sometimes decisive and sometimes probabilistic in their choices. It controls the softness of rationality in utterance production.\nExtra #2 Softmax Function Why Is the Softmax Function Used in the Pragmatic Speaker Formula? In the RSA model, the Pragmatic Speaker chooses among possible utterances to convey an intended referent.\nThe model assumes the speaker is rational but probabilistic.\nThe Formula (Franke \u0026 Jäger, 2016) The speaker’s production probability is defined as:\n$$ P_{prod}(p \\mid r; \\lambda, f) = \\frac{\\exp\\left( \\lambda \\cdot EU(r, p; f) \\right)}{\\sum_{p’} \\exp\\left( \\lambda \\cdot EU(r, p’; f) \\right)} $$\n$p$: property/utterance $r$: intended referent $\\lambda$: rationality parameter $f(p)$: utterance bias or cost $EU(r, p; f)$: expected utility of utterance $p$ for referent $r$ What Is the Softmax Function Doing Here? The softmax function:\n$$ \\text{softmax}_i(x) = \\frac{\\exp(\\lambda x_i)}{\\sum_j \\exp(\\lambda x_j)} $$\nis used to turn utility scores into probabilities. This is crucial in RSA because:\n1. It Maps Utility to Probability Higher utility → higher probability But all utterances still have some chance of being chosen This models graded speaker behavior 2. It Reflects Real Human Behavior Humans don’t always choose the “best” utterance. They might:\nBe uncertain Prefer shorter or easier words Make probabilistic rather than deterministic decisions Softmax captures this variation.\n3. It Controls Rationality via λ λ Value Speaker Behavior λ = 0 Completely random λ = 1 Matches utility proportionality λ → ∞ Always chooses best option λ allows the model to simulate different levels of speaker precision This is sometimes called bounded rationality 4. It Enables Listener Inference Listeners in RSA models reverse-engineer speaker choices.\nSoftmax:\nMakes speaker behavior invertible using Bayes’ rule Lets listeners reason: “Why would the speaker have said that?” 5. It’s a Standard Tool in Decision and Learning Models Used in machine learning (e.g., neural networks) Used in economics (e.g., quantal response models) Used in reinforcement learning (for action selection) Softmax in RSA makes the model mathematically standard, general, and interpretable.\nIntuition Softmax lets the speaker be smart—but not rigid.\nBetter utterances are more likely, but nothing is guaranteed.\nSummary Table Feature Why Softmax Helps in RSA Maps utility → probability Valid probability distribution Captures human behavior Models graded, non-deterministic choices Flexible rationality λ controls how sharp or flat the distribution is Bayesian inference support Listener can reason backwards Standard modeling tool Widely used in cognitive science and AI Extra #3 Lambda Softmax Visualizer \u003c!DOCTYPE html\u003e Softmax Visualization Effect of Lambda (λ) on Speaker Choice Utility of \"square\": 1.0\nUtility of \"green\": 0.5\nλ (Rationality parameter): Extra 4 Bayesian Theorem Theorem: A Beginner’s Guide\nBayes’ Theorem is a fundamental concept in probability theory and statistical reasoning. It describes how to update our beliefs in light of new evidence. This principle is central to many fields, including linguistics, cognitive science, machine learning, and especially pragmatic inference in models like the Rational Speech Act (RSA) framework.\nWhat is Bayes’ Theorem?\nAt its core, Bayes’ Theorem answers the question:\n“Given some new evidence, how should I update my belief about a hypothesis?”\nThe Formula\n$$ P(H \\mid E) = \\frac{P(E \\mid H) \\cdot P(H)}{P(E)} $$\nWhat the symbols mean:\n$P(H \\mid E)$: Posterior\nThe probability of hypothesis $H$ given the new evidence $E$. (What we want to know.)\n$P(E \\mid H)$: Likelihood\nThe probability of seeing the evidence $E$ if the hypothesis $H$ were true.\n$P(H)$: Prior\nThe initial belief about hypothesis $H$, before seeing the new evidence.\n$P(E)$: Marginal likelihood or Evidence\nThe total probability of the evidence under all possible hypotheses.\nSimple Example\nSuppose:\nA rare disease affects 1% of a population. A medical test detects the disease with 99% accuracy (true positive rate). The false positive rate is also 1%. If a person tests positive, what is the probability they actually have the disease?\nStep 1: Define Events\n$D$: Person has the disease $T$: Person tests positive Step 2: Plug in the numbers\n$P(D) = 0.01$ $P(\\neg D) = 0.99$ $P(T \\mid D) = 0.99$ $P(T \\mid \\neg D) = 0.01$ $$ P(T) = P(T \\mid D) \\cdot P(D) + P(T \\mid \\neg D) \\cdot P(\\neg D) = 0.99 \\cdot 0.01 + 0.01 \\cdot 0.99 = 0.0198 $$\n$$ P(D \\mid T) = \\frac{0.99 \\cdot 0.01}{0.0198} \\approx 0.50 $$\nConclusion: Even after testing positive, the probability of actually having the disease is only 50%!\nThis counterintuitive result shows the importance of prior probability in updating beliefs.\nVisual Breakdown\nflowchart TD A[Start: Prior P(H)] --\u003e B[New Evidence P(E | H)] B --\u003e C[Compute Joint: P(E | H) * P(H)] C --\u003e D[Compute P(E)] D --\u003e E[Posterior: P(H | E)] Sources Degen (2023), The Rational Speech Act Framework\nFranke, M., \u0026 Jäger, G. (2016). Probabilistic pragmatics, or why Bayes’ rule is probably important for pragmatics. Zeitschrift für Sprachwissenschaft, 35(1), 3 - 44.\n",
  "wordCount" : "13957",
  "inLanguage": "en",
  "datePublished": "2025-05-06T11:41:54-04:00",
  "dateModified": "2025-05-06T11:41:54-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://zhangjunfelix.github.io/tool/neuralnetworks/rsa/rsanotes/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jun Zhang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://zhangjunfelix.github.io/favicon.ico"
    }
  }
}
</script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ]
  });">
</script>


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://zhangjunfelix.github.io/" accesskey="h" title="Jun Zhang (Alt + H)">Jun Zhang</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://zhangjunfelix.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://zhangjunfelix.github.io/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://zhangjunfelix.github.io/teaching/" title="Teaching">
                    <span>Teaching</span>
                </a>
            </li>
            <li>
                <a href="https://zhangjunfelix.github.io/publications/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
            <li>
                <a href="https://zhangjunfelix.github.io/tool/" title="Toolbox">
                    <span>Toolbox</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Rational Speech Act-learning notes
    </h1>
    <div class="post-meta"><span title='2025-05-06 11:41:54 -0400 -0400'>May 6, 2025</span>&nbsp;·&nbsp;<span>66 min</span>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#i-background-of-rsa">I. Background of RSA</a></li>
        <li><a href="#ii-classical-view-of-meaning-vs-rsa-view-of-meaning">II. classical view of meaning vs. RSA view of meaning</a></li>
        <li><a href="#iii-the-basicstandardvanilla-rsa-model">III. The basic/standard/vanilla RSA model</a></li>
        <li><a href="#iv-more-on-the-mathematical-notations-for-those-not-good-at-mathematical-formalization-like-me-you-may-find-it-a-bit-repetitive">IV. More on the mathematical notations (for those not good at mathematical formalization, like me) (you may find it a bit repetitive)</a></li>
        <li><a href="#41-denotation-function">4.1 Denotation Function</a></li>
        <li><a href="#42-literal-listener-l_0">4.2 Literal Listener $L_0$</a></li>
        <li><a href="#43-pragmatic-speaker--s_1-">4.3 Pragmatic Speaker $ S_1 $</a></li>
        <li><a href="#44-pragmatic-listener--l_1-">4.4 Pragmatic Listener $ L_1 $</a></li>
        <li><a href="#v-a-complete-illustration-of-pragmatic-reasoning-in-rsa">V. A complete illustration of pragmatic reasoning in RSA</a></li>
        <li><a href="#vi-reference-games">VI. Reference Games</a></li>
        <li><a href="#61-what-are-reference-games">6.1 What Are Reference Games?</a></li>
        <li><a href="#62-reference-games--formal-modeling">6.2 Reference Games — Formal Modeling</a></li>
        <li><a href="#64-literal-listener">6.4 Literal Listener</a></li>
        <li><a href="#65-pragmatic-speaker">6.5 Pragmatic Speaker</a></li>
        <li><a href="#66-pragmatic-listener">6.6 Pragmatic Listener</a></li>
        <li><a href="#vii-indirect-speech-acts-in-rsa">VII. Indirect Speech Acts in RSA</a></li>
        <li><a href="#71-what-are-indirect-speech-acts">7.1 What Are Indirect Speech Acts?</a></li>
        <li><a href="#72-the-problem-why-indirect-speech-acts-seem-irrationalbut-arent">7.2 The Problem: Why Indirect Speech Acts Seem Irrational—But Aren’t</a></li>
        <li><a href="#73-the-deeper-problem">7.3 The Deeper Problem</a></li>
        <li><a href="#74-why-do-indirect-speech-acts-work-rsa-explanation">7.4 Why Do Indirect Speech Acts Work? (RSA Explanation)</a></li>
        <li><a href="#75-why-rsa-models-capture-this-rationality">7.5 Why RSA Models Capture This Rationality:</a></li>
        <li><a href="#74-rsa-perspective-on-indirectness">7.4 RSA Perspective on Indirectness</a></li>
        <li><a href="#extra">Extra</a></li>
        <li><a href="#extra-1-lambda-lambda">Extra #1 Lambda $\lambda$</a></li>
        <li><a href="#extra-2-softmax-function">Extra #2 Softmax Function</a></li>
        <li><a href="#extra-3-lambda-softmax-visualizer">Extra #3 Lambda Softmax Visualizer</a></li>
        <li><a href="#extra-4-bayesian-theorem">Extra 4 Bayesian Theorem</a></li>
        <li><a href="#sources">Sources</a></li>
      </ul>
    </li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p>&ldquo;&hellip; one of my avowed aims is to see talking as a special case or variety of purposive, indeed rational, behavior&rdquo; (Grice, 1975: 47)</p>
<hr>
<h3 id="i-background-of-rsa">I. Background of RSA<a hidden class="anchor" aria-hidden="true" href="#i-background-of-rsa">#</a></h3>
<p>The Rational Speech Act (RSA) framework was developed within the broader enterprise of <strong>Probabilistic Pragmatics</strong>, a rapidly growing approach in the study of meaning.</p>
<p><strong>Probabilistic pragmatics</strong> integrates insights from formal and experimental semantics and pragmatics, psycholinguistics, and computational cognitive science. It offers a unified framework for modeling how speakers produce and listeners interpret language in context.</p>
<h4 id="key-characteristics-of-probabilistic-pragmatics">Key Characteristics of Probabilistic Pragmatics<a hidden class="anchor" aria-hidden="true" href="#key-characteristics-of-probabilistic-pragmatics">#</a></h4>
<ul>
<li><strong>Formal Framework</strong>: Provides a structure for implementing hypotheses about how speakers contextually choose among utterance alternatives and how listeners arrive at context-sensitive interpretations.</li>
<li><strong>Formalizing Conversational Principles</strong>: Captures principles such as relevance, brevity, and helpful informativeness (closely aligned with Gricean maxims) in a formal and testable way.</li>
<li><strong>Probabilistic Processes</strong>: Treats language production and interpretation as fundamentally probabilistic, allowing for <strong><em>gradience and variability</em></strong> that classic models struggle to explain.</li>
<li><strong>Bounded Rationality</strong>: Models speakers and listeners as boundedly rational agents, integrating information in ways consistent with general cognitive constraints.</li>
<li><strong>Integration of Factors</strong>: Allows linguistic knowledge to interact with communicative pressures and <strong>subjective prior beliefs</strong> (world knowledge), acknowledging their central role in interpretation.</li>
<li><strong>Bridging Theoretical Traditions</strong>: Seeks to unify &ldquo;language-as-product&rdquo; (representational structure) and &ldquo;language-as-action&rdquo; (context-sensitive decision-making).</li>
<li><strong>Methodology</strong>: Strongly computational and data-driven—models are implemented as algorithms and tested against empirical data.</li>
<li><strong>Contrast with Classic Views</strong>: In contrast to categorical interpretations in classic models, probabilistic pragmatics treats meaning, informativeness, and alternatives as <strong>gradient</strong> and <strong>context-dependent</strong>.</li>
</ul>
<h4 id="types-of-theories-within-probabilistic-pragmatics">Types of Theories within Probabilistic Pragmatics<a hidden class="anchor" aria-hidden="true" href="#types-of-theories-within-probabilistic-pragmatics">#</a></h4>
<ul>
<li><strong><em>Game-theoretic approaches</em></strong> (e.g., Benz &amp; Stevens, 2018)</li>
<li><strong><em>Probabilistic but not fully Bayesian accounts</em></strong> (e.g., Qing &amp; Franke, 2014; Russell, 2012)</li>
</ul>
<p>The <strong>Rational Speech Act (RSA)</strong> framework stands out as arguably the <strong>most influential probabilistic model of pragmatic interpretation</strong>, integrating many of the features described above.</p>
<h3 id="ii-classical-view-of-meaning-vs-rsa-view-of-meaning">II. classical view of meaning vs. RSA view of meaning<a hidden class="anchor" aria-hidden="true" href="#ii-classical-view-of-meaning-vs-rsa-view-of-meaning">#</a></h3>
<table>
  <thead>
      <tr>
          <th><strong>Aspect</strong></th>
          <th><strong>The Classic View of Meaning</strong></th>
          <th><strong>The RSA View of Meaning</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Nature of Interpretation</strong></td>
          <td>Meaning is treated as <strong>categorical</strong>. Implicatures either arise or not; presuppositions either project or not. No room for gradience or interpretational uncertainty.</td>
          <td>Interpretation is <strong>probabilistic</strong>. Listeners hold posterior probability distributions over meanings, capturing uncertainty. The model supports variability and gradience in interpretation.</td>
      </tr>
      <tr>
          <td><strong>Informativeness</strong></td>
          <td>Defined <strong>categorically</strong> using entailment-based alternatives. One statement is more informative if it entails another. No contextual modulation.</td>
          <td>Defined <strong>gradually</strong> and <strong>contextually</strong>. Informativeness reflects how much a literal listener would learn after hearing the utterance (minimizing surprisal). It varies based on the context.</td>
      </tr>
      <tr>
          <td><strong>Alternatives</strong></td>
          <td>Treated as <strong>static and lexicalized</strong> (e.g., scales like &lt;all, some&gt;). The set of alternatives is fixed and not derived from context.</td>
          <td>Treated as <strong>flexible and context-sensitive</strong>. RSA is not a theory of alternatives but a framework for testing hypotheses. Alternatives can be adjusted based on data, and costly or complex alternatives can be penalized.</td>
      </tr>
      <tr>
          <td><strong>World Knowledge (Prior Beliefs)</strong></td>
          <td><strong>Not integrated</strong>. Classic models often exclude prior beliefs or world knowledge from formal reasoning, viewing them as nonlinguistic.</td>
          <td><strong>Formally integrated</strong>. RSA uses Bayes’ rule to model how listeners use <strong>subjective prior beliefs</strong> (world knowledge) to infer intended meaning. These priors do not need to be objectively accurate.</td>
      </tr>
      <tr>
          <td><strong>Inference Process</strong></td>
          <td>Gricean reasoning involves fixed premises. If these are met, an implicature is derived categorically; if not, it doesn’t arise. The speaker is assumed to choose the stronger alternative if it is true.</td>
          <td>RSA models interpretation as a <strong>signaling game</strong>. The speaker selects utterances balancing informativeness and cost. The listener uses <strong>Bayesian inference</strong> over utterances, context, and prior beliefs. This allows for <strong>gradient</strong> and variable inference.</td>
      </tr>
  </tbody>
</table>
<h3 id="iii-the-basicstandardvanilla-rsa-model">III. The basic/standard/vanilla RSA model<a hidden class="anchor" aria-hidden="true" href="#iii-the-basicstandardvanilla-rsa-model">#</a></h3>
<p>The basic <strong>Rational Speech Act (RSA)</strong> model treats language use as a <strong>signaling game</strong>. Speakers and listeners are modeled as agents who reason about:</p>
<ul>
<li>a space of utterances: $ U $</li>
<li>a space of possible meanings: $ M $</li>
</ul>
<blockquote>
<p>What Is a Signaling Game?<br>
The RSA model builds on the idea that language use is a kind of <strong>signaling game</strong> — a concept borrowed from game theory (Lewis, 1969).<br>
A <strong>signaling game</strong> models communication as an interaction between two rational agents:</p>
<ul>
<li>A <strong>speaker</strong> (or sender) who wants to convey a particular meaning.</li>
<li>A <strong>listener</strong> (or receiver) who observes the speaker’s utterance and tries to infer what meaning was intended.</li>
</ul>
<p>Key Ingredients of a Signaling Game</p>
<table>
  <thead>
      <tr>
          <th>Role</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Sender</strong></td>
          <td>Knows some private information (e.g., a meaning) and sends a signal (utterance).</td>
      </tr>
      <tr>
          <td><strong>Signal</strong></td>
          <td>The utterance or message the speaker chooses to send.</td>
      </tr>
      <tr>
          <td><strong>Receiver</strong></td>
          <td>Observes the signal and makes an inference about the speaker’s intended meaning.</td>
      </tr>
      <tr>
          <td><strong>Goal</strong></td>
          <td>Successful communication: the listener correctly infers the speaker’s intended meaning.</td>
      </tr>
  </tbody>
</table></blockquote>
<h4 id="31-semantic-foundation-denotation-functions">3.1 Semantic Foundation (Denotation Functions)<a hidden class="anchor" aria-hidden="true" href="#31-semantic-foundation-denotation-functions">#</a></h4>
<p>RSA begins with a <strong>literal semantics</strong>, defined by a denotation function (see SECTION 4.1 for more explanations):</p>
<p>$$
\llbracket \cdot \rrbracket : U \rightarrow M
$$</p>
<p>This function specifies the set of meanings that are <strong>literally compatible</strong> with each utterance. It is assumed to be shared by both speaker and listener, grounding the recursive pragmatic reasoning that follows.</p>
<h4 id="32-recursive-probabilistic-rules">3.2 Recursive Probabilistic Rules<a hidden class="anchor" aria-hidden="true" href="#32-recursive-probabilistic-rules">#</a></h4>
<p>Building on literal semantics, RSA defines a <strong>recursive hierarchy</strong> of production and interpretation rules. These involve probabilistic reasoning at each level and encode pragmatic inference.</p>
<h4 id="33-literal-listener--p_l_0-">3.3 Literal Listener $ P_{L_0} $<a hidden class="anchor" aria-hidden="true" href="#33-literal-listener--p_l_0-">#</a></h4>
<p>The <strong>literal listener</strong> interprets utterances based on their literal meaning (See SECTION 4.2 for its components and calculations). Their interpretation rule is:</p>
<p>$$
P_{L_0}(m \mid u) = \delta_{m \in \llbracket u \rrbracket} \cdot P(m)
$$</p>
<p>This means the listener updates their prior beliefs $P(m)$
only for meanings that are <strong>literally compatible</strong> with the utterance <em>u</em>. Here, $ \delta_{m \in \llbracket u \rrbracket} $ is an indicator function that returns 1 if $ m \in \llbracket u \rrbracket $, and 0 otherwise.</p>
<p>This step enforces the <strong>Gricean Quality maxim</strong>, ruling out meanings that are literally false.</p>
<h4 id="34-pragmatic-speaker--p_s_1-">3.4 Pragmatic Speaker $\ P_{S_1} $<a hidden class="anchor" aria-hidden="true" href="#34-pragmatic-speaker--p_s_1-">#</a></h4>
<p>The <strong>pragmatic speaker</strong> reasons about the literal listener’s interpretation to choose an utterance that best conveys the intended meaning $m$ (See SECTION 4.3 for more details). The speaker is modeled as a utility-maximizing agent:</p>
<p>$$
P_{S_1}(u \mid m) \propto \exp\left(\alpha \cdot U(u, m)\right)
$$</p>
<p>The <strong>utility function</strong> balances informativeness and cost:</p>
<p>$$
U(u, m) = \log P_{L_0}(m \mid u) - \text{Cost}(u)
$$</p>
<ul>
<li><strong>Informativeness</strong>: How well $u$ communicates $m$, i.e., how likely the literal listener is to infer $m$.</li>
<li><strong>Cost</strong>: A penalty for utterances that are longer, less frequent, or harder to retrieve.</li>
<li><strong>α</strong> (alpha): A rationality parameter controlling how strongly the speaker optimizes utility.</li>
</ul>
<p>This step reflects <strong>Gricean Quantity</strong> (informativeness) and <strong>Manner</strong> (cost) maxims.</p>
<h4 id="35-pragmatic-listener--p_l_1">3.5 Pragmatic Listener  $P_{L_1}$<a hidden class="anchor" aria-hidden="true" href="#35-pragmatic-listener--p_l_1">#</a></h4>
<p>The <strong>pragmatic listener</strong> inverts the speaker model to infer the likely intended meaning:</p>
<p>$$
P_{L_1}(m \mid u) \propto P_{S_1}(u \mid m) \cdot P(m)
$$</p>
<p>This is a <strong>Bayesian inference</strong> over possible meanings, integrating:</p>
<ul>
<li>A model of the speaker’s utterance choice $ P_{S_1} $</li>
<li>The listener’s <strong>prior beliefs</strong> about likely meanings $ P(m) $</li>
</ul>
<p>These <strong>priors</strong> encode <strong>world knowledge</strong> and subjective expectations about communicative goals.</p>
<h4 id="key-insight">Key Insight<a hidden class="anchor" aria-hidden="true" href="#key-insight">#</a></h4>
<p>&ldquo;RSA models replace Grice&rsquo;s maxims with a single, utility-theoretic version of the cooperative principle&rdquo; (Goodman &amp; Frank, 2016: 821)</p>
<p>RSA treats language understanding as a <strong>probabilistic inference problem</strong>, in contrast to classical models which assume categorical interpretation. The pragmatic listener arrives at a <strong>posterior probability distribution</strong> over meanings, capturing the inherent uncertainty in language comprehension.</p>
<p>This recursive reasoning structure links production and interpretation in a unified, probabilistic framework.</p>
<h3 id="iv-more-on-the-mathematical-notations-for-those-not-good-at-mathematical-formalization-like-me-you-may-find-it-a-bit-repetitive">IV. More on the mathematical notations (for those not good at mathematical formalization, like me) (you may find it a bit repetitive)<a hidden class="anchor" aria-hidden="true" href="#iv-more-on-the-mathematical-notations-for-those-not-good-at-mathematical-formalization-like-me-you-may-find-it-a-bit-repetitive">#</a></h3>
<p>To facilitate the explanation, a Scalar Implicature Game is used as an example:</p>
<blockquote>
<p>BASIC SCALAR IMPLICATURE GAME
In this context, there were Alex and 4 cookies on a plate.<br>
Meaning space: $ M = \{ m_0, m_1, m_2, m_3, m_4 \} $</p>
<p>Utterance space: $ U = \{ u_{\text{all}}, u_{\text{some}}, u_{\text{none}} \} $</p>
<p>Semantics:  $ u_{\text{all}}  = \{ m_4 \}$</p>
<p>            $ u_{\text{some}}  = \{ m_1, m_2, m_3, m_4 \} $</p>
<p>            $ u_{\text{none}}  = \{ m_0 \} $</p>
<p>Prior beliefs: $P(m_0)$ = $P(m_1)$ = $P(m_2)$ = $P(m_3)$ = $P(m_4)$ = $0.2$</p></blockquote>
<h3 id="41-denotation-function">4.1 Denotation Function<a hidden class="anchor" aria-hidden="true" href="#41-denotation-function">#</a></h3>
<p>The <strong>denotation function</strong> is a foundational concept in formal semantics and the Rational Speech Act (RSA) framework. In the RSA framework, the <strong>denotation function</strong> is written as:</p>
<p>$$
\llbracket \cdot \rrbracket : U \rightarrow M
$$
This means:</p>
<blockquote>
<p>The denotation function <em>maps each utterance $ u $ from the set of possible utterances $ U $ to a set of meanings $ M $ — specifically, the meanings for which the utterance is <strong>literally true</strong></em>.</p></blockquote>
<h4 id="411--step-by-step-logic-of-the-indicator">4.1.1  Step-by-Step Logic of the Indicator<a hidden class="anchor" aria-hidden="true" href="#411--step-by-step-logic-of-the-indicator">#</a></h4>
<p>Step 1: Evaluate Whether $m$ Is Literally Compatible with $u$</p>
<ul>
<li>Ask: <em>Does this meaning make the utterance true according to its literal meaning?</em></li>
</ul>
<p>Step 2: Output the Result</p>
<ul>
<li>If <strong>YES</strong> → Output <strong>1</strong> (keep this meaning for further consideration).</li>
<li>If <strong>NO</strong> → Output <strong>0</strong> (eliminate this meaning from consideration).</li>
</ul>
<h4 id="412-meaning-of-components">4.1.2 Meaning of Components<a hidden class="anchor" aria-hidden="true" href="#412-meaning-of-components">#</a></h4>
<p>An <strong>utterance</strong> $ u $ is something a speaker might say.<br>
Example:</p>
<blockquote>
<p>“Alex ate some of the cookies.”</p></blockquote>
<p>A <strong>meaning</strong> $ m $ refers to a possible state of the world — what might be true or false.<br>
For example: (in a context where there were four cookies)</p>
<ul>
<li>$ m_0 $: Alex ate 0 cookie.</li>
<li>$ m_1 $: Alex ate 1 cookie.</li>
<li>$ m_2 $: Alex ate 2 cookies.</li>
<li>$ m_3 $: Alex ate 3 cookies.</li>
<li>$ m_4 $: Alex ate 4 cookies.</li>
</ul>
<p>These meanings represent different situations the utterance might refer to.</p>
<h4 id="413--what-does-the-denotation-function-do">4.1.3  What does the denotation function do?<a hidden class="anchor" aria-hidden="true" href="#413--what-does-the-denotation-function-do">#</a></h4>
<p>It defines which of those meanings ($m_0$, $m_1$, $m_2$, $m_3$, $m_4$) make the utterance <strong>literally true</strong>.</p>
<p>For the utterance:
$
u = \text{“Alex ate some of the cookies.”}
$</p>
<p>The denotation is:
$
\llbracket u \rrbracket = { m_1, m_2, m_3, m_4 }
$
, because the statement is true in those situations, but <strong>not</strong> in $\ m_{0} $, where Alex ate no cookies.</p>
<h4 id="414-why-is-this-important-in-rsa">4.1.4 Why Is This Important in RSA?<a hidden class="anchor" aria-hidden="true" href="#414-why-is-this-important-in-rsa">#</a></h4>
<ul>
<li>It directly enforces Grice’s <strong>Quality Maxim</strong> at the literal level: Only meanings that make the utterance literally true are considered.</li>
<li>It performs an efficient filtering step before any deeper probabilistic reasoning.</li>
<li>This grounding in literal truth ensures that <strong>pragmatic reasoning starts from a shared semantic base</strong>.</li>
<li>It makes the RSA framework <strong>compositional</strong> and <strong>grounded in literal semantics</strong>.</li>
</ul>
<h4 id="415-summary-table">4.1.5 Summary Table<a hidden class="anchor" aria-hidden="true" href="#415-summary-table">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Concept</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Utterance $ u $</td>
          <td>A sentence a speaker might say (“Some of the apples are red”)</td>
      </tr>
      <tr>
          <td>Meaning $ m $</td>
          <td>A possible state of the world (e.g., all red, some red, none red)</td>
      </tr>
      <tr>
          <td>Denotation $ \llbracket u \rrbracket $</td>
          <td>The set of meanings for which the utterance is literally true</td>
      </tr>
      <tr>
          <td>Role in RSA</td>
          <td>Helps the literal listener filter out meanings that are literally impossible</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="42-literal-listener-l_0">4.2 Literal Listener $L_0$<a hidden class="anchor" aria-hidden="true" href="#42-literal-listener-l_0">#</a></h3>
<p>The <strong>Literal Listener</strong> — denoted as $L_0$ — interprets an utterance based solely on its <strong>literal semantics</strong>, without considering the speaker’s goals or alternative utterances.</p>
<h4 id="421-literal-listener-equations">4.2.1 Literal Listener Equations<a hidden class="anchor" aria-hidden="true" href="#421-literal-listener-equations">#</a></h4>
<h4 id="1-simplified-proportional-equation">1. Simplified (Proportional) Equation<a hidden class="anchor" aria-hidden="true" href="#1-simplified-proportional-equation">#</a></h4>
<p>$$
P_{L_0}(m \mid u) \propto \delta_{m \in \llbracket u \rrbracket} \cdot P(m)
$$</p>
<p>This means:</p>
<blockquote>
<p>The probability that the <strong>Literal Listener</strong> $L_0$ interprets utterance $u$ as meaning $m$ is proportional to whether the meaning $m$ is compatible with the literal semantics of $u$, multiplied by the prior probability of $ m $.</p></blockquote>
<ul>
<li>This computes <strong>unnormalized scores</strong>.</li>
</ul>
<h4 id="2-full-normalized-equation">2. Full Normalized Equation<a hidden class="anchor" aria-hidden="true" href="#2-full-normalized-equation">#</a></h4>
<p>$$
P_{L_0}(m \mid u) = \frac{\delta_{m \in \llbracket u \rrbracket} \cdot P(m)}{\displaystyle \sum_{m&rsquo;} \delta_{m&rsquo; \in \llbracket u \rrbracket} \cdot P(m&rsquo;)}
$$</p>
<ul>
<li>The denominator ensures that final probabilities <strong>sum to 1</strong>.</li>
<li>$\displaystyle \sum_{m&rsquo;}$ sums over all possible meanings $m&rsquo;$.</li>
</ul>
<p>NOTE:</p>
<blockquote>
<p>Denominator</p>
<p>$$
\sum_{m&rsquo;} \delta_{m&rsquo; \in \llbracket u \rrbracket} \cdot P(m&rsquo;)
$$</p>
<ul>
<li>This computes the total <strong>weight</strong> of all <strong>and only</strong> remaining possible meanings that make the utterance true (the meanings that make the utterance false would be <strong>left out</strong>).</li>
<li>It’s the sum over all possible meanings $ m&rsquo; $ that are <strong>compatible with the utterance</strong>.</li>
<li>This ensures the final probabilities <strong>sum to 1</strong> — this is the <strong>normalization step</strong>.</li>
</ul></blockquote>
<h4 id="why-are-there-two-versions">Why Are There Two Versions?<a hidden class="anchor" aria-hidden="true" href="#why-are-there-two-versions">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Version</th>
          <th>Purpose</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Proportional</td>
          <td>Computes unnormalized values. Useful for comparing relative likelihoods before normalization.</td>
      </tr>
      <tr>
          <td>Normalized</td>
          <td>Computes final, interpretable probabilities that sum to 1. Required for reporting and decision-making.</td>
      </tr>
  </tbody>
</table>
<blockquote>
<blockquote>
<p>&ldquo;$\propto$&rdquo; vs. &ldquo;=&rdquo;</p>
<ul>
<li>&ldquo;$\propto$&rdquo; is used in the unnormalized equation.</li>
<li>&ldquo;$\propto$&rdquo; This form&quot;&quot; gives unnormalized scores — only the relative values matter. You must normalize later by dividing by the total.</li>
<li>&ldquo;=&rdquo; means the result is a proper probability distribution.</li>
<li>&ldquo;=&rdquo; is used in the normalized equation.</li>
</ul></blockquote></blockquote>
<h4 id="422-explanation-of-each-component">4.2.2 Explanation of Each Component<a hidden class="anchor" aria-hidden="true" href="#422-explanation-of-each-component">#</a></h4>
<p>(1) $ P_{L_0}(m \mid u) $<br>
This is the <strong>posterior belief</strong> of the Literal Listener:</p>
<blockquote>
<p>How likely is it that the intended meaning is $ m $, given that the utterance $ u $ was heard?</p></blockquote>
<p>This represents the listener’s literal interpretation of the utterance: $ \llbracket u \rrbracket $</p>
<p>(2) $ \llbracket u \rrbracket $</p>
<p>The set of meanings where $ u $ is literally true.</p>
<p>(3) $ \delta_{m \in \llbracket u \rrbracket} $</p>
<p>This is an <strong>indicator/delta function</strong>. It returns 1 if the meaning $m$ is <strong>compatible</strong> with the literal meaning of the utterance; 0 otherwise</p>
<blockquote>
<p>In effect, this acts as a <strong>truth filter</strong> — it rules out meanings that are not literally possible.</p></blockquote>
<p>(4) $ P(m) $</p>
<p>This is the <strong>prior probability</strong> of each possible meaning, representing the listener’s <strong>expectations</strong> about the world <strong>before</strong> hearing the utterance.</p>
<h4 id="423-how-the-literal-listener-computes-interpretation">4.2.3 How the Literal Listener Computes Interpretation<a hidden class="anchor" aria-hidden="true" href="#423-how-the-literal-listener-computes-interpretation">#</a></h4>
<p>In the RSA model, the <strong>Literal Listener</strong> $ L_0 $ updates beliefs about the world after hearing an utterance $ u $, based purely on <strong>literal semantics</strong> and <strong>prior expectations</strong>.</p>
<h4 id="example-1-alex-ate-some-of-the-cookies">Example 1: “Alex ate some of the cookies.”<a hidden class="anchor" aria-hidden="true" href="#example-1-alex-ate-some-of-the-cookies">#</a></h4>
<h4 id="step-1-define-the-possible-meanings-m">Step 1: Define the Possible Meanings $M$<a hidden class="anchor" aria-hidden="true" href="#step-1-define-the-possible-meanings-m">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$ m_0 $</td>
          <td>Alex ate 0 cookies</td>
      </tr>
      <tr>
          <td>$ m_1 $</td>
          <td>Alex ate 1 cookie</td>
      </tr>
      <tr>
          <td>$ m_2 $</td>
          <td>Alex ate 2 cookies</td>
      </tr>
      <tr>
          <td>$ m_3 $</td>
          <td>Alex ate 3 cookies</td>
      </tr>
      <tr>
          <td>$ m_4 $</td>
          <td>Alex ate 4 cookies</td>
      </tr>
  </tbody>
</table>
<p>Assume a <strong>uniform prior</strong>:</p>
<p>$$
P(m_i) = 0.2 \quad \text{for all } i \in {0, 1, 2, 3, 4}
$$</p>
<h4 id="step-2-determine-literal-semantics-define-llbracket-u-rrbracket">Step 2: Determine Literal Semantics (Define $\llbracket u \rrbracket$)<a hidden class="anchor" aria-hidden="true" href="#step-2-determine-literal-semantics-define-llbracket-u-rrbracket">#</a></h4>
<p>Utterance:</p>
<blockquote>
<p>$ u = $ &ldquo;Alex ate some of the cookies.&rdquo;</p></blockquote>
<p>Literal semantics:</p>
<p>$$
\llbracket u \rrbracket = { m_1, m_2, m_3, m_4 }
$$</p>
<h4 id="step-3">Step 3<a hidden class="anchor" aria-hidden="true" href="#step-3">#</a></h4>
<h4 id="option-1-calculation-using-the-simplified-proportional-equation">Option 1: Calculation Using the <strong>Simplified (Proportional) Equation</strong><a hidden class="anchor" aria-hidden="true" href="#option-1-calculation-using-the-simplified-proportional-equation">#</a></h4>
<p>$$
P_{L_0}(m \mid u) \propto \delta_{m \in \llbracket u \rrbracket} \cdot P(m)
$$</p>
<h4 id="compute-unnormalized-values">Compute Unnormalized Values:<a hidden class="anchor" aria-hidden="true" href="#compute-unnormalized-values">#</a></h4>
<!--$$
\delta_{m \in \llbracket u \rrbracket} = 
\begin{cases}
0 & \text{if } m = m_0 \\ 
1 & \text{if } m \in \{ m_1, m_2, m_3, m_4 \}
\end{cases}
$$ -->
<div align="center">
$$
\delta_{m \in \llbracket u \rrbracket} = 
\begin{cases}
0 & \text{if } m = m_0, \\
1 & \text{if } m \in \{ m_1, m_2, m_3, m_4 \}.
\end{cases}
$$
</div>
<ul>
<li>$P_{L_0}(m_0 \mid u) = \delta_{m_0 \in \llbracket u \rrbracket} \cdot P(m_0) = 0 \cdot 0.2 = 0$</li>
<li>$P_{L_0}(m_i \mid u) = \delta_{m_i \in \llbracket u \rrbracket} \cdot P(m_i) = 1 \cdot 0.2 = 0.2$  for $i \in {1, 2, 3, 4}$</li>
</ul>
<p>At this stage, these values are <strong>unnormalized scores</strong>.</p>
<h4 id="compute-the-denominator-when-you-normalize">Compute the Denominator When You Normalize<a hidden class="anchor" aria-hidden="true" href="#compute-the-denominator-when-you-normalize">#</a></h4>
<ul>
<li>The sum of unnormalized scores <strong>is the denominator</strong>:</li>
</ul>
<p>$$
\text{Denominator} = 0.2 + 0.2 + 0.2 + 0.2 = 0.8
$$</p>
<h4 id="final-step-normalize-to-get-probabilities"><strong>Final Step: Normalize to Get Probabilities</strong><a hidden class="anchor" aria-hidden="true" href="#final-step-normalize-to-get-probabilities">#</a></h4>
<p>$$
P_{L_0}(m_i \mid u) = \frac{\text{Unnormalized Score}}{\text{Denominator}} = \frac{0.2}{0.8} = 0.25 \quad \text{for } m_i \in {1, 2, 3, 4}
$$</p>
<h4 id="option-2-calculation-using-the-full-normalized-equation">Option 2: Calculation Using the Full Normalized Equation<a hidden class="anchor" aria-hidden="true" href="#option-2-calculation-using-the-full-normalized-equation">#</a></h4>
<p>$$
P_{L_0}(m \mid u) = \frac{\delta_{m \in \llbracket u \rrbracket} \cdot P(m)}{\displaystyle \sum_{m&rsquo;} \delta_{m&rsquo; \in \llbracket u \rrbracket} \cdot P(m&rsquo;)}
$$</p>
<h4 id="compute-numerators">Compute Numerators:<a hidden class="anchor" aria-hidden="true" href="#compute-numerators">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>$\delta_{m \in \llbracket u \rrbracket}$</th>
          <th>$P(m)$</th>
          <th>Numerator ($\delta \cdot P(m)$)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_0$</td>
          <td>0 (ruled out)</td>
          <td>0.2</td>
          <td>0</td>
      </tr>
      <tr>
          <td>$m_1$</td>
          <td>1</td>
          <td>0.2</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_2$</td>
          <td>1</td>
          <td>0.2</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_3$</td>
          <td>1</td>
          <td>0.2</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_4$</td>
          <td>1</td>
          <td>0.2</td>
          <td>0.2</td>
      </tr>
  </tbody>
</table>
<h4 id="compute-denominator-normalization-constant">Compute Denominator (Normalization Constant):<a hidden class="anchor" aria-hidden="true" href="#compute-denominator-normalization-constant">#</a></h4>
<p>$$
\displaystyle \sum_{m&rsquo;} \delta_{m&rsquo; \in \llbracket u \rrbracket} \cdot P(m&rsquo;) = P(m_1) + P(m_2) + P(m_3) + P(m_4) = 0.2 + 0.2 + 0.2 + 0.2 = 0.8
$$</p>
<h4 id="compute-final-probabilities">Compute Final Probabilities:<a hidden class="anchor" aria-hidden="true" href="#compute-final-probabilities">#</a></h4>
<ul>
<li>For $m_0$ (ruled out):<br>
$$ P_{L_0}(m_0 \mid u) = 0 $$</li>
<li>For $m_i$ where $i \in {1, 2, 3, 4}$:<br>
$$ P_{L_0}(m_i \mid u) = \frac{0.2}{0.8} = 0.25 $$</li>
</ul>
<h4 id="final-interpretation">Final Interpretation<a hidden class="anchor" aria-hidden="true" href="#final-interpretation">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>Final Probability $P_{L_0}(m \mid u)$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_0$: 0 cookies</td>
          <td>0.0 (ruled out)</td>
      </tr>
      <tr>
          <td>$m_1$: 1 cookie</td>
          <td>0.25</td>
      </tr>
      <tr>
          <td>$m_2$: 2 cookies</td>
          <td>0.25</td>
      </tr>
      <tr>
          <td>$m_3$: 3 cookies</td>
          <td>0.25</td>
      </tr>
      <tr>
          <td>$m_4$: 4 cookies</td>
          <td>0.25</td>
      </tr>
  </tbody>
</table>
<hr>
<h4 id="example-2-alex-ate-none-of-the-cookies">Example 2: “Alex ate none of the cookies.”<a hidden class="anchor" aria-hidden="true" href="#example-2-alex-ate-none-of-the-cookies">#</a></h4>
<h4 id="step-1-define-the-possible-meanings-m-1">Step 1: Define the Possible Meanings $M$<a hidden class="anchor" aria-hidden="true" href="#step-1-define-the-possible-meanings-m-1">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$ m_0 $</td>
          <td>Alex ate 0 cookies</td>
      </tr>
      <tr>
          <td>$ m_1 $</td>
          <td>Alex ate 1 cookie</td>
      </tr>
      <tr>
          <td>$ m_2 $</td>
          <td>Alex ate 2 cookies</td>
      </tr>
      <tr>
          <td>$ m_3 $</td>
          <td>Alex ate 3 cookies</td>
      </tr>
      <tr>
          <td>$ m_4 $</td>
          <td>Alex ate 4 cookies</td>
      </tr>
  </tbody>
</table>
<p>Assume a <strong>uniform prior</strong>:</p>
<p>$$
P(m_i) = 0.2 \quad \text{for all } i \in {0, 1, 2, 3, 4}
$$</p>
<h4 id="step-2-determine-literal-semantics-define-llbracket-u-rrbracket-1">Step 2: Determine Literal Semantics (Define $\llbracket u \rrbracket$)<a hidden class="anchor" aria-hidden="true" href="#step-2-determine-literal-semantics-define-llbracket-u-rrbracket-1">#</a></h4>
<p>Utterance:</p>
<blockquote>
<p>$ u = $ &ldquo;Alex ate none of the cookies.&rdquo;</p></blockquote>
<p>Literal semantics:</p>
<p>$$
\llbracket u \rrbracket = { m_0 }
$$</p>
<h4 id="calculation-using-the-simplified-proportional-equation-option-1">Calculation Using the <strong>Simplified (Proportional) Equation</strong> (option 1)<a hidden class="anchor" aria-hidden="true" href="#calculation-using-the-simplified-proportional-equation-option-1">#</a></h4>
<p>$$
P_{L_0}(m \mid u) \propto \delta_{m \in \llbracket u \rrbracket} \cdot P(m)
$$</p>
<h4 id="compute-unnormalized-values-1">Compute Unnormalized Values:<a hidden class="anchor" aria-hidden="true" href="#compute-unnormalized-values-1">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>$\delta_{m \in \llbracket u \rrbracket}$</th>
          <th>$P(m)$</th>
          <th>Unnormalized Score</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_0$</td>
          <td>1 (kept)</td>
          <td>0.2</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_1$–$m_4$</td>
          <td>0 (ruled out)</td>
          <td>0.2</td>
          <td>0</td>
      </tr>
  </tbody>
</table>
<p>At this stage, the values are <strong>unnormalized scores</strong>.</p>
<h4 id="calculation-using-the-full-normalized-equation-option-2">Calculation Using the <strong>Full Normalized Equation</strong> (option 2)<a hidden class="anchor" aria-hidden="true" href="#calculation-using-the-full-normalized-equation-option-2">#</a></h4>
<p>$$
P_{L_0}(m \mid u) = \frac{\delta_{m \in \llbracket u \rrbracket} \cdot P(m)}{\displaystyle \sum_{m&rsquo;} \delta_{m&rsquo; \in \llbracket u \rrbracket} \cdot P(m&rsquo;)}
$$</p>
<h4 id="compute-the-denominator-normalization-constant">Compute the Denominator (Normalization Constant):<a hidden class="anchor" aria-hidden="true" href="#compute-the-denominator-normalization-constant">#</a></h4>
<p>$$
\text{Total} = P(m_0) = 0.2
$$</p>
<h4 id="compute-final-probabilities-1">Compute Final Probabilities:<a hidden class="anchor" aria-hidden="true" href="#compute-final-probabilities-1">#</a></h4>
<ul>
<li>For $m_0$:<br>
$$ P_{L_0}(m_0 \mid u) = \frac{0.2}{0.2} = 1.0 $$</li>
<li>For all other $m_i$:<br>
$$ P_{L_0}(m_i \mid u) = 0 \quad \text{for } i \in {1, 2, 3, 4} $$</li>
</ul>
<h4 id="final-interpretation-1">Final Interpretation<a hidden class="anchor" aria-hidden="true" href="#final-interpretation-1">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>Final Probability $P_{L_0}(m \mid u)$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$ m_0 $: 0 cookies</td>
          <td>1.0 (certain)</td>
      </tr>
      <tr>
          <td>$ m_1 $ to $ m_4 $</td>
          <td>0.0 (ruled out)</td>
      </tr>
  </tbody>
</table>
<hr>
<h4 id="example-3-alex-ate-all-of-the-cookies">Example 3: “Alex ate all of the cookies.”<a hidden class="anchor" aria-hidden="true" href="#example-3-alex-ate-all-of-the-cookies">#</a></h4>
<h4 id="step-1-define-the-possible-meanings-m-2">Step 1: Define the Possible Meanings $M$<a hidden class="anchor" aria-hidden="true" href="#step-1-define-the-possible-meanings-m-2">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$ m_0 $</td>
          <td>Alex ate 0 cookies</td>
      </tr>
      <tr>
          <td>$ m_1 $</td>
          <td>Alex ate 1 cookie</td>
      </tr>
      <tr>
          <td>$ m_2 $</td>
          <td>Alex ate 2 cookies</td>
      </tr>
      <tr>
          <td>$ m_3 $</td>
          <td>Alex ate 3 cookies</td>
      </tr>
      <tr>
          <td>$ m_4 $</td>
          <td>Alex ate 4 cookies</td>
      </tr>
  </tbody>
</table>
<p>Assume a <strong>uniform prior</strong>:</p>
<p>$$
P(m_i) = 0.2 \quad \text{for all } i \in {0, 1, 2, 3, 4}
$$</p>
<h4 id="step-2-determine-literal-semantics-define-llbracket-u-rrbracket-2">Step 2: Determine Literal Semantics (Define $\llbracket u \rrbracket$)<a hidden class="anchor" aria-hidden="true" href="#step-2-determine-literal-semantics-define-llbracket-u-rrbracket-2">#</a></h4>
<p>Utterance:</p>
<blockquote>
<p>$ u = $ &ldquo;Alex ate all of the cookies.&rdquo;</p></blockquote>
<p>Literal semantics:</p>
<p>$$
\llbracket u \rrbracket = { m_4 }
$$</p>
<h4 id="calculation-using-the-simplified-proportional-equation-option-1-1">Calculation Using the <strong>Simplified (Proportional) Equation</strong> (option 1)<a hidden class="anchor" aria-hidden="true" href="#calculation-using-the-simplified-proportional-equation-option-1-1">#</a></h4>
<p>$$
P_{L_0}(m \mid u) \propto \delta_{m \in \llbracket u \rrbracket} \cdot P(m)
$$</p>
<h4 id="compute-unnormalized-values-2">Compute Unnormalized Values<a hidden class="anchor" aria-hidden="true" href="#compute-unnormalized-values-2">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>$\delta_{m \in \llbracket u \rrbracket}$</th>
          <th>$P(m)$</th>
          <th>Unnormalized Score</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_4$</td>
          <td>1 (kept)</td>
          <td>0.2</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_0$–$m_3$</td>
          <td>0 (ruled out)</td>
          <td>0.2</td>
          <td>0</td>
      </tr>
  </tbody>
</table>
<p>At this stage, the values are <strong>unnormalized scores</strong>.</p>
<h4 id="calculation-using-the-full-normalized-equation-option-2-1">Calculation Using the <strong>Full Normalized Equation</strong> (option 2)<a hidden class="anchor" aria-hidden="true" href="#calculation-using-the-full-normalized-equation-option-2-1">#</a></h4>
<p>$$
P_{L_0}(m \mid u) = \frac{\delta_{m \in \llbracket u \rrbracket} \cdot P(m)}{\displaystyle \sum_{m&rsquo;} \delta_{m&rsquo; \in \llbracket u \rrbracket} \cdot P(m&rsquo;)}
$$</p>
<h4 id="compute-the-denominator-normalization-constant-1">Compute the Denominator (Normalization Constant):<a hidden class="anchor" aria-hidden="true" href="#compute-the-denominator-normalization-constant-1">#</a></h4>
<p>$$
\text{Total} = P(m_4) = 0.2
$$</p>
<h4 id="compute-final-probabilities-2">Compute Final Probabilities:<a hidden class="anchor" aria-hidden="true" href="#compute-final-probabilities-2">#</a></h4>
<ul>
<li>For $m_4$:<br>
$$ P_{L_0}(m_4 \mid u) = \frac{0.2}{0.2} = 1.0 $$</li>
<li>For all other $m_i$:<br>
$$ P_{L_0}(m_i \mid u) = 0 \quad \text{for } i \in {0, 1, 2, 3} $$</li>
</ul>
<h4 id="final-interpretation-2">Final Interpretation<a hidden class="anchor" aria-hidden="true" href="#final-interpretation-2">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th></th>
          <th>Final Probability $P_{L_0}(m \mid u)$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$ m_4 $: 4 cookies</td>
          <td></td>
          <td>1.0 (certain)</td>
      </tr>
      <tr>
          <td>$ m_0 $ to $ m_3 $</td>
          <td></td>
          <td>0.0 (ruled out)</td>
      </tr>
  </tbody>
</table>
<ul>
<li>The simplified equation identifies that only $m_4$ is possible but doesn’t directly provide the final, normalized probability.</li>
<li>The full equation confirms that after normalization, the listener is <strong>completely certain</strong> the intended meaning is $m_4$.</li>
</ul>
<hr>
<h4 id="what-these-examples-show">What These Examples Show<a hidden class="anchor" aria-hidden="true" href="#what-these-examples-show">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Utterance</th>
          <th>Remaining Possible Meanings</th>
          <th>Final Distribution</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>“Some”</td>
          <td>$ m_1 $ to $ m_4 $</td>
          <td>Evenly distributed (0.25 each)</td>
      </tr>
      <tr>
          <td>“None”</td>
          <td>$ m_0 $ only</td>
          <td>Certain (prob = 1)</td>
      </tr>
      <tr>
          <td>“All”</td>
          <td>$ m_4 $ only</td>
          <td>Certain (prob = 1)</td>
      </tr>
  </tbody>
</table>
<h4 id="423--a-more-detailed-example-based-on-the-full-equation-for-alex-ate-some-of-the-cookies">4.2.3  A more detailed example based on the full equation (for &ldquo;Alex ate some of the cookies.&rdquo;)<a hidden class="anchor" aria-hidden="true" href="#423--a-more-detailed-example-based-on-the-full-equation-for-alex-ate-some-of-the-cookies">#</a></h4>
<h4 id="what-are-m-and-m-in-the-literal-listener-formula">What Are $m$ and $m&rsquo;$ in the Literal Listener Formula?<a hidden class="anchor" aria-hidden="true" href="#what-are-m-and-m-in-the-literal-listener-formula">#</a></h4>
<p>$$
P_{L_0}(m \mid u) = \frac{\delta_{m \in \llbracket u \rrbracket} \cdot P(m)}{\displaystyle \sum_{m&rsquo;} \delta_{m&rsquo; \in \llbracket u \rrbracket} \cdot P(m&rsquo;)}
$$</p>
<table>
  <thead>
      <tr>
          <th>Symbol</th>
          <th>Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m$</td>
          <td>A <strong>specific possible meaning</strong> the listener is considering. Example: &ldquo;Alex ate 2 cookies.&rdquo;</td>
      </tr>
      <tr>
          <td>$m'$</td>
          <td>A <strong>placeholder variable</strong> for summing over <strong>all possible meanings</strong>.</td>
      </tr>
  </tbody>
</table>
<ul>
<li>The numerator computes the contribution of one particular meaning $m$.</li>
<li>The denominator sums over <strong>all meanings $m&rsquo;$</strong> to calculate the normalization constant.</li>
</ul>
<h4 id="example-alex-ate-some-of-the-cookies">Example: “Alex ate some of the cookies”<a hidden class="anchor" aria-hidden="true" href="#example-alex-ate-some-of-the-cookies">#</a></h4>
<h4 id="step-1-define-possible-meanings">Step 1: Define Possible Meanings<a hidden class="anchor" aria-hidden="true" href="#step-1-define-possible-meanings">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>Description</th>
          <th>Prior $P(m)$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_0$</td>
          <td>Alex ate 0 cookies</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_1$</td>
          <td>Alex ate 1 cookie</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_2$</td>
          <td>Alex ate 2 cookies</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_3$</td>
          <td>Alex ate 3 cookies</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_4$</td>
          <td>Alex ate 4 cookies</td>
          <td>0.2</td>
      </tr>
  </tbody>
</table>
<h4 id="step-2-determine-literal-semantics">Step 2: Determine Literal Semantics<a hidden class="anchor" aria-hidden="true" href="#step-2-determine-literal-semantics">#</a></h4>
<p>$$
\llbracket \text{“some”} \rrbracket = { m_1, m_2, m_3, m_4 }
$$</p>
<ul>
<li>“Some” means <strong>at least one cookie</strong> was eaten, so $m_0$ is ruled out.</li>
</ul>
<h4 id="step-3-compute-the-denominator-normalization">Step 3: Compute the Denominator (Normalization)<a hidden class="anchor" aria-hidden="true" href="#step-3-compute-the-denominator-normalization">#</a></h4>
<p>$$
\sum_{m&rsquo;} \delta_{m&rsquo; \in \llbracket u \rrbracket} \cdot P(m&rsquo;) = 1 * P(m_1) + 1 * P(m_2) + 1 * P(m_3) + 1 * P(m_4)
$$</p>
<p>$$
\text{Total} = 1 * 0.2 + 1 * 0.2 + 1 * 0.2 + 1 * 0.2 = 0.8
$$</p>
<h4 id="step-4-compute-final-probabilities-for-each-meaning">Step 4: Compute Final Probabilities for Each Meaning<a hidden class="anchor" aria-hidden="true" href="#step-4-compute-final-probabilities-for-each-meaning">#</a></h4>
<p>$$
P_{L_0}(m \mid u) = \frac{P(m)}{0.8} \quad \text{if } m \in \llbracket u \rrbracket
$$</p>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>$P_{L_0}(m \mid u)$ Calculation</th>
          <th>Final Probability $P_{L_0}(m \mid u)$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_0$</td>
          <td>Ruled out by literal meaning</td>
          <td>0.0</td>
      </tr>
      <tr>
          <td>$m_1$</td>
          <td>$\frac{0.2}{0.8}$</td>
          <td>0.25</td>
      </tr>
      <tr>
          <td>$m_2$</td>
          <td>$\frac{0.2}{0.8}$</td>
          <td>0.25</td>
      </tr>
      <tr>
          <td>$m_3$</td>
          <td>$\frac{0.2}{0.8}$</td>
          <td>0.25</td>
      </tr>
      <tr>
          <td>$m_4$</td>
          <td>$\frac{0.2}{0.8}$</td>
          <td>0.25</td>
      </tr>
  </tbody>
</table>
<ul>
<li>$m$: The <strong>specific meaning</strong> you are evaluating.</li>
<li>$m&rsquo;$: The <strong>variable used to sum over all meanings</strong> during normalization.</li>
<li>The literal listener <strong>rules out incompatible meanings</strong> and redistributes prior beliefs over the remaining possibilities.</li>
</ul>
<h4 id="424-why-normalize">4.2.4 Why normalize?<a hidden class="anchor" aria-hidden="true" href="#424-why-normalize">#</a></h4>
<p>In the RSA model (and probabilistic modeling more broadly), <strong>normalization</strong> is the process of turning raw or unnormalized scores into a <strong>valid probability distribution</strong> — one where all values are between 0 and 1 and <strong>sum to exactly 1</strong>.</p>
<p>Probabilities must satisfy two conditions: (1) They must be <strong>non-negative</strong>; (2) They must <strong>sum to 1</strong>.</p>
<p>However, RSA agents (like the Literal Listener or Pragmatic Speaker) often compute values that are <strong>proportional to probabilities</strong>, not properly normalized. So we apply <strong>normalization</strong> to make them valid.</p>
<p>Normalization is needed whenever probabilities are defined <strong>up to proportionality</strong>.<br>
Normalization:</p>
<p>(1) Converts raw scores into valid probabilities<br>
(2) Filters and redistributes prior beliefs based on literal truth<br>
(3) Is essential for computing meaningful results in RSA and Bayesian models</p>
<p>It is the final — and often implicit — step in probabilistic interpretation.</p>
<h4 id="425--summary">4.2.5  Summary<a hidden class="anchor" aria-hidden="true" href="#425--summary">#</a></h4>
<p>The Literal Listener <strong>does not</strong> reason about why the speaker chose one utterance over another. It simply filters meanings based on:</p>
<ul>
<li>Literal truth conditions (defined by $ \llbracket u \rrbracket $)</li>
<li>Prior beliefs about what is likely</li>
</ul>
<p>This <strong>provides the foundation</strong> for higher levels of reasoning in RSA, where more sophisticated listeners and speakers reason about each other recursively.</p>
<hr>
<h3 id="43-pragmatic-speaker--s_1-">4.3 Pragmatic Speaker $ S_1 $<a hidden class="anchor" aria-hidden="true" href="#43-pragmatic-speaker--s_1-">#</a></h3>
<p>The <strong>Pragmatic Speaker</strong> (denoted as $ S_1 $) models the speaker as a <strong>rational agent</strong> who chooses utterances strategically. The speaker&rsquo;s goal is to:</p>
<ul>
<li>Communicate the intended meaning effectively (<strong>informativeness</strong>).</li>
<li>Minimize production effort or cost (<strong>cost</strong>).</li>
</ul>
<p>This balances the <strong>Gricean Quantity Maxim</strong> (be informative) and the <strong>Manner Maxim</strong> (avoid unnecessary effort).</p>
<h4 id="431-the-pragmatic-speaker-formula">4.3.1 The Pragmatic Speaker Formula<a hidden class="anchor" aria-hidden="true" href="#431-the-pragmatic-speaker-formula">#</a></h4>
<h4 id="1-simplified-proportional-equation-1">1. Simplified (Proportional) Equation<a hidden class="anchor" aria-hidden="true" href="#1-simplified-proportional-equation-1">#</a></h4>
<p>$$
P_{S_1}(u \mid m) \propto \exp \left( \alpha \cdot U(u, m) \right)
$$<br>
where $ U(u, m) = \log P_{L_0}(m \mid u) - \text{cost}(u) $</p>
<p>This means:</p>
<blockquote>
<p>&ldquo;The probability that the <strong>Pragmatic Speaker</strong> $ S_1 $ will produce utterance $ u $ given that they want to communicate meaning $ m $, is proportional to the exponential of $ \alpha $ times the utility of utterance $ u $ for meaning $ m $.&rdquo;</p></blockquote>
<blockquote>
<p>This formula defines the speaker&rsquo;s production $ u $ as softmax optimizing $ u $&rsquo;s utility for communicating $ m $, $ U (u, m) $.</p></blockquote>
<p>Meaning of its components</p>
<ul>
<li>$ P_{S_1}(u \mid m) $: Probability the speaker chooses utterance $ u $ to communicate meaning $ m $.</li>
<li>$ \alpha $: Rationality parameter controlling how strongly the speaker optimizes utility.</li>
<li>$ U(u, m) $: Utility of utterance $ u $ for communicating meaning $ m $.</li>
<li><strong>$ \propto $</strong> is read as <strong>“proportional to”</strong>. It is used in equations to express that one quantity <strong>scales with another</strong>, but the exact value is not yet determined — <strong>because we still need to compute a normalization step</strong>.</li>
<li>This computes <strong>unnormalized scores</strong> for each utterance $u$.</li>
</ul>
<h4 id="2-full-normalized-equation-softmax-function">2. Full Normalized Equation (Softmax Function)<a hidden class="anchor" aria-hidden="true" href="#2-full-normalized-equation-softmax-function">#</a></h4>
<p>$$
P_{S_1}(u \mid m) = \frac{\exp \left( \alpha \cdot U(u, m) \right)}{\displaystyle \sum_{u&rsquo;} \exp \left( \alpha \cdot U(u&rsquo;, m) \right)}
$$</p>
<ul>
<li>This computes the final probabilities by normalizing the unnormalized scores.</li>
<li>The denominator ensures that probabilities <strong>sum to 1</strong>.</li>
</ul>
<hr>
<h4 id="432-understanding-utility-uu-m">4.3.2 Understanding Utility $U(u, m)$<a hidden class="anchor" aria-hidden="true" href="#432-understanding-utility-uu-m">#</a></h4>
<p>The utility function balances <strong>informativeness</strong> and <strong>cost</strong>:</p>
<p>An utterance&rsquo;s utility $ U (u, m) $ is defined as a trade-off between the utterance&rsquo;s <strong>informativeness</strong> as characterized by $ P_{L_0}(m \mid u) $ &ndash;how likely it is that a literal listener will corectly infer $ m $ from $ u $&rsquo;s literal semantics alone&ndash; and its cost, as defined in the utility function:</p>
<p>$$
U(u, m) = \log P_{L_0}(m \mid u) - \text{cost}(u)
$$</p>
<ul>
<li>$ \log P_{L_0}(m \mid u) $: <strong>Informativeness</strong> — how well the literal listener would infer $ m $ from $ u $.</li>
<li>$ \text{cost}(u) $: <strong>Cost</strong> of producing utterance $ u $ (e.g., effort, complexity, length).</li>
</ul>
<h4 id="how-does-this-work">How Does This Work?<a hidden class="anchor" aria-hidden="true" href="#how-does-this-work">#</a></h4>
<p>(1) <strong>Informativeness</strong>:</p>
<ul>
<li>The informativeness term captures the spirit of the Gricean Quantity Maxims (even Relation).</li>
<li>If the utterance makes the intended meaning very likely for the literal listener, its utility is high.</li>
<li>Example: “All the cookies were eaten” perfectly conveys the state where all cookies are gone.</li>
</ul>
<p>(2) <strong>Cost</strong>:</p>
<ul>
<li>The cost term captures the spirit of part of the Gricean Maxim of Mnner: the cheaper (e.g., shorter) the utterance, the better.</li>
<li>Longer or more complex utterances may have higher cost.</li>
<li>Example: “Alex ate all four cookies” might be costlier than simply saying “Alex ate all.”</li>
</ul>
<p>(3) <strong>Balancing</strong>:</p>
<ul>
<li>The pragmatic speaker prefers utterances that are <strong>informative but low in cost</strong>.</li>
<li>If two utterances are equally informative, the speaker prefers the cheaper one.</li>
</ul>
<h4 id="433-the-rationality-parameter-alpha">4.3.3 The Rationality Parameter $\alpha$<a hidden class="anchor" aria-hidden="true" href="#433-the-rationality-parameter-alpha">#</a></h4>
<ul>
<li>$ \alpha $ is a utlity-scaling parameter, which governs the extent to which the speaker is a utility-maximizing agent.</li>
<li>If $ \alpha = 0 $: The speaker chooses utterances <strong>randomly</strong>.</li>
<li>If $ \alpha = 1 $: The speaker chooses utterances <strong>proportionally to utility</strong> (probability matching).</li>
<li>If $ \alpha \to \infty $: The speaker ceases to choose utterances probabilistically and always chooses the <strong>utterance with the highest utility</strong> (fully rational).</li>
</ul>
<p>This softmax function makes the model flexible, allowing for varying levels of rationality in speaker behavior.</p>
<p>To compute pragmatic speaker probabilities, we must set a value for $ \alpha $ and define the cost of utterances.</p>
<table>
  <thead>
      <tr>
          <th>$\alpha$ Value</th>
          <th>Speaker Behavior</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>0</td>
          <td>Random utterance choice.</td>
      </tr>
      <tr>
          <td>1</td>
          <td>Probability matches utility.</td>
      </tr>
      <tr>
          <td>$\to \infty$</td>
          <td>Always chooses the utterance with the highest utility.</td>
      </tr>
  </tbody>
</table>
<h4 id="434-understanding-propto-and-why-normalize">4.3.4 Understanding $\propto$ and Why Normalize?<a hidden class="anchor" aria-hidden="true" href="#434-understanding-propto-and-why-normalize">#</a></h4>
<p><strong>$\propto$</strong> means <strong>“proportional to”</strong>.<br>
It indicates the values are relative but not yet normalized.</p>
<blockquote>
<p>The probability of choosing utterance $ u $ to express meaning $ m $ is <strong>proportional to</strong> the exponentiated utility, but this is <strong>not yet the final probability</strong>.</p></blockquote>
<h4 id="why-normalize">Why Normalize?<a hidden class="anchor" aria-hidden="true" href="#why-normalize">#</a></h4>
<ul>
<li>Probabilities must sum to <strong>1</strong>.</li>
<li>The full normalized equation ensures this by dividing unnormalized scores by their total sum.</li>
</ul>
<h4 id="435-worked-example-cookie-scenario">4.3.5 Worked Example: Cookie Scenario<a hidden class="anchor" aria-hidden="true" href="#435-worked-example-cookie-scenario">#</a></h4>
<p>Suppose the speaker wants to communicate that &ldquo;<strong>Alex ate all the cookies</strong>&rdquo;, i.e., $m_4$.</p>
<h4 id="step-1-compute-literal-listener-beliefs">Step 1: Compute Literal Listener Beliefs<a hidden class="anchor" aria-hidden="true" href="#step-1-compute-literal-listener-beliefs">#</a></h4>
<p>$$
P_{L_0}(m_4 \mid u_{\text{all}}) = 1.0, \quad P_{L_0}(m_4 \mid u_{\text{some}}) = 0.25
$$</p>
<blockquote>
<p>e.g.: Computing $P_{L_0}(m_4 \mid u_{\text{some}}) = 0.25$</p>
<p><strong>Define the Possible Meanings $M$</strong>: $ m_0 $ = 0 cookies; $ m_1 $ = 1 cookies; $ m_2 $ = 2 cookies; $ m_3 $ = 3 cookies; $ m_4 $ = 4 cookies;<br>
<strong>uniform priors</strong>: $ P(m_i) = 0.2 \quad \text{for all } i \in {0, 1, 2, 3, 4} $</p>
<h4 id="step-2-determine-literal-semantics-define-llbracket-u-rrbracket-3">Step 2: Determine Literal Semantics (Define $\llbracket u \rrbracket$)<a hidden class="anchor" aria-hidden="true" href="#step-2-determine-literal-semantics-define-llbracket-u-rrbracket-3">#</a></h4>
<p>Utterance:<br>
$ u = $ &ldquo;Alex ate some of the cookies.&rdquo;</p>
<p>Literal semantics:</p>
<p>$$
\llbracket u \rrbracket = { m_1, m_2, m_3, m_4 }
$$</p>
<h4 id="step-3-calculation-using-the-simplified-proportional-equation">Step 3: Calculation Using the <strong>Simplified (Proportional) Equation</strong><a hidden class="anchor" aria-hidden="true" href="#step-3-calculation-using-the-simplified-proportional-equation">#</a></h4>
<p>$$
P_{L_0}(m \mid u) \propto \delta_{m \in \llbracket u \rrbracket} \cdot P(m)
$$</p>
<h4 id="compute-unnormalized-values-3">Compute Unnormalized Values:<a hidden class="anchor" aria-hidden="true" href="#compute-unnormalized-values-3">#</a></h4>
<div align="center">
$$
\delta_{m \in \llbracket u \rrbracket} = 
\begin{cases}
0 & \text{if } m = m_0, \\
1 & \text{if } m \in \{ m_1, m_2, m_3, m_4 \}.
\end{cases}
$$
</div>
<ul>
<li>
<p>$P_{L_0}(m_i \mid u) = \delta_{m_i \in \llbracket u \rrbracket} \cdot P(m_i) = 1 \cdot 0.2 = 0.2$  for $i \in {1, 2, 3, 4}$</p>
</li>
<li>
<p>$P_{L_0}(m_4 \mid u_{some}) = \delta_{m_4 \in \llbracket u_{some} \rrbracket} \cdot P(m_4) = 1 \cdot 0.2 = 0.2$</p>
</li>
<li>
<p>At this stage, these values are <strong>unnormalized scores</strong>.</p>
</li>
</ul>
<h4 id="compute-the-denominator-when-you-normalize-1">Compute the Denominator When You Normalize<a hidden class="anchor" aria-hidden="true" href="#compute-the-denominator-when-you-normalize-1">#</a></h4>
<ul>
<li>The sum of unnormalized scores <strong>is the denominator</strong> (all meanings $m_i$ that make $u_{some}$ true):</li>
</ul>
<p>$$
\text{Denominator} = 0.2 + 0.2 + 0.2 + 0.2 = 0.8
$$</p>
<h4 id="final-step-normalize-to-get-probabilities-1"><strong>Final Step: Normalize to Get Probabilities</strong><a hidden class="anchor" aria-hidden="true" href="#final-step-normalize-to-get-probabilities-1">#</a></h4>
<p>$$
P_{L_0}(m_4 \mid u) = \frac{\text{Unnormalized Score}}{\text{Denominator}} = \frac{0.2}{0.8} = 0.25
$$</p></blockquote>
<h4 id="step-2-compute-utilities">Step 2: Compute Utilities<a hidden class="anchor" aria-hidden="true" href="#step-2-compute-utilities">#</a></h4>
<p>Assume $\text{cost}(u) = 0$ for simplicity.</p>
<ul>
<li>$U(u_{\text{all}}, m_4) = \log P_{L_0}(m_4 \mid u_{all}) - \text{cost}(u_{all}) = \log(1.0) - 0 = 0 - 0 = 0$</li>
<li>$U(u_{\text{some}}, m_4) = \log P_{L_0}(m_4 \mid u_{some}) - \text{cost}(u_{some}) = \log(0.25) - 0 \approx -1.386 -0 \approx -1.386 $</li>
</ul>
<h4 id="step-3-compute-unnormalized-scores-assume-alpha--1">Step 3: Compute Unnormalized Scores (Assume $\alpha = 1$)<a hidden class="anchor" aria-hidden="true" href="#step-3-compute-unnormalized-scores-assume-alpha--1">#</a></h4>
<ul>
<li>$P_{S_1}(u_{all} \mid m_4) \propto \exp \left( \alpha \cdot U(u_{all}, m_4) \right) \propto \exp(1 \cdot 0) \propto \exp(0) = 1$</li>
<li>$P_{S_1}(u_{some} \mid m_4) \propto \exp \left( \alpha \cdot U(u_{some}, m_4) \right) \propto \exp(1 \cdot -1.386) \propto \exp(-1.386) \approx 0.25$</li>
</ul>
<h4 id="step-4-normalize">Step 4: Normalize<a hidden class="anchor" aria-hidden="true" href="#step-4-normalize">#</a></h4>
<p>$$
\text{Total} = 1 + 0.25 = 1.25
$$</p>
<ul>
<li>$P_{S_1}(u_{\text{all}} \mid m_4) = \frac{1}{1.25} = 0.8$</li>
<li>$P_{S_1}(u_{\text{some}} \mid m_4) = \frac{0.25}{1.25} = 0.2$</li>
</ul>
<h4 id="final-interpretation-3">Final Interpretation<a hidden class="anchor" aria-hidden="true" href="#final-interpretation-3">#</a></h4>
<ul>
<li>The speaker is <strong>four times more likely</strong> to say &ldquo;all&rdquo; than &ldquo;some&rdquo; when Alex ate all the cookies.</li>
</ul>
<h4 id="436-a-complete-example-cookies-scenario">4.3.6 A Complete Example: Cookies Scenario<a hidden class="anchor" aria-hidden="true" href="#436-a-complete-example-cookies-scenario">#</a></h4>
<p><strong>Possible Meanings (M):</strong></p>
<ul>
<li>$ m_1 $: Alex ate <strong>all</strong> the cookies.</li>
<li>$ m_2 $: Alex ate <strong>some but not all</strong> cookies.</li>
<li>$ m_3 $: Alex ate <strong>none</strong> of the cookies.</li>
</ul>
<p><strong>Possible Utterances (U):</strong></p>
<ul>
<li>$ u_{\text{some}} $: “Alex ate some cookies.”</li>
<li>$ u_{\text{all}} $: “Alex ate all the cookies.”</li>
</ul>
<blockquote>
<h4 id="step-1-compute-literal-listeners-belief">Step 1: Compute Literal Listener’s Belief<a hidden class="anchor" aria-hidden="true" href="#step-1-compute-literal-listeners-belief">#</a></h4>
<p>Assume:</p>
<p>$$
P_{L_0}(m_1 \mid u_{\text{all}}) = 1.0,\quad P_{L_0}(m_1 \mid u_{\text{some}}) = 0.25
$$</p>
<p>This means:</p>
<ul>
<li>“All” perfectly communicates that Alex ate all cookies.</li>
<li>“Some” leaves uncertainty.</li>
</ul>
<h4 id="step-2-compute-utility">Step 2: Compute Utility<a hidden class="anchor" aria-hidden="true" href="#step-2-compute-utility">#</a></h4>
<p>Assume cost is zero for simplicity.</p>
<p>$$  U(u_{\text{all}}, m_1) = \log(1.0) = 0  $$</p>
<p>$$ U(u_{\text{some}}, m_1) = \log(0.25) = -1.386  $$</p>
<h4 id="step-3-compute-production-probabilities-assume--alpha--1-">Step 3: Compute Production Probabilities (Assume $ \alpha = 1 $)<a hidden class="anchor" aria-hidden="true" href="#step-3-compute-production-probabilities-assume--alpha--1-">#</a></h4>
<p>$$ P_{S_1}(u_{\text{all}} \mid m_1) \propto \exp(1 \cdot 0) = 1  $$</p>
<p>$$
P_{S_1}(u_{\text{some}} \mid m_1) \propto \exp(1 \cdot -1.386) \approx 0.25
$$</p>
<p>Normalize:</p>
<p>$$ P_{S_1}(u_{\text{all}} \mid m_1) = \frac{1}{1 + 0.25} = 0.8   $$
$$
P_{S_1}(u_{\text{some}} \mid m_1) = \frac{0.25}{1 + 0.25} = 0.2
$$</p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>The speaker is <strong>four times more likely</strong> to say “all” than “some” when Alex ate all cookies.</li>
</ul></blockquote>
<h4 id="437-how-to-understand-figure-2">4.3.7 How to understand Figure 2?<a hidden class="anchor" aria-hidden="true" href="#437-how-to-understand-figure-2">#</a></h4>
<h4 id="figure-2-rsa-speaker-production-probabilities"><strong>Figure 2: RSA Speaker Production Probabilities</strong><a hidden class="anchor" aria-hidden="true" href="#figure-2-rsa-speaker-production-probabilities">#</a></h4>
<p align="center">
  <img src="/images/figure2Degen2023.png" alt="Figure 2: Pragmatic Speaker Production Probabilities" width="300px">
</p>
<p>Figure 2 in Degen (2023) visualizes how the <strong>Pragmatic Speaker&rsquo;s production probabilities</strong> change as a function of the <strong>rationality parameter $\alpha$</strong>, assuming the intended meaning is that Alex ate all the cookies ($m_4$).</p>
<h4 id="rsa-model-equation-for-the-pragmatic-speaker">RSA Model Equation for the Pragmatic Speaker<a hidden class="anchor" aria-hidden="true" href="#rsa-model-equation-for-the-pragmatic-speaker">#</a></h4>
<p>$$
P_{S_1}(u \mid m) \propto \exp \left( \alpha \cdot U(u, m) \right)
$$</p>
<ul>
<li>$U(u, m)$ is the <strong>utility</strong> of utterance $u$ for expressing meaning $m$.</li>
<li>$\alpha$ controls how strongly the speaker favors high-utility utterances.</li>
</ul>
<h4 id="utility-calculation">Utility Calculation<a hidden class="anchor" aria-hidden="true" href="#utility-calculation">#</a></h4>
<p>$$
U(u, m) = \log P_{L_0}(m \mid u) - \text{cost}(u)
$$</p>
<ul>
<li>For the meaning $m_4$ (Alex ate all cookies):
<ul>
<li>$P_{L_0}(m_4 \mid u_{\text{all}}) = 1$ (perfectly informative)</li>
<li>$P_{L_0}(m_4 \mid u_{\text{some}}) = 0.25$ (less informative)</li>
</ul>
</li>
</ul>
<h4 id="how-to-read-the-figure">How to Read the Figure<a hidden class="anchor" aria-hidden="true" href="#how-to-read-the-figure">#</a></h4>
<ul>
<li><strong>Y-Axis</strong>: Probability of choosing each utterance ($P_{S_1}(u \mid m_4)$).</li>
<li><strong>X-Axis</strong>: Value of the rationality parameter $\alpha$.
<ul>
<li>Low $\alpha$: The speaker is less rational and chooses utterances almost randomly.</li>
<li>High $\alpha$: The speaker strongly prefers the more informative utterance (“all”).</li>
</ul>
</li>
</ul>
<p>As $\alpha$ increases, the model predicts the speaker will overwhelmingly prefer to say <strong>“all”</strong> rather than <strong>“some”</strong>.</p>
<hr>
<h4 id="what-this-demonstrates">What This Demonstrates<a hidden class="anchor" aria-hidden="true" href="#what-this-demonstrates">#</a></h4>
<ul>
<li>The figure shows the effect of the <strong>softmax function</strong>:<br>
Higher utilities lead to higher probabilities, but this relationship is smoothed and modulated by $\alpha$.</li>
<li>This accounts for graded, probabilistic behavior rather than hard, deterministic choices.</li>
<li>The RSA model predicts that humans adjust their speech behavior depending on context, goals, and cognitive resources (modeled by $\alpha$).</li>
</ul>
<h4 id="438-understanding-exp-in-the-pragmatic-speaker-equation">4.3.8 Understanding $\exp()$ in the Pragmatic Speaker Equation<a hidden class="anchor" aria-hidden="true" href="#438-understanding-exp-in-the-pragmatic-speaker-equation">#</a></h4>
<p>In the Pragmatic Speaker formula:</p>
<p>$$
P_{S_1}(u \mid m) \propto \exp\left( \alpha \cdot U(u, m) \right)
$$</p>
<p>The $\exp()$ function plays a critical role in transforming <strong>utilities into positive values</strong> suitable for computing probabilities.</p>
<hr>
<h4 id="what-does-exp-mean">What Does $\exp()$ Mean?<a hidden class="anchor" aria-hidden="true" href="#what-does-exp-mean">#</a></h4>
<ul>
<li>$\exp(x)$ is the <strong>exponential function</strong>, equivalent to $e^x$, where $e \approx 2.718$.</li>
<li>It converts utility values (which can be negative or positive) into <strong>positive scores</strong>.</li>
<li>This ensures that all computed scores for probabilities remain <strong>non-negative</strong>, which is required for valid probability calculations.</li>
</ul>
<hr>
<h4 id="why-use-exp">Why Use $\exp()$?<a hidden class="anchor" aria-hidden="true" href="#why-use-exp">#</a></h4>
<ol>
<li>
<p><strong>Transforms Utilities into Positive Scores</strong></p>
<ul>
<li>Since utilities can be negative, exponentiation ensures that all scores used for probability calculations are positive.</li>
</ul>
</li>
<li>
<p><strong>Amplifies Differences Between Utilities</strong></p>
<ul>
<li>Larger utilities result in exponentially larger scores, making higher-utility utterances significantly more likely.</li>
</ul>
</li>
<li>
<p><strong>Creates Smooth, Graded Probabilities (Softmax Function)</strong></p>
<ul>
<li>Even utterances with lower utility still have a non-zero chance of being selected, depending on the value of $\alpha$.</li>
</ul>
</li>
</ol>
<hr>
<h4 id="example-calculations">Example Calculations<a hidden class="anchor" aria-hidden="true" href="#example-calculations">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Utility $U(u, m)$</th>
          <th>$\exp(U(u, m))$</th>
          <th>Interpretation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>0</td>
          <td>1.0</td>
          <td>Baseline value.</td>
      </tr>
      <tr>
          <td>-1.386</td>
          <td>0.25</td>
          <td>Lower utility, lower score.</td>
      </tr>
      <tr>
          <td>2</td>
          <td>$\exp(2) \approx 7.389$</td>
          <td>High utility, very large score.</td>
      </tr>
  </tbody>
</table>
<hr>
<h4 id="key-takeaways">Key Takeaways<a hidden class="anchor" aria-hidden="true" href="#key-takeaways">#</a></h4>
<ul>
<li>$\exp()$ ensures that probabilities are <strong>positive and differentiable</strong>, enabling the use of softmax for probabilistic choices.</li>
<li>It reflects the intuition that the speaker is exponentially more likely to choose high-utility utterances, but not exclusively.</li>
<li>This function, combined with $\alpha$, controls how deterministic or probabilistic the speaker’s choices are.</li>
</ul>
<hr>
<h4 id="439-summary-table">4.3.9 Summary Table<a hidden class="anchor" aria-hidden="true" href="#439-summary-table">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$ P_{S_1}(u \mid m) $</td>
          <td>Speaker’s probability of choosing $ u $ given $ m $</td>
      </tr>
      <tr>
          <td>$ U(u, m) $</td>
          <td>Utility of utterance $ u $ for meaning $ m $</td>
      </tr>
      <tr>
          <td>$ \alpha $</td>
          <td>Rationality parameter controlling how deterministic the speaker is</td>
      </tr>
      <tr>
          <td>Cost $ \text{cost}(u) $</td>
          <td>Production effort or complexity of the utterance</td>
      </tr>
  </tbody>
</table>
<p>In sum, the pragmatic speaker chooses utterances strategically, balancing informativeness and cost. This formalization explains why speakers might sometimes <strong>avoid perfectly informative utterances</strong> when they are costly, or why they might <strong>choose simpler alternatives</strong> when the difference in informativeness is</p>
<hr>
<h3 id="44-pragmatic-listener--l_1-">4.4 Pragmatic Listener $ L_1 $<a hidden class="anchor" aria-hidden="true" href="#44-pragmatic-listener--l_1-">#</a></h3>
<p>The <strong>Pragmatic Listener</strong> in RSA, often denoted as $L_1$, performs <strong>Bayesian inference</strong> to reason about what meaning $m$ the speaker likely intended after hearing the utterance $u$.</p>
<p>This listener doesn&rsquo;t just rely on literal meaning; it also considers how likely the speaker would have chosen the utterance $u$ under each possible meaning $m$.</p>
<h4 id="441-equation-for-pragmatic-listener--l_1-">4.4.1 Equation for Pragmatic Listener $ L_1 $<a hidden class="anchor" aria-hidden="true" href="#441-equation-for-pragmatic-listener--l_1-">#</a></h4>
<h4 id="1-simplified-proportional-form"><strong>1. Simplified (Proportional) Form</strong><a hidden class="anchor" aria-hidden="true" href="#1-simplified-proportional-form">#</a></h4>
<p>$$
P_{L_1}(m \mid u) \propto P_{S_1}(u \mid m) \cdot P(m)
$$</p>
<ul>
<li>
<p>This tells us that the posterior belief about meaning $m$ is <strong>proportional</strong> to:</p>
<ul>
<li>The probability that the pragmatic speaker would choose $u$ to express $m$.</li>
<li>The prior probability of meaning $m$ before hearing the utterance.</li>
</ul>
</li>
<li>
<p>This equation computes <strong>unnormalized scores</strong>. To turn them into probabilities, we need to normalize.</p>
</li>
</ul>
<h4 id="2-full-normalized-equation-1"><strong>2. Full Normalized Equation</strong><a hidden class="anchor" aria-hidden="true" href="#2-full-normalized-equation-1">#</a></h4>
<p>$$
P_{L_1}(m \mid u) = \frac{P_{S_1}(u \mid m) \cdot P(m)}{\sum_{m&rsquo;} P_{S_1}(u \mid m&rsquo;) \cdot P(m&rsquo;)}
$$</p>
<ul>
<li>The denominator ensures that the final probabilities <strong>sum to 1</strong>.</li>
<li>$\sum_{m&rsquo;}$ sums over all possible meanings $m&rsquo;$.</li>
</ul>
<h4 id="explanation-of-components"><strong>Explanation of Components</strong><a hidden class="anchor" aria-hidden="true" href="#explanation-of-components">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$P_{L_1}(m \mid u)$</td>
          <td>The <strong>posterior probability</strong> that the intended meaning is $m$ after hearing utterance $u$.</td>
      </tr>
      <tr>
          <td>$P_{S_1}(u \mid m)$</td>
          <td>The <strong>pragmatic speaker’s probability</strong> of choosing utterance $u$ when intending meaning $m$. Computed via softmax over utilities.</td>
      </tr>
      <tr>
          <td>$P(m)$</td>
          <td>The <strong>prior probability</strong> of meaning $m$ before hearing any utterance (reflects world knowledge or expectations).</td>
      </tr>
      <tr>
          <td>Denominator (Normalization)</td>
          <td>Ensures the final probabilities sum to 1 by dividing by the total unnormalized probability mass.</td>
      </tr>
  </tbody>
</table>
<h4 id="how-does-this-work-conceptually"><strong>How Does This Work Conceptually?</strong><a hidden class="anchor" aria-hidden="true" href="#how-does-this-work-conceptually">#</a></h4>
<ol>
<li>
<p>The listener hears $u$ and asks:</p>
<blockquote>
<p><em>“How likely would a rational speaker have said this if they intended each possible meaning?”</em></p></blockquote>
</li>
<li>
<p>The listener also considers how likely each meaning was <strong>before</strong> hearing $u$ (using the prior $P(m)$).</p>
</li>
<li>
<p>Combining these two factors, the listener updates their beliefs and computes the final probabilities over meanings.</p>
</li>
</ol>
<h4 id="why-are-there-two-versions-of-the-equation"><strong>Why Are There Two Versions of the Equation?</strong><a hidden class="anchor" aria-hidden="true" href="#why-are-there-two-versions-of-the-equation">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Version</th>
          <th>Purpose</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Proportional</td>
          <td>Used to calculate relative likelihoods before normalization. Efficient for intermediate steps.</td>
      </tr>
      <tr>
          <td>Normalized</td>
          <td>Produces valid probabilities that sum to 1. Required for final interpretation and reporting results.</td>
      </tr>
  </tbody>
</table>
<h4 id="442-case-study-scalar-implicature-for-some-likelihood-of-m_4-being-true-when-u_textsome-is-heard">4.4.2 Case Study: Scalar Implicature for &ldquo;Some&rdquo; (Likelihood of $m_4$ being true when $u_{\text{some}}$ is heard)<a hidden class="anchor" aria-hidden="true" href="#442-case-study-scalar-implicature-for-some-likelihood-of-m_4-being-true-when-u_textsome-is-heard">#</a></h4>
<ul>
<li>
<p><strong>Utterance space</strong>:<br>
$U = { u_{\text{all}}, u_{\text{some}}, u_{\text{none}} }$</p>
</li>
<li>
<p><strong>Possible meanings</strong>:<br>
$M = { m_0, m_1, m_2, m_3, m_4 }$</p>
<ul>
<li>$m_0$: Alex ate 0 cookies</li>
<li>$m_1$: Alex ate 1 cookie</li>
<li>$m_2$: Alex ate 2 cookies</li>
<li>$m_3$: Alex ate 3 cookies</li>
<li>$m_4$: Alex ate all 4 cookies</li>
</ul>
</li>
<li>
<p>Assume a <strong>uniform prior</strong>:<br>
$P(m_i) = 0.2$ for all $i$.</p>
</li>
<li>
<p>Utterance heard:<br>
$u = u_{\text{some}}$ (&ldquo;Alex ate some of the cookies.&rdquo;)</p>
</li>
</ul>
<h4 id="step-1-compute-the-pragmatic-speakers-probabilities-of-choosing-an-utterance-when-m_4-is-true-p_s_1u-mid-m_4">Step 1: Compute the Pragmatic Speaker’s Probabilities of choosing an utterance when $m_4$ is true [$P_{S_1}(u \mid m_4)$]<a hidden class="anchor" aria-hidden="true" href="#step-1-compute-the-pragmatic-speakers-probabilities-of-choosing-an-utterance-when-m_4-is-true-p_s_1u-mid-m_4">#</a></h4>
<h4 id="literal-listeners-interpretations">Literal Listener’s Interpretations:<a hidden class="anchor" aria-hidden="true" href="#literal-listeners-interpretations">#</a></h4>
<p><strong>For $u_{\text{all}}$</strong>:<br>
$P_{L_0}(m_4 \mid u_{\text{all}}) = \delta_{m_4 \in \llbracket u_{\text{all}} \rrbracket} \cdot P(m_4) = 1 \cdot 0.2 = 0.2 $ (This is Unnormalized probability)</p>
<p>Normalization Step</p>
<p>$$
\sum_{m&rsquo;} \delta_{m&rsquo; \in \llbracket u_{\text{all}} \rrbracket} \cdot P(m&rsquo;) = P(m_4) = 0.2
$$</p>
<p>Final probability:</p>
<p>$$
P_{L_0}(m_4 \mid u_{\text{all}}) = \frac{0.2}{0.2} = 1.0
$$</p>
<blockquote>
<p>Step-by-Step Expansion</p>
<ul>
<li>
<p>Recall that $\llbracket u_{\text{all}} \rrbracket = { m_4 }$, because the utterance &ldquo;all&rdquo; is only literally true if Alex ate all the cookies.</p>
</li>
<li>
<p>So the indicator function $\delta_{m&rsquo; \in \llbracket u_{\text{all}} \rrbracket}$ evaluates as:</p>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th>$m'$</th>
          <th>$\delta_{m&rsquo; \in \llbracket u_{\text{all}} \rrbracket}$</th>
          <th>$P(m&rsquo;)$</th>
          <th>Contribution</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_0$</td>
          <td>0 (ruled out)</td>
          <td>0.2</td>
          <td>$0 \cdot 0.2 = 0$</td>
      </tr>
      <tr>
          <td>$m_1$</td>
          <td>0 (ruled out)</td>
          <td>0.2</td>
          <td>$0 \cdot 0.2 = 0$</td>
      </tr>
      <tr>
          <td>$m_2$</td>
          <td>0 (ruled out)</td>
          <td>0.2</td>
          <td>$0 \cdot 0.2 = 0$</td>
      </tr>
      <tr>
          <td>$m_3$</td>
          <td>0 (ruled out)</td>
          <td>0.2</td>
          <td>$0 \cdot 0.2 = 0$</td>
      </tr>
      <tr>
          <td>$m_4$</td>
          <td>1 (kept)</td>
          <td>0.2</td>
          <td>$1 \cdot 0.2 = 0.2$</td>
      </tr>
  </tbody>
</table>
<ul>
<li>Summing all contributions:</li>
</ul>
<p>$$
\text{Total} = 0 + 0 + 0 + 0 + 0.2 = 0.2
$$</p></blockquote>
<p><strong>For $u_{\text{some}}$</strong></p>
<ul>
<li>$P_{L_0}(m_4 \mid u_{some}) = \delta_{m_4 \in \llbracket u_{some} \rrbracket} \cdot P(m_4) = 1 \cdot 0.2 = 0.2 $ (This is Unnormalized probability)</li>
</ul>
<p>Normalization Step</p>
<p>$$
\sum_{m&rsquo;} \delta_{m&rsquo; \in \llbracket u_{\text{all}} \rrbracket} \cdot P(m&rsquo;) = P(m_1)+P(m_2)+P(m_3) +P(m_4) = 0.2 + 0.2 + 0.2 + 0.2 = 0.8
$$</p>
<p>Final probability:</p>
<p>$$
P_{L_0}(m_4 \mid u_{\text{some}}) = \frac{0.2}{0.8} = 0.25
$$</p>
<h4 id="utility-calculation-1">Utility Calculation:<a hidden class="anchor" aria-hidden="true" href="#utility-calculation-1">#</a></h4>
<p>Using $U(u, m) = \log P_{L_0}(m \mid u) - cost(u)$ and assuming no cost:</p>
<ul>
<li>$U(u_{all}, m_4) = \log P_{L_0}(m_4 \mid u_{all}) - cost(u_{all})= \log(1) - 0 = 0$</li>
<li>$U(u_{some}, m_4) = \log P_{L_0}(m_4 \mid u_{some}) - cost(u_{some})= \log(0.25) - 0 \approx -1.386 - 0 \approx -1.386$</li>
</ul>
<h4 id="compute-pragmatic-speaker-p_s_1u-mid-m_4">Compute Pragmatic Speaker $P_{S_1}(u \mid m_4)$:<a hidden class="anchor" aria-hidden="true" href="#compute-pragmatic-speaker-p_s_1u-mid-m_4">#</a></h4>
<ul>
<li>
<p>Unnormalized:</p>
<ul>
<li>$P_{S_1}(u_{all} \mid m_4) \propto \exp \left( \alpha \cdot U(u_{all}, m_4) \right) \propto \exp \left( 1 \cdot 0 \right) \propto \exp(0) = 1$</li>
<li>$P_{S_1}(u_{\text{some}} \mid m_4) \propto \exp \left( \alpha \cdot U(u_{some}, m_4) \right) \propto \exp \left( 1 \cdot -1.386 \right) \propto \exp(-1.386) \approx 0.25$</li>
</ul>
</li>
<li>
<p>Normalize:</p>
<ul>
<li>Total = $1 + 0.25 = 1.25$</li>
<li>$P_{S_1}(u_{\text{all}} \mid m_4) = \frac{1}{1.25} = 0.8$</li>
<li>$P_{S_1}(u_{\text{some}} \mid m_4) = \frac{0.25}{1.25} = 0.2$</li>
</ul>
</li>
</ul>
<h4 id="step-2-compute-the-pragmatic-listeners-posterior-p_l_1m-mid-u_textsome">Step 2: Compute the Pragmatic Listener’s Posterior $P_{L_1}(m \mid u_{\text{some}})$<a hidden class="anchor" aria-hidden="true" href="#step-2-compute-the-pragmatic-listeners-posterior-p_l_1m-mid-u_textsome">#</a></h4>
<p>Apply Bayes’ Rule:</p>
<p>$$
P_{L_1}(m \mid u_{\text{some}}) \propto P_{S_1}(u_{\text{some}} \mid m) \cdot P(m)
$$</p>
<p>Compute unnormalized scores for all meanings:</p>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>$P_{S_1}(u_{\text{some}} \mid m)$</th>
          <th>$P(m)$</th>
          <th>Product</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_0$</td>
          <td>0 (ruled out by literal meaning)</td>
          <td>0.2</td>
          <td>0.0</td>
      </tr>
      <tr>
          <td>$m_1$</td>
          <td>Assume 0.3</td>
          <td>0.2</td>
          <td>0.06</td>
      </tr>
      <tr>
          <td>$m_2$</td>
          <td>Assume 0.3</td>
          <td>0.2</td>
          <td>0.06</td>
      </tr>
      <tr>
          <td>$m_3$</td>
          <td>Assume 0.3</td>
          <td>0.2</td>
          <td>0.06</td>
      </tr>
      <tr>
          <td>$m_4$</td>
          <td>0.2 (computed above)</td>
          <td>0.2</td>
          <td>0.04</td>
      </tr>
  </tbody>
</table>
<p>Total sum = $0.06 + 0.06 + 0.06 + 0.04 = 0.22$</p>
<h4 id="final-posterior-probabilities">Final Posterior Probabilities:<a hidden class="anchor" aria-hidden="true" href="#final-posterior-probabilities">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>Final $P_{L_1}(m \mid u_{\text{some}})$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_0$</td>
          <td>0.0 (ruled out)</td>
      </tr>
      <tr>
          <td>$m_1$</td>
          <td>$\frac{0.06}{0.22} \approx 0.273$</td>
      </tr>
      <tr>
          <td>$m_2$</td>
          <td>$\frac{0.06}{0.22} \approx 0.273$</td>
      </tr>
      <tr>
          <td>$m_3$</td>
          <td>$\frac{0.06}{0.22} \approx 0.273$</td>
      </tr>
      <tr>
          <td>$m_4$</td>
          <td>$\frac{0.04}{0.22} \approx 0.182$</td>
      </tr>
  </tbody>
</table>
<h4 id="interpretation"><strong>Interpretation</strong><a hidden class="anchor" aria-hidden="true" href="#interpretation">#</a></h4>
<ul>
<li>After hearing &ldquo;some of the cookies were eaten,&rdquo; the listener assigns the highest probability to $m_1$ (Alex ate only 1 cookie).</li>
<li>Although $m_4$ is possible, it’s less likely because if Alex had eaten all the cookies, the speaker would have likely said “all” instead.</li>
<li>This demonstrates how the RSA model formally derives the <strong>scalar implicature</strong> that “some” often implies “not all.”</li>
</ul>
<hr>
<h3 id="v-a-complete-illustration-of-pragmatic-reasoning-in-rsa">V. A complete illustration of pragmatic reasoning in RSA<a hidden class="anchor" aria-hidden="true" href="#v-a-complete-illustration-of-pragmatic-reasoning-in-rsa">#</a></h3>
<p>This section, building on the previous sections, offers a complete demonstration of how listensers interpret scalar utterances, like &ldquo;Alex ate some of the cookies.&rdquo;</p>
<h4 id="common-setup-and-assumptions">Common Setup and Assumptions<a hidden class="anchor" aria-hidden="true" href="#common-setup-and-assumptions">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>Description</th>
          <th>Prior $P(m)$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_0$</td>
          <td>Alex ate 0 cookies</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_1$</td>
          <td>Alex ate 1 cookie</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_2$</td>
          <td>Alex ate 2 cookies</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_3$</td>
          <td>Alex ate 3 cookies</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_4$</td>
          <td>Alex ate 4 cookies</td>
          <td>0.2</td>
      </tr>
  </tbody>
</table>
<p>Utterance space:</p>
<p>$$
U = { u_{\text{none}}, u_{\text{some}}, u_{\text{all}} }
$$</p>
<p>Literal Semantics:</p>
<ul>
<li>$\llbracket u_{\text{none}} \rrbracket = { m_0 }$</li>
<li>$\llbracket u_{\text{some}} \rrbracket = { m_1, m_2, m_3, m_4 }$</li>
<li>$\llbracket u_{\text{all}} \rrbracket = { m_4 }$</li>
</ul>
<p>Assume <strong>$\alpha = 1$</strong> and <strong>$\text{cost}(u) = 0$</strong> for all utterances.</p>
<h4 id="reasoning-level-1-literal-listener-l_">REASONING LEVEL #1 <strong>Literal Listener ($L_0$)</strong><a hidden class="anchor" aria-hidden="true" href="#reasoning-level-1-literal-listener-l_">#</a></h4>
<h4 id="step-1-apply-the-full-equation">Step 1: Apply the Full Equation<a hidden class="anchor" aria-hidden="true" href="#step-1-apply-the-full-equation">#</a></h4>
<p>$$
P_{L_0}(m \mid u_{\text{some}}) = \frac{\delta_{m \in \llbracket u_{\text{some}} \rrbracket} \cdot P(m)}{\displaystyle \sum_{m&rsquo;} \delta_{m&rsquo; \in \llbracket u_{\text{some}} \rrbracket} \cdot P(m&rsquo;)}
$$</p>
<p>Compute Denominator:</p>
<p>$$
\text{Total} = 0.2 + 0.2 + 0.2 + 0.2 = 0.8
$$</p>
<p>Compute Final Probabilities:</p>
<ul>
<li>$P_{L_0}(m_0 \mid u_{\text{some}}) = 0$</li>
<li>$P_{L_0}(m_i \mid u_{\text{some}}) = \frac{0.2}{0.8} = 0.25$ for $i \in {1, 2, 3, 4}$</li>
</ul>
<h4 id="l_0-final-belief-distribution">$L_0$ Final Belief Distribution<a hidden class="anchor" aria-hidden="true" href="#l_0-final-belief-distribution">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>$P_{L_0}(m \mid u_{\text{some}})$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_0$</td>
          <td>0.0 (ruled out)</td>
      </tr>
      <tr>
          <td>$m_1$</td>
          <td>0.25</td>
      </tr>
      <tr>
          <td>$m_2$</td>
          <td>0.25</td>
      </tr>
      <tr>
          <td>$m_3$</td>
          <td>0.25</td>
      </tr>
      <tr>
          <td>$m_4$</td>
          <td>0.25</td>
      </tr>
  </tbody>
</table>
<h4 id="reasoning-level-2-pragmatic-speaker-s_">REASONING LEVEL #2 <strong>Pragmatic Speaker ($S_1$)</strong><a hidden class="anchor" aria-hidden="true" href="#reasoning-level-2-pragmatic-speaker-s_">#</a></h4>
<p>The speaker reasons about how the Literal Listener interprets utterances to decide which utterance to produce.</p>
<h4 id="step-1-compute-utility-for-each-utterance-when-meaning--m_4">Step 1: Compute Utility for Each Utterance When Meaning = $m_4$<a hidden class="anchor" aria-hidden="true" href="#step-1-compute-utility-for-each-utterance-when-meaning--m_4">#</a></h4>
<p>Using:</p>
<p>$$
U(u, m) = \log P_{L_0}(m \mid u) - \text{cost}(u)
$$</p>
<table>
  <thead>
      <tr>
          <th>Utterance</th>
          <th>$P_{L_0}(m_4 \mid u)$</th>
          <th>$U(u, m_4)$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$u_{\text{all}}$</td>
          <td>1.0</td>
          <td>$\log(1) = 0$</td>
      </tr>
      <tr>
          <td>$u_{\text{some}}$</td>
          <td>0.25</td>
          <td>$\log(0.25) = -1.386$</td>
      </tr>
      <tr>
          <td>$u_{\text{none}}$</td>
          <td>0 (ruled out)</td>
          <td>$-\infty$</td>
      </tr>
  </tbody>
</table>
<h4 id="step-2-compute-unnormalized-scores">Step 2: Compute Unnormalized Scores<a hidden class="anchor" aria-hidden="true" href="#step-2-compute-unnormalized-scores">#</a></h4>
<p>$$
P_{S_1}(u \mid m_4) \propto \exp\left( \alpha \cdot U(u, m_4) \right)
$$</p>
<table>
  <thead>
      <tr>
          <th>Utterance</th>
          <th>$\exp(\alpha \cdot U)$</th>
          <th>Unnormalized Score</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$u_{\text{all}}$</td>
          <td>$\exp(0) = 1$</td>
          <td>1.0</td>
      </tr>
      <tr>
          <td>$u_{\text{some}}$</td>
          <td>$\exp(-1.386) \approx 0.25$</td>
          <td>0.25</td>
      </tr>
      <tr>
          <td>$u_{\text{none}}$</td>
          <td>0</td>
          <td>0</td>
      </tr>
  </tbody>
</table>
<p>Total Sum = $1 + 0.25 = 1.25$</p>
<h4 id="step-3-normalize-to-get-final-speaker-probabilities">Step 3: Normalize to Get Final Speaker Probabilities<a hidden class="anchor" aria-hidden="true" href="#step-3-normalize-to-get-final-speaker-probabilities">#</a></h4>
<ul>
<li>$P_{S_1}(u_{\text{all}} \mid m_4) = \frac{1}{1.25} = 0.8$</li>
<li>$P_{S_1}(u_{\text{some}} \mid m_4) = \frac{0.25}{1.25} = 0.2$</li>
</ul>
<h4 id="s_1-final-production-probabilities-given-m_4">$S_1$ Final Production Probabilities (Given $m_4$)<a hidden class="anchor" aria-hidden="true" href="#s_1-final-production-probabilities-given-m_4">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Utterance</th>
          <th>$P_{S_1}(u \mid m_4)$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>&ldquo;All&rdquo;</td>
          <td>0.8</td>
      </tr>
      <tr>
          <td>&ldquo;Some&rdquo;</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>&ldquo;None&rdquo;</td>
          <td>0</td>
      </tr>
  </tbody>
</table>
<h4 id="reasoning-level-3-pragmatic-listener-l_">REASONING LEVEL #3 <strong>Pragmatic Listener ($L_1$)</strong><a hidden class="anchor" aria-hidden="true" href="#reasoning-level-3-pragmatic-listener-l_">#</a></h4>
<p>The listener now reasons about what meaning the speaker most likely intended after hearing the utterance.</p>
<h4 id="step-1-apply-the-full-equation-1">Step 1: Apply the Full Equation<a hidden class="anchor" aria-hidden="true" href="#step-1-apply-the-full-equation-1">#</a></h4>
<p>$$
P_{L_1}(m \mid u_{\text{some}}) = \frac{P_{S_1}(u_{\text{some}} \mid m) \cdot P(m)}{\displaystyle \sum_{m&rsquo;} P_{S_1}(u_{\text{some}} \mid m&rsquo;) \cdot P(m&rsquo;)}
$$</p>
<h4 id="step-2-compute-p_s_1u_textsome-mid-m-for-all-meanings">Step 2: Compute $P_{S_1}(u_{\text{some}} \mid m)$ for All Meanings<a hidden class="anchor" aria-hidden="true" href="#step-2-compute-p_s_1u_textsome-mid-m-for-all-meanings">#</a></h4>
<p>We already computed this for $m_4$:</p>
<ul>
<li>$P_{S_1}(u_{\text{some}} \mid m_4) = 0.2$</li>
</ul>
<p>Assume for this example:</p>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>$P_{S_1}(u_{\text{some}} \mid m)$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_0$</td>
          <td>0 (ruled out by literal semantics)</td>
      </tr>
      <tr>
          <td>$m_1$</td>
          <td>0.4</td>
      </tr>
      <tr>
          <td>$m_2$</td>
          <td>0.3</td>
      </tr>
      <tr>
          <td>$m_3$</td>
          <td>0.2</td>
      </tr>
      <tr>
          <td>$m_4$</td>
          <td>0.2</td>
      </tr>
  </tbody>
</table>
<h4 id="step-3-compute-numerator-and-denominator">Step 3: Compute Numerator and Denominator<a hidden class="anchor" aria-hidden="true" href="#step-3-compute-numerator-and-denominator">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>$P_{S_1}(u_{\text{some}} \mid m)$</th>
          <th>$P(m)$</th>
          <th>Product</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_0$</td>
          <td>0</td>
          <td>0.2</td>
          <td>0</td>
      </tr>
      <tr>
          <td>$m_1$</td>
          <td>0.4</td>
          <td>0.2</td>
          <td>0.08</td>
      </tr>
      <tr>
          <td>$m_2$</td>
          <td>0.3</td>
          <td>0.2</td>
          <td>0.06</td>
      </tr>
      <tr>
          <td>$m_3$</td>
          <td>0.2</td>
          <td>0.2</td>
          <td>0.04</td>
      </tr>
      <tr>
          <td>$m_4$</td>
          <td>0.2</td>
          <td>0.2</td>
          <td>0.04</td>
      </tr>
  </tbody>
</table>
<p>Denominator (Normalization Constant):<br>
$$
\text{Total} = 0.08 + 0.06 + 0.04 + 0.04 = 0.22
$$</p>
<h4 id="step-4-compute-final-l_1-posterior-beliefs">Step 4: Compute Final $L_1$ Posterior Beliefs<a hidden class="anchor" aria-hidden="true" href="#step-4-compute-final-l_1-posterior-beliefs">#</a></h4>
<ul>
<li>$P_{L_1}(m_1 \mid u_{\text{some}}) = \frac{0.08}{0.22} \approx 0.364$</li>
<li>$P_{L_1}(m_2 \mid u_{\text{some}}) = \frac{0.06}{0.22} \approx 0.273$</li>
<li>$P_{L_1}(m_3 \mid u_{\text{some}}) = \frac{0.04}{0.22} \approx 0.182$</li>
<li>$P_{L_1}(m_4 \mid u_{\text{some}}) = \frac{0.04}{0.22} \approx 0.182$</li>
<li>$P_{L_1}(m_0 \mid u_{\text{some}}) = 0$</li>
</ul>
<h4 id="l_1-final-interpretation">$L_1$ Final Interpretation<a hidden class="anchor" aria-hidden="true" href="#l_1-final-interpretation">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Meaning</th>
          <th>$P_{L_1}(m \mid u_{\text{some}})$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$m_0$: 0 cookies</td>
          <td>0.0 (ruled out)</td>
      </tr>
      <tr>
          <td>$m_1$: 1 cookie</td>
          <td>0.364</td>
      </tr>
      <tr>
          <td>$m_2$: 2 cookies</td>
          <td>0.273</td>
      </tr>
      <tr>
          <td>$m_3$: 3 cookies</td>
          <td>0.182</td>
      </tr>
      <tr>
          <td>$m_4$: 4 cookies</td>
          <td>0.182</td>
      </tr>
  </tbody>
</table>
<h4 id="overall-takeaways"><strong>Overall Takeaways</strong><a hidden class="anchor" aria-hidden="true" href="#overall-takeaways">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Agent</th>
          <th>Main Process</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$L_0$</td>
          <td>Filters meanings based on literal truth.</td>
      </tr>
      <tr>
          <td>$S_1$</td>
          <td>Chooses utterances to maximize utility.</td>
      </tr>
      <tr>
          <td>$L_1$</td>
          <td>Infers intended meaning based on the utterance and speaker model.</td>
      </tr>
  </tbody>
</table>
<ul>
<li>Each level of reasoning in the RSA model refines the interpretation process using both <strong>prior knowledge</strong> and <strong>rational inference</strong>.</li>
</ul>
<hr>
<h3 id="vi-reference-games">VI. Reference Games<a hidden class="anchor" aria-hidden="true" href="#vi-reference-games">#</a></h3>
<p>This section presents how RSA explains language production and comprehension in Reference Games based on Franke &amp; Jäger (2016).</p>
<h3 id="61-what-are-reference-games">6.1 What Are Reference Games?<a hidden class="anchor" aria-hidden="true" href="#61-what-are-reference-games">#</a></h3>
<p><strong>Reference Games</strong> are simplified experimental tasks designed to study how speakers choose expressions to refer to objects and how listeners interpret those expressions.</p>
<h4 id="key-features-of-reference-games">Key Features of Reference Games<a hidden class="anchor" aria-hidden="true" href="#key-features-of-reference-games">#</a></h4>
<ul>
<li><strong>Interactive Setting</strong>: Involves a speaker and a listener.</li>
<li><strong>Goal</strong>:
<ul>
<li>The speaker must communicate a specific referent (target object) to the listener.</li>
<li>The listener must infer which object the speaker is referring to based on the utterance.</li>
</ul>
</li>
</ul>
<h4 id="typical-experimental-setup">Typical Experimental Setup<a hidden class="anchor" aria-hidden="true" href="#typical-experimental-setup">#</a></h4>
<p>Here is an example:</p>
<p align="center">
  <img src="/images/referencegameexample.png" alt="Reference Game Example" width="600px">
</p>
<ul>
<li>
<p>Objects in Context:<br>
Example:</p>
<ul>
<li>Green Square</li>
<li>Green Circle</li>
<li>Blue Circle</li>
</ul>
</li>
<li>
<p>Possible Utterances:</p>
<ul>
<li>&ldquo;green&rdquo;</li>
<li>&ldquo;square&rdquo;</li>
<li>&ldquo;circle&rdquo;</li>
<li>&ldquo;blue&rdquo;</li>
</ul>
</li>
</ul>
<p>Task:<br>
<strong>If the target is the <em>Green Square</em>, should the speaker say &ldquo;green&rdquo; or &ldquo;square&rdquo;?</strong><br>
- &ldquo;Square&rdquo; is more <strong>informative</strong> because it uniquely identifies the referent.</p>
<h4 id="connection-to-gricean-maxims">Connection to Gricean Maxims<a hidden class="anchor" aria-hidden="true" href="#connection-to-gricean-maxims">#</a></h4>
<ul>
<li>
<p><strong>Maxim of Quantity</strong>: Be as informative as required.<br>
→ Speakers prefer utterances that best help the listener identify the referent.</p>
</li>
<li>
<p><strong>Maxim of Manner</strong>: Avoid unnecessary effort.<br>
→ Speakers also tend to avoid overly complex or costly expressions.</p>
</li>
</ul>
<h4 id="what-makes-reference-games-powerful">What Makes Reference Games Powerful?<a hidden class="anchor" aria-hidden="true" href="#what-makes-reference-games-powerful">#</a></h4>
<ul>
<li>They provide a simple, controlled environment for testing:
<ul>
<li>How speakers balance <strong>informativeness</strong> and <strong>production cost</strong>.</li>
<li>How listeners reason about speaker choices (<strong>pragmatic inference</strong>).</li>
</ul>
</li>
<li>Reference games form the foundation for formal models of language use like the <strong>RSA model</strong>.</li>
</ul>
<h3 id="62-reference-games--formal-modeling">6.2 Reference Games — Formal Modeling<a hidden class="anchor" aria-hidden="true" href="#62-reference-games--formal-modeling">#</a></h3>
<p>In this section, we explore how the RSA model accounts for reasoning in reference games, following the mathematical formalizations used in <strong>Franke &amp; Jäger (2016)</strong>.</p>
<h4 id="1-literal-listener-l_0">1. Literal Listener ($L_0$)<a hidden class="anchor" aria-hidden="true" href="#1-literal-listener-l_0">#</a></h4>
<p>$$
P_{\text{literal}}(r \mid p) =
\begin{cases}
\frac{1}{|{r&rsquo; : p \text{ is true of } r&rsquo;}|} &amp; \text{if } p \text{ is true of } r \\
0 &amp; \text{otherwise}
\end{cases}
$$</p>
<ul>
<li>This assigns <strong>uniform probability</strong> over all referents that literally satisfy the utterance.</li>
<li>The Literal Listener does <strong>not</strong> consider why the speaker chose this utterance—only whether it’s true of a referent.</li>
</ul>
<h4 id="explanation-of-formula-components">Explanation of Formula Components<a hidden class="anchor" aria-hidden="true" href="#explanation-of-formula-components">#</a></h4>
<ul>
<li>
<p>$r$: Referent (target object)</p>
</li>
<li>
<p>$p$: Property or utterance</p>
</li>
<li>
<p>${ r&rsquo; : p \text{ is true of } r&rsquo; }$<br>
→ This is the <strong>set of all referents</strong> for which the property $p$ holds true.<br>
<em>Example</em>: If $p$ is &ldquo;green&rdquo;, this set contains all green objects.</p>
</li>
<li>
<p>$|{ r&rsquo; : p \text{ is true of } r&rsquo; }|$<br>
→ This is the <strong>size of that set</strong>, i.e., how many referents have the property $p$.</p>
</li>
<li>
<p>$\frac{1}{|{ \dots }|}$<br>
→ This means that <strong>each compatible referent is assigned equal probability</strong>, based on the total number of compatible referents.</p>
</li>
<li>
<p>$\text{if } p \text{ is true of } r$<br>
→ This part of the case condition says that the formula only applies if $r$ is actually compatible with the utterance $p$.</p>
</li>
</ul>
<h4 id="what-does-r-represent">What Does $r&rsquo;$ Represent?<a hidden class="anchor" aria-hidden="true" href="#what-does-r-represent">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Symbol</th>
          <th>Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$r$</td>
          <td>The specific referent the listener is considering.</td>
      </tr>
      <tr>
          <td>$r'$</td>
          <td>A variable representing <strong>all possible referents</strong> in the context. Used for counting how many referents are compatible with the utterance.</td>
      </tr>
  </tbody>
</table>
<ul>
<li>The notation ${ r&rsquo; : p \text{ is true of } r&rsquo; }$ defines the set of <strong>all referents where the utterance $p$ is literally true</strong>.</li>
<li>The term $| \dots |$ counts how many referents are in that set.</li>
</ul>
<blockquote>
<h4 id="example-listener-hears-green"><strong>Example: Listener Hears “green”</strong><a hidden class="anchor" aria-hidden="true" href="#example-listener-hears-green">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Referent</th>
          <th>Properties</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$r_1$</td>
          <td>Green, Square</td>
      </tr>
      <tr>
          <td>$r_2$</td>
          <td>Green, Circle</td>
      </tr>
      <tr>
          <td>$r_3$</td>
          <td>Blue, Circle</td>
      </tr>
  </tbody>
</table>
<ul>
<li>${ r&rsquo; : p = \text{&ldquo;green&rdquo;} \text{ is true of } r&rsquo; } = { r_1, r_2 }$</li>
<li>$|{ \dots }| = 2$</li>
</ul>
<h4 id="compute-probabilities">Compute Probabilities:<a hidden class="anchor" aria-hidden="true" href="#compute-probabilities">#</a></h4>
<ul>
<li>$P_{\text{literal}}(r_1 \mid \text{&ldquo;green&rdquo;}) = \frac{1}{2} = 0.5$</li>
<li>$P_{\text{literal}}(r_2 \mid \text{&ldquo;green&rdquo;}) = 0.5$</li>
<li>$P_{\text{literal}}(r_3 \mid \text{&ldquo;green&rdquo;}) = 0$</li>
</ul></blockquote>
<h4 id="2-pragmatic-speaker-s_1">2. Pragmatic Speaker ($S_1$)<a hidden class="anchor" aria-hidden="true" href="#2-pragmatic-speaker-s_1">#</a></h4>
<p>$$
P_{prod}(p \mid r; \lambda, f) =
\frac{\exp\left(\lambda \cdot EU_{speaker}(r, p; f)\right)}{\sum_{p&rsquo;} \exp\left(\lambda \cdot EU_{speaker}(r, p&rsquo;; f)\right)}
$$</p>
<ul>
<li>$ EU_{speaker}(r, p; f) = P_{literal}(r \mid p) + f(p) $</li>
<li>$f(p)$: Utterance bias or cost</li>
<li>$\lambda$: Rationality parameter (inverse temperature)</li>
</ul>
<h4 id="3-pragmatic-listener-l_1">3. Pragmatic Listener ($L_1$)<a hidden class="anchor" aria-hidden="true" href="#3-pragmatic-listener-l_1">#</a></h4>
<p>$$
P_{\text{comp}}(r \mid p) = \frac{P(r) \cdot P_{\text{prod}}(p \mid r)}{\sum_{r&rsquo;} P(r&rsquo;) \cdot P_{\text{prod}}(p \mid r&rsquo;)}
$$</p>
<h4 id="63-further-break-downs-of-the-mathematical-notations-in-franke--jager-2016-and-degen-2023">6.3 Further break-downs of the mathematical notations in Franke &amp; Jager (2016) and Degen (2023)<a hidden class="anchor" aria-hidden="true" href="#63-further-break-downs-of-the-mathematical-notations-in-franke--jager-2016-and-degen-2023">#</a></h4>
<h4 id="literal-listener-notation-comparison">Literal Listener: Notation Comparison<a hidden class="anchor" aria-hidden="true" href="#literal-listener-notation-comparison">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Concept</th>
          <th>Franke &amp; Jäger (2016)</th>
          <th>Degen (2023)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Agent</td>
          <td>$P_{\text{literal}}(r \mid p)$</td>
          <td>$P_{L_0}(m \mid u)$</td>
      </tr>
      <tr>
          <td>Meaning / Referent</td>
          <td>$r$ = referent</td>
          <td>$m$ = meaning</td>
      </tr>
      <tr>
          <td>Utterance</td>
          <td>$p$ = property</td>
          <td>$u$ = utterance</td>
      </tr>
      <tr>
          <td>Literal Semantics</td>
          <td>$p$ is true of $r$</td>
          <td>$m \in \llbracket u \rrbracket$</td>
      </tr>
      <tr>
          <td>Truth Filter</td>
          <td>Verbal condition</td>
          <td>Indicator: $\delta_{m \in \llbracket u \rrbracket}$</td>
      </tr>
      <tr>
          <td>Uniformity</td>
          <td>Uniform over compatible referents</td>
          <td>Prior $P(m)$ allows non-uniformity</td>
      </tr>
      <tr>
          <td>Normalization</td>
          <td>Divide by number of compatible referents</td>
          <td>Sum over priors of compatible meanings</td>
      </tr>
  </tbody>
</table>
<h4 id="key-takeaways-1">Key Takeaways<a hidden class="anchor" aria-hidden="true" href="#key-takeaways-1">#</a></h4>
<ul>
<li>Both formulations implement literal semantics: they restrict attention to meanings that are literally compatible with the utterance.</li>
<li>Degen’s version allows more <strong>Bayesian flexibility</strong> and incorporates <strong>world knowledge via priors</strong>.</li>
<li>Franke &amp; Jäger’s approach is useful for <strong>simple reference tasks</strong> with uniform assumptions.</li>
</ul>
<h3 id="64-literal-listener">6.4 Literal Listener<a hidden class="anchor" aria-hidden="true" href="#64-literal-listener">#</a></h3>
<p><em>(Based on Franke &amp; Jäger, 2016)</em></p>
<h4 id="step-1-understand-the-task-setup">Step 1: Understand the Task Setup<a hidden class="anchor" aria-hidden="true" href="#step-1-understand-the-task-setup">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Object</th>
          <th>Color</th>
          <th>Shape</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$r_1$</td>
          <td>Green</td>
          <td>Square</td>
      </tr>
      <tr>
          <td>$r_2$</td>
          <td>Green</td>
          <td>Cirle</td>
      </tr>
      <tr>
          <td>$r_3$</td>
          <td>Blue</td>
          <td>Circle</td>
      </tr>
  </tbody>
</table>
<p><em>Possible Utterances (Properties)</em>:</p>
<p>$p \in \{ &ldquo;green&rdquo;, &ldquo;circle&rdquo;, &ldquo;square&rdquo;, &ldquo;blue&rdquo; \}$</p>
<h4 id="how-does-the-literal-listener-interpret-an-utterance">How Does the Literal Listener Interpret an Utterance?<a hidden class="anchor" aria-hidden="true" href="#how-does-the-literal-listener-interpret-an-utterance">#</a></h4>
<p>The Literal Listener reasons <strong>purely based on literal semantics</strong>, without considering why the speaker chose a particular utterance.</p>
<h4 id="formula">Formula:<a hidden class="anchor" aria-hidden="true" href="#formula">#</a></h4>
<p>$$
P_{\text{literal}}(r \mid p) =
\begin{cases}
\frac{1}{|{ r&rsquo; : p \text{ is true of } r&rsquo; }|} &amp; \text{if } p \text{ is true of } r \\
0 &amp; \text{otherwise}
\end{cases}
$$</p>
<ul>
<li>If the utterance $p$ is true of object $r$, assign <strong>equal probability</strong> among all such objects.</li>
<li>Otherwise, assign probability $0$.</li>
</ul>
<h4 id="example-1-listener-hears-green">Example 1: Listener Hears “green”<a hidden class="anchor" aria-hidden="true" href="#example-1-listener-hears-green">#</a></h4>
<ol>
<li>
<p>Determine which objects that are compatible with the property &ldquo;green&rdquo;:<br>
→ $r_1$ (Green Square), $r_2$ (Green Circle)</p>
</li>
<li>
<p>Assign Probabilities:</p>
<table>
  <thead>
      <tr>
          <th>Referent</th>
          <th>Compatible with &ldquo;green&rdquo;?</th>
          <th>$P_{\text{literal}}(r \mid p = \text{&ldquo;green&rdquo;})$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$r_1$</td>
          <td>Yes</td>
          <td>0.5</td>
      </tr>
      <tr>
          <td>$r_2$</td>
          <td>Yes</td>
          <td>0.5</td>
      </tr>
      <tr>
          <td>$r_3$</td>
          <td>No</td>
          <td>0</td>
      </tr>
  </tbody>
</table>
</li>
</ol>
<blockquote>
<ul>
<li>${ r&rsquo; : p = \text{&ldquo;green&rdquo;} \text{ is true of } r&rsquo; } = { r_1, r_2 }$</li>
<li>$|{ \dots }| = 2$</li>
</ul>
<h4 id="compute-probabilities-1">Compute Probabilities:<a hidden class="anchor" aria-hidden="true" href="#compute-probabilities-1">#</a></h4>
<ul>
<li>$P_{\text{literal}}(r_1 \mid \text{&ldquo;green&rdquo;}) = \frac{1}{2} = 0.5$</li>
<li>$P_{\text{literal}}(r_2 \mid \text{&ldquo;green&rdquo;}) = \frac{1}{2} = 0.5$</li>
<li>$P_{\text{literal}}(r_3 \mid \text{&ldquo;green&rdquo;}) = 0$</li>
</ul></blockquote>
<h4 id="example-2-listener-hears-square">Example 2: Listener Hears “square”<a hidden class="anchor" aria-hidden="true" href="#example-2-listener-hears-square">#</a></h4>
<ol>
<li>
<p>Determine which objects are compatible with the property &ldquo;square&rdquo;:<br>
→ Only $r_1$.</p>
</li>
<li>
<p>Assign Probabilities:</p>
<table>
  <thead>
      <tr>
          <th>Referent</th>
          <th>Compatible with &ldquo;square&rdquo;?</th>
          <th>$P_{\text{literal}}(r \mid p = \text{&ldquo;square&rdquo;})$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$r_1$</td>
          <td>Yes</td>
          <td>$1.0$</td>
      </tr>
      <tr>
          <td>$r_2$</td>
          <td>No</td>
          <td>$0$</td>
      </tr>
      <tr>
          <td>$r_3$</td>
          <td>No</td>
          <td>$0$</td>
      </tr>
  </tbody>
</table>
</li>
</ol>
<blockquote>
<ul>
<li>${ r&rsquo; : p = \text{&ldquo;square&rdquo;} \text{ is true of } r&rsquo; } = { r_1 }$</li>
<li>$ |{ \dots }| = 1 $</li>
</ul>
<h4 id="compute-probabilities-2">Compute Probabilities:<a hidden class="anchor" aria-hidden="true" href="#compute-probabilities-2">#</a></h4>
<ul>
<li>$P_{\text{literal}}(r_1 \mid \text{&ldquo;square&rdquo;}) = \frac{1}{1} = 1.0$</li>
<li>$P_{\text{literal}}(r_2 \mid \text{&ldquo;square&rdquo;}) = 0$</li>
<li>$P_{\text{literal}}(r_3 \mid \text{&ldquo;square&rdquo;}) = 0$</li>
</ul></blockquote>
<h4 id="example-3-listener-hears-blue">Example 3: Listener Hears “blue”<a hidden class="anchor" aria-hidden="true" href="#example-3-listener-hears-blue">#</a></h4>
<ol>
<li>
<p>Determine which objects are compatible with the property &ldquo;square&rdquo;:<br>
→ Only $r_1$.</p>
</li>
<li>
<p>Assign Probabilities:</p>
<table>
  <thead>
      <tr>
          <th>Referent</th>
          <th>Compatible with &ldquo;square&rdquo;?</th>
          <th>$P_{\text{literal}}(r \mid p = \text{&ldquo;square&rdquo;})$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$r_1$</td>
          <td>No</td>
          <td>$0.0$</td>
      </tr>
      <tr>
          <td>$r_2$</td>
          <td>No</td>
          <td>$0$</td>
      </tr>
      <tr>
          <td>$r_3$</td>
          <td>Yes</td>
          <td>$1.0$</td>
      </tr>
  </tbody>
</table>
</li>
</ol>
<blockquote>
<ul>
<li>${ r&rsquo; : p = \text{&ldquo;blue&rdquo;} \text{ is true of } r&rsquo; } = { r_3 }$</li>
<li>$ |{ \dots }| = 1 $</li>
</ul>
<h4 id="compute-probabilities-3">Compute Probabilities:<a hidden class="anchor" aria-hidden="true" href="#compute-probabilities-3">#</a></h4>
<ul>
<li>$P_{\text{literal}}(r_1 \mid \text{&ldquo;square&rdquo;}) = 0$</li>
<li>$P_{\text{literal}}(r_2 \mid \text{&ldquo;square&rdquo;}) = 0$</li>
<li>$P_{\text{literal}}(r_3 \mid \text{&ldquo;square&rdquo;}) = \frac{1}{1} = 1.0$</li>
</ul></blockquote>
<h3 id="65-pragmatic-speaker">6.5 Pragmatic Speaker<a hidden class="anchor" aria-hidden="true" href="#65-pragmatic-speaker">#</a></h3>
<p>$$
P_{prod}(p \mid r; \lambda, f) =
\frac{\exp\left( \lambda \cdot EU_{speaker}(r, p; f) \right)}{\displaystyle \sum_{p&rsquo;} \exp\left( \lambda \cdot EU_{speaker}(r, p&rsquo;; f) \right)}
$$</p>
<p>where $EU_{speaker}(r, p; f)$ is <strong>Utility Function</strong>:</p>
<p>$$
EU_{speaker}(r, p; f) = P_{literal}(r \mid p) + f(p)
$$</p>
<ul>
<li>$P_{literal}(r \mid p)$: How likely is it that a literal listener will pick referent $r$ after hearing $p$?</li>
<li>$f(p)$: Cost or bias term, penalizing utterances that are long, complex, or dispreferred.</li>
</ul>
<h4 id="explanation-of-components-1"><strong>Explanation of Components</strong><a hidden class="anchor" aria-hidden="true" href="#explanation-of-components-1">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Symbol</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$p$</td>
          <td>Property (utterance).</td>
      </tr>
      <tr>
          <td>$r$</td>
          <td>Referent (target object).</td>
      </tr>
      <tr>
          <td>$P_{prod} (p \mid r; \lambda, f)$</td>
          <td>Probability of Pragmatic Speaker choosing a property/utterance</td>
      </tr>
      <tr>
          <td>$\lambda$</td>
          <td>Rationality parameter (speaker’s sensitivity to utility differences).</td>
      </tr>
      <tr>
          <td>$f(p)$</td>
          <td>Cost or bias associated with utterance $p$.</td>
      </tr>
      <tr>
          <td>$ EU_{speaker}$</td>
          <td>Expected utility of utterance $p$ for referent $r$.</td>
      </tr>
  </tbody>
</table>
<p><strong><em>Combine the two</em></strong></p>
<p>$$ P_{prod}(p \mid r; \lambda, f) =
\frac{\exp\left( \lambda \cdot EU_{speaker}(r, p; f) \right)}{\displaystyle \sum_{p&rsquo;} \exp\left( \lambda \cdot EU_{speaker}(r, p&rsquo;; f) \right)} = \frac{\exp\left( \lambda \cdot (P_{literal}(r \mid p) + f(p)) \right)}{\displaystyle \sum_{p&rsquo;} \exp\left( \lambda \cdot (P_{literal}(r \mid p&rsquo;) + f(p&rsquo;)) \right)}
$$</p>
<h4 id="worked-example"><strong>Worked Example</strong><a hidden class="anchor" aria-hidden="true" href="#worked-example">#</a></h4>
<p>Scenario:</p>
<table>
  <thead>
      <tr>
          <th>Object</th>
          <th>Color</th>
          <th>Shape</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$r_1$</td>
          <td>Green</td>
          <td>Square</td>
      </tr>
      <tr>
          <td>$r_2$</td>
          <td>Green</td>
          <td>Circle</td>
      </tr>
      <tr>
          <td>$r_3$</td>
          <td>Blue</td>
          <td>Circle</td>
      </tr>
  </tbody>
</table>
<ul>
<li>Target referent $r = r_1$ (Green Square).</li>
<li>Utterance/Property space: &ldquo;green&rdquo;, &ldquo;square&rdquo;. (<strong>Note that we limit the utterance choices to be of only &ldquo;green&rdquo; and &ldquo;square&rdquo;</strong> which are literally true of &ldquo;Green Square&rdquo;)</li>
</ul>
<blockquote>
<p>Note that the set of utterances to choose from matters in final Speaker&rsquo;s probability in choosing which utterance to use</p>
<ul>
<li>$u_1 \in \{ &ldquo;green&rdquo;, &ldquo;square&rdquo; \}$</li>
<li>$u_2 \in \{ \text{&ldquo;green&rdquo;}, \text{&ldquo;square&rdquo;}, \text{&ldquo;circle&rdquo;}, \text{&ldquo;blue&rdquo;} \}$</li>
</ul>
<h4 id="why-the-two-u-sets">why the two $u$ sets?<a hidden class="anchor" aria-hidden="true" href="#why-the-two-u-sets">#</a></h4>
<p>They lead to different Speaker&rsquo;s probabilities in choosing utterances to refer to a particular referent.
<strong>in the $u_2$ option, those semantically incompatible/literally false utterances/properties were also assigned probabilities</strong>.</p>
<h4 id="should-false-utterances-be-included">Should False Utterances Be Included?<a hidden class="anchor" aria-hidden="true" href="#should-false-utterances-be-included">#</a></h4>
<h4 id="option-1-full-utterance-set-even-non-true-ones">Option 1: Full Utterance Set (Even Non-True Ones)<a hidden class="anchor" aria-hidden="true" href="#option-1-full-utterance-set-even-non-true-ones">#</a></h4>
<p>Assumption:<br>
Speakers can, and sometimes do, use suboptimal or false utterances.</p>
<h4 id="model-characteristics">Model Characteristics:<a hidden class="anchor" aria-hidden="true" href="#model-characteristics">#</a></h4>
<ul>
<li>All utterances in the property space are <strong>included</strong>, regardless of literal truth.</li>
<li>Even non-true properties like <code>&quot;blue&quot;</code> (for a green object) receive <strong>non-zero probability</strong>, though down-weighted.</li>
<li>Matches the <strong>default RSA formulation</strong> as used in:
<ul>
<li><em>Franke &amp; Jäger (2016)</em></li>
<li><em>Goodman &amp; Frank (2016)</em></li>
</ul>
</li>
</ul>
<h4 id="when-to-use">When to Use:<a hidden class="anchor" aria-hidden="true" href="#when-to-use">#</a></h4>
<ul>
<li>You want to model <strong>noisy, probabilistic human behavior</strong>.</li>
<li>You&rsquo;re studying <strong>psycholinguistic behavior</strong>, overinformativeness, or slips.</li>
<li>You&rsquo;re interested in <strong>graded inference</strong> and <strong>soft competition</strong>.</li>
</ul>
<hr>
<h4 id="option-2-restricted-utterance-set-only-true-properties">Option 2: Restricted Utterance Set (Only True Properties)<a hidden class="anchor" aria-hidden="true" href="#option-2-restricted-utterance-set-only-true-properties">#</a></h4>
<h4 id="assumption">Assumption:<a hidden class="anchor" aria-hidden="true" href="#assumption">#</a></h4>
<p>Speakers are always semantically competent and never use false utterances.</p>
<h4 id="model-characteristics-1">Model Characteristics:<a hidden class="anchor" aria-hidden="true" href="#model-characteristics-1">#</a></h4>
<ul>
<li>The utterance set is <strong>filtered</strong> to include only properties that are literally true of the referent.</li>
<li>Softmax normalization is applied <strong>only over true utterances</strong>.</li>
<li>Produces <strong>sharper speaker preferences</strong> and downstream listener inference.</li>
</ul>
<h4 id="when-to-use-1">When to Use:<a hidden class="anchor" aria-hidden="true" href="#when-to-use-1">#</a></h4>
<ul>
<li>You&rsquo;re modeling <strong>idealized agents</strong> in formal semantics or decision theory.</li>
<li>Your research assumes <strong>strict informativeness</strong>.</li>
<li>You&rsquo;re simulating <strong>maximally rational communication</strong>.</li>
</ul></blockquote>
<p><strong><em>Speaker production probabilities calculation</em></strong><br>
<em>Step 1: Assume:</em></p>
<ul>
<li>$\lambda = 1$ (moderate rationality).</li>
<li>$f(p) = 0$ (no production cost).</li>
</ul>
<p><em>Step 2: Compute $P_{literal}(r \mid p)$</em></p>
<ul>
<li>
<p>Literal Listener for $&ldquo;green&rdquo;$:</p>
<ul>
<li>Compatible referents: $r_1$, $r_2$.</li>
<li>$P_{\text{literal}}(r_1 \mid \text{&ldquo;green&rdquo;}) = \frac{1}{2} = 0.5$</li>
</ul>
</li>
<li>
<p>Literal Listener for $&ldquo;square&rdquo;$:</p>
<ul>
<li>Only $r_1$ is a square.</li>
<li>$P_{literal}(r_1 \mid {&ldquo;square&rdquo;}) = 1.0$</li>
</ul>
</li>
</ul>
<p><em>Step 3: Compute Utility for Each Utterance</em></p>
<p>$EU_{speaker}(r_1, &ldquo;green&rdquo;; f) = P_{literal}(r_1 \mid &ldquo;green&rdquo;) + f(&ldquo;green&rdquo;) = 0.5 + 0 = 0.5$<br>
$EU_{speaker}(r_1, &ldquo;square&rdquo;; f) = P_{literal}(r_1 \mid &ldquo;square&rdquo;) + f(&ldquo;square&rdquo;) = 1.0 + 0 = 1.0$</p>
<p><em>Step 4: Compute Unnormalized Scores</em> Using $\exp\left(\lambda \cdot EU_{speaker}(r, p; f)\right) $:</p>
<ul>
<li>$ \exp\left(\lambda \cdot  EU_{speaker}(r_1, &ldquo;green&rdquo;; f) \right) = \exp(1 \cdot 0.5) = \exp(0.5) \approx 1.649$</li>
<li>$\exp\left(\lambda \cdot EU_{speaker}(r_1, &ldquo;square&rdquo;; f) \right) = \exp(1 \cdot 1.0) = \exp(1) \approx 2.718$</li>
</ul>
<blockquote>
<p><strong>What Does $\exp(0.5)$ Mean?</strong></p>
<ul>
<li>$\exp(x)$ is the <strong>exponential function</strong>, defined as:</li>
</ul>
<p>$ \exp(x) = e^x $</p>
<ul>
<li>Where <strong>$e \approx 2.71828$</strong> is $Euler’s$ number, a fundamental constant in mathematics.</li>
</ul>
<p><strong>Why Is This Important in RSA?</strong></p>
<ul>
<li>The exponential function <strong>transforms utility values into positive scores</strong>.</li>
<li>It ensures that higher-utility utterances lead to higher scores in the <strong>softmax calculation</strong> for final probabilities.</li>
<li>Without exponentiation, negative utilities could produce invalid (negative) probabilities.</li>
</ul>
<h4 id="quick-reference-table"><strong>Quick Reference Table</strong><a hidden class="anchor" aria-hidden="true" href="#quick-reference-table">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Utility $EU(p, r)$</th>
          <th>$\exp(EU)$</th>
          <th>Interpretation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>0</td>
          <td>1.0</td>
          <td>Baseline utility.</td>
      </tr>
      <tr>
          <td>0.5</td>
          <td>1.649</td>
          <td>Moderately high utility.</td>
      </tr>
      <tr>
          <td>1.0</td>
          <td>2.718</td>
          <td>High utility.</td>
      </tr>
      <tr>
          <td>-1.386</td>
          <td>0.25</td>
          <td>Low utility (as in $\log(0.25)$).</td>
      </tr>
  </tbody>
</table></blockquote>
<p><em>Step 5: Normalize to Get Final Probabilities</em></p>
<p>$ Total = \exp\left( \lambda \cdot EU_{speaker}(r_1, green&rsquo;; f) \right) + \exp\left( \lambda \cdot EU_{speaker}(r_1, square&rsquo;; f) \right) =1.649 + 2.718 = 4.367 $</p>
<ul>
<li>$P_{prod}({&ldquo;green&rdquo;} \mid r_1) = \frac{\exp\left( \lambda \cdot EU_{speaker}(r_1, &ldquo;green&rdquo;; f) \right)}{\sum_{p&rsquo;} \exp\left( \lambda \cdot EU_{speaker}(r, p&rsquo;; f) \right)} =
\frac{1.649}{4.367} \approx 0.378$</li>
<li>$P_{\text{prod}}(\text{&ldquo;square&rdquo;} \mid r_1) = \frac{\exp\left( \lambda \cdot EU_{speaker}(r_1, &ldquo;square&rdquo;; f) \right)}{\sum_{p&rsquo;} \exp\left( \lambda \cdot EU_{speaker}(r, p&rsquo;; f) \right)} = \frac{2.718}{4.367} \approx 0.622$</li>
</ul>
<h4 id="final-interpretation-4">Final Interpretation<a hidden class="anchor" aria-hidden="true" href="#final-interpretation-4">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Utterance</th>
          <th>$P_{\text{prod}}(p \mid r_1)$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>&ldquo;green&rdquo;</td>
          <td>0.378 (38%)</td>
      </tr>
      <tr>
          <td>&ldquo;square&rdquo;</td>
          <td>0.622 (62%)</td>
      </tr>
  </tbody>
</table>
<ul>
<li>These are the Pragmatic Speaker&rsquo;s probabilities in choosing &ldquo;green&rdquo; or &ldquo;square&rdquo; to refer to $r_1$  (Green Square)</li>
<li>The speaker prefers &ldquo;square&rdquo; but still sometimes chooses &ldquo;green&rdquo;.</li>
<li>This reflects probabilistic, graded behavior rather than deterministic selection.</li>
</ul>
<h4 id="key-insights"><strong>Key Insights</strong><a hidden class="anchor" aria-hidden="true" href="#key-insights">#</a></h4>
<ul>
<li>
<p><strong>Higher Utility → Higher Probability</strong><br>
But not necessarily 100%, depending on $\lambda$.</p>
</li>
<li>
<p><strong>Effect of $\lambda$</strong>:</p>
<ul>
<li>If $\lambda = 0$, the speaker would choose &ldquo;green&rdquo; and &ldquo;square&rdquo; randomly.</li>
<li>If $\lambda \to \infty$, the speaker would always choose &ldquo;square&rdquo;.</li>
</ul>
</li>
<li>
<p><strong>Effect of Cost $f(p)$</strong>:</p>
<ul>
<li>If &ldquo;square&rdquo; had a higher cost, the speaker might prefer &ldquo;green&rdquo; despite its lower informativeness.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="66-pragmatic-listener">6.6 Pragmatic Listener<a hidden class="anchor" aria-hidden="true" href="#66-pragmatic-listener">#</a></h3>
<p>In the RSA model, the <strong>pragmatic listener</strong> ($L_1$) updates beliefs about the speaker’s intended referent based on the utterance they receive. This is done using <strong>Bayes&rsquo; Rule</strong> and the listener’s model of the speaker.</p>
<p>$$
P_{comp}(\text{choose } r \mid \text{receive } p;\ \lambda, f) = \frac{P(r) \cdot P_{prod}(p \mid r;\ \lambda, f)}{\displaystyle \sum_{r&rsquo;} P(r&rsquo;) \cdot P_{prod}(p \mid r&rsquo;;\ \lambda, f)}
$$</p>
<h4 id="explanation-of-components-2">Explanation of Components<a hidden class="anchor" aria-hidden="true" href="#explanation-of-components-2">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Symbol</th>
          <th>Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$r$</td>
          <td>A possible referent (object)</td>
      </tr>
      <tr>
          <td>$p$</td>
          <td>The utterance (property) received from the speaker</td>
      </tr>
      <tr>
          <td>$P(r)$</td>
          <td>Prior probability of referent $r$</td>
      </tr>
      <tr>
          <td>$P_{prod}(p \mid r;\ \lambda, f)$</td>
          <td>Probability that a speaker with parameters $(\lambda, f)$ would produce $p$ when referring to $r$</td>
      </tr>
      <tr>
          <td>$\lambda$</td>
          <td>Rationality parameter (how optimally the speaker chooses)</td>
      </tr>
      <tr>
          <td>$f(p)$</td>
          <td>Cost or bias associated with producing utterance $p$</td>
      </tr>
      <tr>
          <td>$r'$</td>
          <td>Variable over all referents used to normalize the distribution</td>
      </tr>
  </tbody>
</table>
<h4 id="intuition">Intuition<a hidden class="anchor" aria-hidden="true" href="#intuition">#</a></h4>
<blockquote>
<p>The listener <strong>reasons backward</strong>:<br>
“If the speaker chose to say $p$, which referent $r$ would make that utterance most likely—assuming they were optimizing utility using $\lambda$ and $f$?”</p></blockquote>
<p>This is a form of <strong>Bayesian social reasoning</strong>:</p>
<ul>
<li>The listener assumes the speaker is rational (to some degree),</li>
<li>and uses this model to infer likely referents.</li>
</ul>
<h4 id="example">Example<a hidden class="anchor" aria-hidden="true" href="#example">#</a></h4>
<p><em><strong>Referents</strong></em></p>
<table>
  <thead>
      <tr>
          <th>Code</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$r_1$</td>
          <td>Green Square</td>
      </tr>
      <tr>
          <td>$r_2$</td>
          <td>Green Circle</td>
      </tr>
      <tr>
          <td>$r_3$</td>
          <td>Blue Circle</td>
      </tr>
  </tbody>
</table>
<p>Possible utterances (utterance space): &ldquo;green&rdquo;, &ldquo;square&rdquo;, &ldquo;circle&rdquo;, &ldquo;blue&rdquo;</p>
<p><em>Step 1: Assumed Prior</em></p>
<p>Uniform prior:  $ P(r_1) = P(r_2) = P(r_3) = \frac{1}{3} $</p>
<p><em>Step 2: calculate Pragmatic Speaker&rsquo;s Probability of choosing &ldquo;green&rdquo; for each of three referent</em>
(using λ = 1, no cost bias $f(p) = 0$):</p>
<blockquote>
<p><strong>1. probability of intending $r_1$ (green square)</strong> for each option in the utterance/property space</p>
<h4 id="step-1-literal-listener-probabilities">Step 1: Literal Listener Probabilities<a hidden class="anchor" aria-hidden="true" href="#step-1-literal-listener-probabilities">#</a></h4>
<ul>
<li>$P_{literal}(r_1 \mid \text{&ldquo;green&rdquo;}) = \frac{1}{2} = 0.5$</li>
<li>$P_{literal}(r_1 \mid \text{&ldquo;square&rdquo;}) = 1.0$</li>
<li>$P_{literal}(r_1 \mid \text{&ldquo;circle&rdquo;}) = 0$</li>
<li>$P_{literal}(r_1 \mid \text{&ldquo;blue&rdquo;}) = 0$</li>
</ul>
<h4 id="step-2-pragmatic-speaker-probabilities">Step 2: Pragmatic Speaker Probabilities<a hidden class="anchor" aria-hidden="true" href="#step-2-pragmatic-speaker-probabilities">#</a></h4>
<p>The formula of Pragmatic Speaker is:
$$ P_{prod}(p \mid r; \lambda, f) =  \frac{\exp\left( \lambda \cdot EU_{speaker}(r, p; f) \right)}{\sum_{p&rsquo;} \exp\left( \lambda \cdot EU_{speaker}(r, p&rsquo;; f) \right)} = \frac{\exp\left( \lambda \cdot (P_{literal}(r \mid p) + f(p)) \right)}{\sum_{p&rsquo;} \exp\left( \lambda \cdot (P_{literal}(r \mid p&rsquo;) + f(p&rsquo;)) \right)}
$$</p>
<h4 id="step-21-compute-eu-values">step 2.1: Compute $EU$ Values<a hidden class="anchor" aria-hidden="true" href="#step-21-compute-eu-values">#</a></h4>
<ul>
<li>$EU(r_1, \text{&ldquo;green&rdquo;; f}) = P_{literal}(r_1 \mid &ldquo;green&rdquo;) + f(&ldquo;green&rdquo;) = 0.5 + 0 = 0.5$</li>
<li>$EU(r_1, \text{&ldquo;square&rdquo;}; f) = P_{literal}(r_1 \mid &ldquo;green&rdquo;) + f(p) = 1.0 + 0 = 1.0$</li>
<li>$EU(r_1, \text{&ldquo;circle&rdquo;}; f) = P_{literal}(r_1 \mid &ldquo;green&rdquo;) + f(p) = 0 + 0 = 0$</li>
<li>$EU(r_1, \text{&ldquo;blue&rdquo;}; f) = P_{literal}(r_1 \mid &ldquo;blue&rdquo;) + f(p) = 0 + 0 = 0$</li>
</ul>
<h4 id="step-22-exponentiate-and-normalize">Step 2.2: Exponentiate and Normalize<a hidden class="anchor" aria-hidden="true" href="#step-22-exponentiate-and-normalize">#</a></h4>
<p>Exponentiate:</p>
<ul>
<li>$\exp\left( \lambda \cdot EU_{speaker}(r_1, &ldquo;green&rdquo;; f) \right) = \exp(1 \cdot 0.5) = \exp(0.5)\approx 1.649$</li>
<li>$\exp\left( \lambda \cdot EU_{speaker}(r_1, &ldquo;square&rdquo;; f) \right) = \exp(1 \cdot 1.0) = \exp(1)\approx 2.718$</li>
<li>$\exp\left( \lambda \cdot EU_{speaker}(r_1, &ldquo;circle&rdquo;; f) \right) = \exp(1 \cdot 0) = \exp(0) = 1.0$</li>
<li>$\exp\left( \lambda \cdot EU_{speaker}(r_1, &ldquo;blue&rdquo;; f) \right) = \exp(1 \cdot 0) = \exp(0) = 1.0$</li>
</ul>
<p>Total sum:<br>
$$
1.649 + 2.718 + 1.0 + 1.0 = 6.367
$$</p>
<p>Final probabilities:</p>
<ul>
<li>$P_{prod}(\text{&ldquo;green&rdquo;} \mid r_1) = \frac{1.649}{6.367} \approx 0.2589$</li>
<li>$P_{prod}(\text{&ldquo;square&rdquo;} \mid r_1) = \frac{2.718}{6.367} \approx 0.4269$</li>
<li>$P_{prod}(\text{&ldquo;circle&rdquo;} \mid r_1) = \frac{1.0}{6.367} \approx 0.1571$</li>
<li>$P_{prod}(\text{&ldquo;blue&rdquo;} \mid r_1) = \frac{1.0}{6.367} \approx 0.1571$</li>
</ul>
<hr>
<p><strong>2. probability of intending $r_2$ (green circle)</strong> for each option in the utterance/property space</p>
<h4 id="literal-listener-probabilities">Literal Listener Probabilities<a hidden class="anchor" aria-hidden="true" href="#literal-listener-probabilities">#</a></h4>
<ul>
<li>$P_{literal}(r_2 \mid &ldquo;green&rdquo;) = \frac{1}{2} = 0.5$</li>
<li>$P_{literal}(r_2 \mid &ldquo;square&rdquo;) = 0$</li>
<li>$P_{literal}(r_2 \mid &ldquo;circle&rdquo;) = \frac{1}{2} = 0.5$</li>
<li>$P_{literal}(r_2 \mid &ldquo;blue&rdquo;) = 0$</li>
</ul>
<h4 id="step-2-pragmatic-speaker-probabilities-1">Step 2: Pragmatic Speaker Probabilities<a hidden class="anchor" aria-hidden="true" href="#step-2-pragmatic-speaker-probabilities-1">#</a></h4>
<h4 id="step-21-compute-eu-values-1">step 2.1: Compute $EU$ Values<a hidden class="anchor" aria-hidden="true" href="#step-21-compute-eu-values-1">#</a></h4>
<ul>
<li>$EU(r_2, \text{&ldquo;green&rdquo;; f}) = P_{literal}(r_1 \mid &ldquo;green&rdquo;) + f(&ldquo;green&rdquo;) = 0.5 + 0 = 0.5$</li>
<li>$EU(r_2, \text{&ldquo;square&rdquo;}; f) = P_{literal}(r_1 \mid &ldquo;green&rdquo;) + f(p) = 0 + 0 = 0$</li>
<li>$EU(r_2, \text{&ldquo;circle&rdquo;}; f) = P_{literal}(r_1 \mid &ldquo;green&rdquo;) + f(p) = 0.5 + 0 = 0.5 $</li>
<li>$EU(r_2, \text{&ldquo;blue&rdquo;}; f) = P_{literal}(r_1 \mid &ldquo;blue&rdquo;) + f(p) = 0 + 0 = 0 $</li>
</ul>
<h4 id="step-22-exponentiate-and-normalize-1">Step 2.2: Exponentiate and Normalize<a hidden class="anchor" aria-hidden="true" href="#step-22-exponentiate-and-normalize-1">#</a></h4>
<p>Exponentiate:</p>
<ul>
<li>$\exp\left( \lambda \cdot EU_{speaker}(r_2, &ldquo;green&rdquo;; f) \right) = \exp(1 \cdot 0.5) = \exp(0.5)\approx 1.649$</li>
<li>$\exp\left( \lambda \cdot EU_{speaker}(r_2, &ldquo;square&rdquo;; f) \right) = \exp(1 \cdot 0) = \exp(0) = 1$</li>
<li>$\exp\left( \lambda \cdot EU_{speaker}(r_2, &ldquo;circle&rdquo;; f) \right) = \exp(1 \cdot 0.5) = \exp(0.5) \approx 1.649$</li>
<li>$\exp\left( \lambda \cdot EU_{speaker}(r_2, &ldquo;blue&rdquo;; f) \right) = \exp(1 \cdot 0) = \exp(0) = 1$</li>
</ul>
<p>Total:<br>
$$
1.649 + 1 + 1.649 + 1 = 5.298
$$</p>
<p>Final probabilities:</p>
<ul>
<li>$P_{prod}(\text{&ldquo;green&rdquo;} \mid r_2) = \frac{1.649}{4.298} \approx 0.3112$</li>
<li>$P_{prod}(\text{&ldquo;square&rdquo;} \mid r_2) = \frac{1}{4.298} \approx 0.1888$</li>
<li>$P_{prod}(\text{&ldquo;circle&rdquo;} \mid r_2) = \frac{1.649}{4.298} \approx 0.3112$</li>
<li>$P_{prod}(\text{&ldquo;blue&rdquo;} \mid r_2) = \frac{1}{4.298} \approx 0.1888$</li>
</ul>
<hr>
<p><strong>3. probability of intending $r_3$ (blue circle)</strong> for each option in the utterance/property space</p>
<h4 id="literal-listener-probabilities-1">Literal Listener Probabilities<a hidden class="anchor" aria-hidden="true" href="#literal-listener-probabilities-1">#</a></h4>
<ul>
<li>$P_{literal}(r_2 \mid &ldquo;green&rdquo;) = 0$</li>
<li>$P_{literal}(r_2 \mid &ldquo;square&rdquo;) = 0$</li>
<li>$P_{literal}(r_2 \mid &ldquo;circle&rdquo;) = \frac{1}{2} = 0.5$</li>
<li>$P_{literal}(r_2 \mid &ldquo;blue&rdquo;) = \frac{1}{1} = 1$</li>
</ul>
<h4 id="step-2-pragmatic-speaker-probabilities-2">Step 2: Pragmatic Speaker Probabilities<a hidden class="anchor" aria-hidden="true" href="#step-2-pragmatic-speaker-probabilities-2">#</a></h4>
<h4 id="step-21-compute-eu-values-2">step 2.1: Compute $EU$ Values<a hidden class="anchor" aria-hidden="true" href="#step-21-compute-eu-values-2">#</a></h4>
<ul>
<li>$EU(r_3, \text{&ldquo;green&rdquo;; f}) = P_{literal}(r_3 \mid &ldquo;green&rdquo;) + f(&ldquo;green&rdquo;) = 0 + 0 = 0$</li>
<li>$EU(r_3, \text{&ldquo;square&rdquo;}; f) = P_{literal}(r_3 \mid &ldquo;green&rdquo;) + f(p) = 0 + 0 = 0$</li>
<li>$EU(r_3, \text{&ldquo;circle&rdquo;}; f) = P_{literal}(r_3 \mid &ldquo;green&rdquo;) + f(p) = 0.5 + 0 = 0.5 $</li>
<li>$EU(r_3, \text{&ldquo;blue&rdquo;}; f) = P_{literal}(r_3 \mid &ldquo;blue&rdquo;) + f(p) = 1 + 0 = 1 $</li>
</ul>
<h4 id="step-22-exponentiate-and-normalize-2">Step 2.2: Exponentiate and Normalize<a hidden class="anchor" aria-hidden="true" href="#step-22-exponentiate-and-normalize-2">#</a></h4>
<p>Exponentiate:</p>
<ul>
<li>$\exp\left( \lambda \cdot EU_{speaker}(r_3, &ldquo;green&rdquo;; f) \right) = \exp(1 \cdot 0) = \exp(0) = 1$</li>
<li>$\exp\left( \lambda \cdot EU_{speaker}(r_3, &ldquo;square&rdquo;; f) \right) = \exp(1 \cdot 0) = \exp(0) = 1$</li>
<li>$\exp\left( \lambda \cdot EU_{speaker}(r_3, &ldquo;circle&rdquo;; f) \right) = \exp(1 \cdot 0.5) = \exp(0.5) \approx 1.649$</li>
<li>$\exp\left( \lambda \cdot EU_{speaker}(r_3, &ldquo;blue&rdquo;; f) \right) = \exp(1 \cdot 1) = \exp(1) \approx 2.718$</li>
</ul>
<p>Total:  $ 1 + 1 + 1.649 + 2.718 = 6.367 $</p>
<p>Final probabilities:</p>
<ul>
<li>$P_{prod}(\text{&ldquo;green&rdquo;} \mid r_3) = \frac{1}{6.367} \approx 0.1571$</li>
<li>$P_{prod}(\text{&ldquo;square&rdquo;} \mid r_3) = \frac{1}{6.367} \approx 0.1571$</li>
<li>$P_{prod}(\text{&ldquo;circle&rdquo;} \mid r_3) = \frac{1.64}{6.367} \approx 0.2589$</li>
<li>$P_{prod}(\text{&ldquo;blue&rdquo;} \mid r_3) = \frac{2.718}{6.367} \approx 0.4269$</li>
</ul></blockquote>
<hr>
<h3 id="vii-indirect-speech-acts-in-rsa">VII. Indirect Speech Acts in RSA<a hidden class="anchor" aria-hidden="true" href="#vii-indirect-speech-acts-in-rsa">#</a></h3>
<p><em>Based on Franke &amp; Jäger (2016), Section 6</em></p>
<h3 id="71-what-are-indirect-speech-acts">7.1 What Are Indirect Speech Acts?<a hidden class="anchor" aria-hidden="true" href="#71-what-are-indirect-speech-acts">#</a></h3>
<p>An <strong>indirect speech act</strong> occurs when a speaker utters something whose <strong>literal meaning</strong> differs from their <strong>intended communicative goal</strong>. For example:</p>
<blockquote>
<p><strong>&ldquo;Can you pass the salt?&rdquo;</strong></p>
<ul>
<li><strong>Literal interpretation</strong>: A question about your ability.</li>
<li><strong>Pragmatic interpretation</strong>: A polite request to pass the salt.</li>
</ul></blockquote>
<p>These types of speech acts are pervasive in everyday communication and raise a key challenge for formal models of meaning. How can a listener derive an intended meaning that diverges from the literal form?</p>
<p>The <strong>Rational Speech Act (RSA)</strong> model provides a solution by modeling speakers and listeners as rational agents engaging in recursive social reasoning — not just about <em>what was said</em>, but <em>why it was said</em> in a given context.</p>
<hr>
<p>Certainly! Below is the revised section <strong>“6.1 The Problem”</strong> formatted in <strong>Hugo markdown syntax</strong> with clear bullet points, examples, and beginner-friendly explanation. This version is tailored for your RSA learning notes website.</p>
<hr>
<h3 id="72-the-problem-why-indirect-speech-acts-seem-irrationalbut-arent">7.2 The Problem: Why Indirect Speech Acts Seem Irrational—But Aren’t<a hidden class="anchor" aria-hidden="true" href="#72-the-problem-why-indirect-speech-acts-seem-irrationalbut-arent">#</a></h3>
<p>Indirect speech acts pose a foundational puzzle for rational models of communication. At first glance, they appear to violate classic Gricean principles:</p>
<ul>
<li><strong>Quantity</strong>: Be as informative as necessary.</li>
<li><strong>Manner</strong>: Avoid obscurity and ambiguity.</li>
<li><strong>Relation</strong>: Be relevant.</li>
<li><strong>Quality</strong>: Say what you believe to be true.</li>
</ul>
<p>Yet in daily life, people routinely use—and understand—<strong>indirect speech</strong> effectively. So why do speakers often choose <strong>less direct expressions</strong>, especially when their intent is clear?</p>
<h4 id="whats-the-puzzle">What&rsquo;s the Puzzle?<a hidden class="anchor" aria-hidden="true" href="#whats-the-puzzle">#</a></h4>
<p>According to Gricean maxims, indirect utterances seem:</p>
<ul>
<li><strong>Less informative</strong> than needed (Quantity),</li>
<li><strong>More ambiguous</strong> than necessary (Manner),</li>
<li>Sometimes even <strong>irrelevant</strong> on the surface (Relation).</li>
</ul>
<p>This suggests that indirect speech is <strong>irrational</strong>—and yet, it&rsquo;s ubiquitous and interpretable. To resolve this contradiction, Franke and Jäger (2016) draw from the broader field of <strong>rationalist pragmatics</strong>, especially ideas from <strong>Brown &amp; Levinson (1987)</strong> and <strong>Steven Pinker (2008)</strong>.</p>
<p>Their key insight:</p>
<blockquote>
<p><strong>Indirect speech is rational when we consider social context, strategic goals, and reputational concerns.</strong></p></blockquote>
<ul>
<li>
<h4 id="three-social-pragmatic-motivations-after-pinker-2008">Three Social-Pragmatic Motivations (after Pinker, 2008)<a hidden class="anchor" aria-hidden="true" href="#three-social-pragmatic-motivations-after-pinker-2008">#</a></h4>
</li>
</ul>
<h4 id="1-plausible-deniability">1. Plausible Deniability<a hidden class="anchor" aria-hidden="true" href="#1-plausible-deniability">#</a></h4>
<blockquote>
<p><em>Example</em>:<br>
The veiled bribe is another recognizable plot device, as when the kidnapper in Fargo shows a police officer his drivers’s license in a wallet with a fifty-dollar bill protruding from it and suggests, ‘So maybe the best thing would be to take care of that here in Brainerd.’ (Pinker 2008: 374)</p>
<p align="center"></blockquote>
  <img src="/images/Brainerd.jpg" alt="Fargo" width="600px">
</p>
<ul>
<li>The <strong>literal meaning</strong> is vague.</li>
<li>The <strong>implied meaning</strong> (a bribe) is understood—but <strong>not explicit</strong>.</li>
<li>If challenged, the speaker can deny the implicature.</li>
</ul>
<p>👉 <strong>Why?</strong> Indirectness helps the speaker avoid social or legal consequences if the implicature is rejected or punished.</p>
<h4 id="2-establishing-shared-knowledge-without-common-knowledge">2. Establishing Shared Knowledge Without Common Knowledge<a hidden class="anchor" aria-hidden="true" href="#2-establishing-shared-knowledge-without-common-knowledge">#</a></h4>
<blockquote>
<p><em>Example</em>:
“Would you like to come up for coffee?” (instead of “Do you want to have sex?”)</p></blockquote>
<ul>
<li>The utterance signals intent.</li>
<li>Both speaker and hearer <strong>recognize the implicature</strong>.</li>
<li>But they avoid making it <strong>common knowledge</strong>—publicly acknowledged by both parties.</li>
</ul>
<p><strong>Why?</strong> Maintaining deniability or politeness can protect both parties from embarrassment or rejection.</p>
<h4 id="3-preserving-social-relationships-and-roles">3. Preserving Social Relationships and Roles<a hidden class="anchor" aria-hidden="true" href="#3-preserving-social-relationships-and-roles">#</a></h4>
<blockquote>
<p><em>Example</em>:
A customer tells the maître d’, “Is there any way to get a table sooner?” instead of “Here’s $50 to seat me now.”</p></blockquote>
<ul>
<li>The <strong>direct request</strong> would frame the situation as a <strong>market transaction</strong>.</li>
<li>The <strong>indirect request</strong> preserves the social role of a courteous guest.</li>
</ul>
<p>👉 <strong>Why?</strong> Indirect speech <strong>protects the relational frame</strong> and avoids signaling an inappropriate relationship type (e.g., transactional vs. respectful).</p>
<hr>
<blockquote>
<ul>
<li><strong>shared knowledge vs. common knowledge</strong></li>
</ul>
<h4 id="key-distinction">Key Distinction<a hidden class="anchor" aria-hidden="true" href="#key-distinction">#</a></h4>
<p>Understanding <strong>indirect speech acts</strong> often requires distinguishing between two types of mutual understanding:</p>
<h4 id="shared-knowledge">Shared Knowledge<a hidden class="anchor" aria-hidden="true" href="#shared-knowledge">#</a></h4>
<ul>
<li>Both speaker and listener know a fact.</li>
<li>However, <strong>there is no public acknowledgment</strong> that both know it.</li>
<li>This creates a zone of <strong>plausible deniability</strong>.</li>
</ul>
<h4 id="common-knowledge">Common Knowledge<a hidden class="anchor" aria-hidden="true" href="#common-knowledge">#</a></h4>
<ul>
<li>Not only do both know it, but <strong>they know that the other knows it</strong>—and this fact is <strong>mutually acknowledged</strong>.</li>
<li>Once a fact becomes common knowledge, it’s <strong>out in the open</strong> and cannot be easily denied.</li>
</ul>
<h4 id="example-1-romantic-context">Example 1: Romantic Context<a hidden class="anchor" aria-hidden="true" href="#example-1-romantic-context">#</a></h4>
<p><strong>Indirect speech</strong>: “Would you like to come up for coffee?”<br>
<strong>Direct speech</strong>: “Do you want to have sex?”</p>
<ul>
<li>Both interlocutors may <strong>know</strong> the intent behind &ldquo;coffee&rdquo; (shared knowledge).</li>
<li>But they <strong>avoid stating it directly</strong>, which would make it common knowledge.</li>
<li>This preserves <strong>social harmony</strong> and allows either party to save face.</li>
</ul>
<h4 id="example-2-workplace-context">Example 2: Workplace Context<a hidden class="anchor" aria-hidden="true" href="#example-2-workplace-context">#</a></h4>
<p><strong>Indirect</strong>: “It’s a little warm in here, isn’t it?”<br>
<strong>Direct</strong>: “Please turn on the air conditioning.”</p>
<ul>
<li>The <strong>boss’s intent</strong> is understood (shared knowledge),</li>
<li>But it’s <strong>not framed as a direct order</strong>, keeping it from becoming common knowledge.</li>
<li>This helps <strong>preserve hierarchical boundaries</strong> without confrontation.</li>
</ul>
<p>Indirect speech often works by <strong>deliberately avoiding the shift from shared to common knowledge</strong>.</p>
<hr>
<ul>
<li><strong>Avoiding the Mixing of Relationship Types</strong></li>
</ul>
<p>One reason for using <strong>indirect speech acts</strong> is to <strong>preserve the boundaries</strong> between <strong>different social relationship types</strong>, such as professional vs. romantic, or hierarchical vs. egalitarian.</p>
<p>When people communicate across <strong>multiple social dimensions</strong>, they must <strong>signal which frame of relationship</strong> they are currently invoking. Direct speech might <strong>trigger an unwanted interpretation</strong> associated with another relationship type.</p>
<h4 id="example-professor-and-student">Example: Professor and Student<a hidden class="anchor" aria-hidden="true" href="#example-professor-and-student">#</a></h4>
<p><strong>Indirect</strong>: “Would you be interested in attending the conference with me?”<br>
<strong>Direct</strong>: “Would you like to go on a date with me?”</p>
<ul>
<li>The <strong>indirect question</strong> may preserve the <strong>professional relationship</strong>, while opening space for a social connection.</li>
<li>A <strong>direct question</strong> risks re-framing the interaction as <strong>romantic</strong>, which may be inappropriate or problematic.</li>
</ul>
<h4 id="example-manager-and-subordinate">Example: Manager and Subordinate<a hidden class="anchor" aria-hidden="true" href="#example-manager-and-subordinate">#</a></h4>
<p><strong>Indirect</strong>: “If you have time later, maybe you could take a look at this?”<br>
<strong>Direct</strong>: “I’m assigning this task to you.”</p>
<ul>
<li>A <strong>manager might avoid sounding too authoritarian</strong>, maintaining a more <strong>collegial tone</strong>.</li>
<li>Mixing directive and collaborative frames could create discomfort.</li>
<li><strong>Indirectness helps the speaker frame the interaction</strong> in a way that suits the current social dynamic.</li>
</ul></blockquote>
<hr>
<h3 id="73-the-deeper-problem">7.3 The Deeper Problem<a hidden class="anchor" aria-hidden="true" href="#73-the-deeper-problem">#</a></h3>
<p>The real puzzle isn’t “why do indirect speech acts work?” but rather:</p>
<blockquote>
<p><strong>When and why is it rational to use them?</strong></p></blockquote>
<p>What seems irrational in terms of raw information transfer becomes rational when:</p>
<ul>
<li>Social dynamics,</li>
<li>Strategic ambiguity,</li>
<li>and Relationship management</li>
</ul>
<p>are taken into account.</p>
<p>Franke and Jäger argue that <strong>game-theoretic models</strong>, especially from the <strong>RSA/IBR family</strong>, can model these choices:</p>
<ul>
<li>Agents (speakers and listeners) have <strong>utilities</strong> that include <strong>social and reputational costs</strong>.</li>
<li>Speakers reason about <strong>how listeners will interpret them</strong>.</li>
<li>Listeners reason about <strong>why a speaker chose a particular form</strong> over another.</li>
</ul>
<hr>
<h3 id="74-why-do-indirect-speech-acts-work-rsa-explanation">7.4 Why Do Indirect Speech Acts Work? (RSA Explanation)<a hidden class="anchor" aria-hidden="true" href="#74-why-do-indirect-speech-acts-work-rsa-explanation">#</a></h3>
<p>Indirect speech acts—where speakers imply meaning rather than state it directly—can seem puzzling. RSA offers a clear game-theoretic approach to explaining why these acts work rationally, despite their apparent indirectness.</p>
<h4 id="step-by-step-explanation-stalnakers-example">Step-by-Step Explanation (Stalnaker&rsquo;s Example)<a hidden class="anchor" aria-hidden="true" href="#step-by-step-explanation-stalnakers-example">#</a></h4>
<p>The authors illustrate their RSA approach using a real-world scenario involving an indirect speech act by the US Treasury Secretary:</p>
<blockquote>
<blockquote>
<p><em>In May, 2003, the US Treasury Secretary, John Snow, in response to a question, made some remarks that caused the dollar to drop precipitously in value. The Wall Street Journal sharply criticized him for ‘playing with fire,’ and characterized his remarks as ‘dumping on his own currency,’ ‘bashing the dollar,’ and ‘talking the dollar down.’ What he in fact said was this: ‘<strong>When the dollar is at a lower level it helps exports, and I think exports are getting stronger as a result</strong>.’ This was an uncontroversial factual claim that everyone, whatever his or her views about what US government currency policy is or should be, would agree with. Why did it have such an impact? (Stalnaker 2005: 82)</em></p></blockquote></blockquote>
<p><strong>Utterance $s$ (indirect)</strong>:</p>
<blockquote>
<p>“When the dollar is at a lower level it helps exports, and I think exports are getting stronger as a result.”</p></blockquote>
<p><strong>Implied meaning (direct)</strong>:</p>
<blockquote>
<p>“The US Treasury will take measures to lower the dollar’s exchange rate.”</p></blockquote>
<p><strong>A rationalist account based on decision and game theory</strong></p>
<p><em><strong>Step 1: Define Speaker’s Possible Intentions (&ldquo;Types&rdquo;)</strong></em></p>
<p>We consider two possible &ldquo;types&rdquo; of speaker (S):</p>
<ul>
<li><strong>Type $t_1$</strong>: The speaker <strong>plans to reduce</strong> the dollar’s value.</li>
<li><strong>Type $t_2$</strong>: The speaker has <strong>no plan to reduce</strong> the dollar’s value.</li>
</ul>
<p>prior assumptions about the relative likelihood of $t_1$ and $t_2$:</p>
<ul>
<li>prior probability distribution of $t_1$: $P(t_1)$</li>
<li>prior probability distribution of $t_1$: $P(t_2)$</li>
</ul>
<p>The listener $L$ initially has uncertainty regarding these two possibilities:</p>
<p>$$
P(t_1) + P(t_2) = 1,\quad 0 &lt; P(t_1) &lt; 1
$$</p>
<p><strong>Step 2: Determine Likelihood of Utterance Given Speaker’s Intentions</strong></p>
<p>How likely is it that $S$ would utter $s$ in $t_1$, and in $t_2$?</p>
<p>Next, consider the likelihood that each speaker type would produce the indirect utterance $s$:</p>
<ul>
<li>$P(s|t_1)$: Probability the speaker would say $s$ if planning to lower the dollar ($t_1$).</li>
<li>$P(s|t_2)$: Probability the speaker would say $s$ if not planning any action ($t_2$).</li>
</ul>
<p>Because the statement made is generally an economic truism, it&rsquo;s plausible for both types to utter it.</p>
<ul>
<li>for $t_1$ it would be a useful argument to justify his intentions; thus, the statement is significantly more meaningful for a speaker who plans to act (type $t_1$)</li>
<li>for $t_2$, t2 utters this sentence, just to say something meaningless during a public hearing. As there myriads of meaningless statements to choose from, this likelihood is small.</li>
</ul>
<p>Thus:
$$
P(s|t_1) &gt; P(s|t_2)
$$</p>
<p>Intuitively:</p>
<ul>
<li>A speaker planning to act ($t_1$) would choose such a statement intentionally (making their intent clear indirectly).</li>
<li>A speaker not planning to act ($t_2$) would rarely choose this specific statement, given many equally trivial statements available.</li>
</ul>
<p><strong>Step 3: Listener Updates Beliefs (Bayesian Reasoning)</strong></p>
<p>the listener $L$ will use Bayes’ rule to compute the posterior probability distribution over S’s types $t$, given the signal observed $s$</p>
<p>$$
P(t_1|s) = \frac{P(s|t_1)P(t_1)}{\sum_{(t&rsquo;)}P(s|t&rsquo;)P(t&rsquo;)}
$$</p>
<p>also:</p>
<p>$$
P(t_1|s) = \frac{P(s|t_1)P(t_1)}{P(s|t_1)P(t_1) + P(s|t_2)P(t_2)}
$$</p>
<p>Since $P(s|t_1) &gt; P(s|t_2)$, we have:</p>
<p>$$
P(t_1|s) &gt; P(t_1)
$$</p>
<p>This means hearing the utterance <strong>increases the listener’s belief</strong> that the speaker plans to lower the dollar.</p>
<h4 id="key-insight-1">Key Insight<a hidden class="anchor" aria-hidden="true" href="#key-insight-1">#</a></h4>
<ul>
<li>The indirect utterance (economic truism) <strong>raises the listener&rsquo;s belief</strong> in the speaker&rsquo;s intention without explicitly confirming it.</li>
<li>If the utterance were completely direct (e.g., “The US Treasury will lower the dollar”), it would remove all uncertainty—potentially creating commitment or unwanted accountability for the speaker.</li>
</ul>
<p>Thus, indirectness <strong>maintains plausible deniability</strong> while effectively communicating intent.</p>
<blockquote>
<blockquote>
<p><strong>Why does $ P(s \mid t_1) &gt; P(s \mid t_2) $ imply $ P(t_1 \mid s) &gt; P(t_1) $?</strong><br>
$$P(s \mid t_1) &gt; P(s \mid t_2)$$
$$ \frac{P(s \mid t_1)}{P(s \mid t_2)} &gt; 1 $$
$$ \frac{P(s \mid t_1)P(t_1)}{P(s \mid t_2)(P(t_2))} &gt; \frac{P(t_1)}{P(t_2)}$$
$$ \frac{ \frac{P(s \mid t_1)P(t_1)}{P(s)}}{\frac{P(s \mid t_2)(P(t_2))}{P(s)}} &gt; \frac{P(t_1)}{P(t_2)}$$
as Bayesian Theorem stipulates: $\frac{P(s \mid t)P(t)}{P(s)} = P(t \mid s)$
$$ \frac{P(t_1 \mid s)}{P(t_2 \mid s)} &gt; \frac{P(t_1)}{P(t_2)} $$
as in this setting: $P(t_1) + P(t_2) = 1$ and $P(t_1 \mid s) + P(t_2 \mid s) = 1$
$$\frac{P(t_1 \mid s)}{1 - P(t_1 \mid s)} &gt; \frac{P(t_1)}{1 - P(t_1)}$$
$$P(t_1 \mid s) - P(t_1)P(t_1 \mid s) &gt; P(t_1) - P(t_1)P(t_1 \ mid s)$$
$$P(t_1 \mid s) &gt; P(t_1)$$</p></blockquote></blockquote>
<hr>
<h3 id="75-why-rsa-models-capture-this-rationality">7.5 Why RSA Models Capture This Rationality:<a hidden class="anchor" aria-hidden="true" href="#75-why-rsa-models-capture-this-rationality">#</a></h3>
<p>RSA rationalizes indirect speech acts as:</p>
<ul>
<li><strong>Strategic choices</strong> made by speakers to influence listener beliefs without explicit commitment.</li>
<li><strong>Optimally balancing</strong> social cost (being explicit can be risky) and communicative effectiveness (the listener still infers intent clearly).</li>
</ul>
<hr>
<h4 id="rsa-summary-stalnakers-example">RSA Summary (Stalnaker&rsquo;s Example):<a hidden class="anchor" aria-hidden="true" href="#rsa-summary-stalnakers-example">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>Explanation (in RSA terms)</th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$t_1, t_2$</td>
          <td>Possible speaker intentions</td>
          <td></td>
      </tr>
      <tr>
          <td>$P(t)$</td>
          <td>Listener’s prior beliefs</td>
          <td></td>
      </tr>
      <tr>
          <td>$P(s|t)$</td>
          <td>Likelihood speaker utters sentence given intention</td>
          <td></td>
      </tr>
      <tr>
          <td>$P(t|s) $</td>
          <td>Listener’s updated belief after hearing utterance</td>
          <td></td>
      </tr>
      <tr>
          <td>Indirectness</td>
          <td>Rational strategy for influencing listener beliefs without full accountability</td>
          <td></td>
      </tr>
  </tbody>
</table>
<hr>
<h4 id="intuition-check">Intuition Check:<a hidden class="anchor" aria-hidden="true" href="#intuition-check">#</a></h4>
<blockquote>
<p><strong>Indirectness is rational</strong> because it strategically <strong>affects listener beliefs</strong> without explicitly committing the speaker. It shifts probabilities, making some interpretations highly likely—but not guaranteed—thus preserving crucial <strong>social and strategic flexibility</strong>.</p></blockquote>
<hr>
<blockquote>
<p><strong>Step 3: Bayesian Reasoning and Listener Belief Updating</strong></p>
<p>When a listener hears an utterance — especially an <em>indirect</em> one — they <strong>update their beliefs</strong> about what the speaker intends.<br>
RSA models this belief updating using <strong>Bayes’ rule</strong>, a core principle in probabilistic reasoning.</p>
<h4 id="example-franke--jäger-2016">Example (Franke &amp; Jäger, 2016)<a hidden class="anchor" aria-hidden="true" href="#example-franke--jäger-2016">#</a></h4>
<blockquote>
<p><strong>Utterance (indirect)</strong>:<br>
“When the dollar is at a lower level, it helps exports, and I think exports are getting stronger as a result.”</p>
<p><strong>Possible speaker intentions</strong>:<br>
• <em>t₁</em>: The speaker intends to lower the dollar.<br>
• <em>t₂</em>: The speaker does <strong>not</strong> intend any action.</p></blockquote>
<h4 id="bayes-rule-formal">Bayes&rsquo; Rule (Formal)<a hidden class="anchor" aria-hidden="true" href="#bayes-rule-formal">#</a></h4>
<p>$$
P(t_1 \mid s) = \frac{P(s \mid t_1) \cdot P(t_1)}{P(s)}
$$</p>
<p>• $P(t_1 \mid s)$: Updated belief about speaker&rsquo;s intention <em>t₁</em> after hearing utterance <em>s</em>.<br>
• $P(s \mid t_1)$: Likelihood of producing utterance <em>s</em> given intention <em>t₁</em>.<br>
• $P(t_1)$: Prior belief about intention <em>t₁</em>.<br>
• $P(s)$: Total probability of <em>s</em>, marginalizing over all possible intentions.</p>
<h4 id="sample-calculation">Sample Calculation:<a hidden class="anchor" aria-hidden="true" href="#sample-calculation">#</a></h4>
<ul>
<li>Prior: $P(t_1) = 0.5$, $P(t_2) = 0.5$</li>
<li>Likelihood: $P(s \mid t_1) = 0.8$, $P(s \mid t_2) = 0.2$</li>
<li>Marginal:<br>
$$
P(s) = (0.8 \cdot 0.5) + (0.2 \cdot 0.5) = 0.5
$$</li>
<li>Posterior:<br>
$$
P(t_1 \mid s) = \frac{0.8 \cdot 0.5}{0.5} = 0.8
$$</li>
</ul>
<p>The listener now assigns <strong>80% probability</strong> to the speaker intending to lower the dollar — a strong update from the original 50%.</p>
<h4 id="why-it-matters">Why It Matters<a hidden class="anchor" aria-hidden="true" href="#why-it-matters">#</a></h4>
<p>RSA shows that <strong>even vague or indirect utterances</strong> can shift beliefs dramatically through rational inference.<br>
This explains why <strong>indirect speech acts are effective</strong>, despite their surface ambiguity.</p></blockquote>
<hr>
<h3 id="74-rsa-perspective-on-indirectness">7.4 RSA Perspective on Indirectness<a hidden class="anchor" aria-hidden="true" href="#74-rsa-perspective-on-indirectness">#</a></h3>
<p>In the RSA framework, interpretation goes beyond truth conditions. Speakers are modeled as optimizing communicative goals, and listeners reason backward from utterances to infer those goals.</p>
<p>Franke &amp; Jäger (2016) formalize this with the notion of <strong>goal-based RSA</strong>:</p>
<ul>
<li>Each utterance $u$ can serve multiple <strong>communicative goals</strong> $g$.</li>
<li>Listeners compute the most likely <strong>goal</strong> the speaker intended, given the utterance.</li>
</ul>
<h4 id="communicative-goals-as-targets-of-inference">Communicative Goals as Targets of Inference<a hidden class="anchor" aria-hidden="true" href="#communicative-goals-as-targets-of-inference">#</a></h4>
<p>Let:</p>
<ul>
<li>
<p>$U$ = utterance space</p>
</li>
<li>
<p>$G$ = space of possible communicative goals</p>
</li>
<li>
<p>Example goals:</p>
<ul>
<li>$g_1$: Request (e.g., &ldquo;please pass the salt&rdquo;)</li>
<li>$g_2$: Information inquiry (e.g., &ldquo;are you able to pass the salt?&rdquo;)</li>
</ul>
</li>
</ul>
<h4 id="rsa-inference-for-indirect-speech">RSA Inference for Indirect Speech<a hidden class="anchor" aria-hidden="true" href="#rsa-inference-for-indirect-speech">#</a></h4>
<h4 id="step-1-pragmatic-speaker">Step 1: Pragmatic Speaker<a hidden class="anchor" aria-hidden="true" href="#step-1-pragmatic-speaker">#</a></h4>
<p>The <strong>speaker</strong> chooses an utterance $u$ to realize goal $g$ by maximizing its <strong>utility</strong>:</p>
<p>$$
P_{S_1}(u \mid g) \propto \exp\left( \lambda \cdot U(u, g) \right)
$$</p>
<ul>
<li>$U(u, g)$ is the utility of using utterance $u$ to achieve goal $g$</li>
<li>$\lambda$ is the <strong>rationality parameter</strong>, controlling how deterministic the speaker is</li>
</ul>
<h4 id="step-2-pragmatic-listener">Step 2: Pragmatic Listener<a hidden class="anchor" aria-hidden="true" href="#step-2-pragmatic-listener">#</a></h4>
<p>The <strong>listener</strong> infers the likely goal $g$ that motivated $u$:</p>
<p>$$
P_{L_1}(g \mid u) \propto P_{S_1}(u \mid g) \cdot P(g)
$$</p>
<ul>
<li>$P(g)$: prior probability of goal $g$ (based on context/world knowledge)</li>
<li>$P_{S_1}(u \mid g)$: speaker’s production probability</li>
</ul>
<p>This equation is a direct application of <strong>Bayes’ Rule</strong>.</p>
<h4 id="illustration-can-you-pass-the-salt">Illustration: “Can you pass the salt?”<a hidden class="anchor" aria-hidden="true" href="#illustration-can-you-pass-the-salt">#</a></h4>
<h4 id="scenario">Scenario<a hidden class="anchor" aria-hidden="true" href="#scenario">#</a></h4>
<ul>
<li>Speaker utters: <em>&ldquo;Can you pass the salt?&rdquo;</em></li>
<li>Literal content = question about ability ($g_2$)</li>
<li>Context: speaker and listener are seated at dinner</li>
</ul>
<h4 id="listeners-reasoning">Listener’s reasoning:<a hidden class="anchor" aria-hidden="true" href="#listeners-reasoning">#</a></h4>
<ol>
<li>
<p><strong>Literal listener</strong> interprets $u$ as ability question.</p>
</li>
<li>
<p><strong>Pragmatic listener</strong> reasons:</p>
<blockquote>
<p>“If the speaker truly wanted information, would they have said this?
Or is it more likely they’re politely requesting action?”</p></blockquote>
</li>
</ol>
<p>Given the <strong>contextual prior</strong> $P(g_1) &gt; P(g_2)$, and assuming $P_{S_1}(u \mid g_1)$ is high, the listener concludes the intended goal is $g_1$ (request).</p>
<h4 id="why-this-works">Why This Works<a hidden class="anchor" aria-hidden="true" href="#why-this-works">#</a></h4>
<p>RSA formalizes a long-standing idea in pragmatics:</p>
<ul>
<li>Utterances are <strong>goal-directed</strong></li>
<li>Indirectness arises from <strong>optimization under social and contextual constraints</strong></li>
</ul>
<p>Rather than assuming fixed speech act types, RSA derives them as the outcome of:</p>
<ul>
<li><strong>Rational choice by the speaker</strong>, balancing informativeness, politeness, and cost</li>
<li><strong>Rational inference by the listener</strong>, integrating form, context, and expectations</li>
</ul>
<h4 id="summary-table">Summary Table<a hidden class="anchor" aria-hidden="true" href="#summary-table">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$g$</td>
          <td>Communicative goal (e.g., request, information-seeking)</td>
      </tr>
      <tr>
          <td>$u$</td>
          <td>Observed utterance (e.g., “Can you pass the salt?”)</td>
      </tr>
      <tr>
          <td>$P(g)$</td>
          <td>Prior plausibility of goal $g$</td>
      </tr>
      <tr>
          <td>$P_{S_1}(u \mid g)$</td>
          <td>Probability speaker would say $u$ to achieve $g$</td>
      </tr>
      <tr>
          <td>$P_{L_1}(g \mid u)$</td>
          <td>Listener’s inferred probability that $g$ is the intended goal</td>
      </tr>
  </tbody>
</table>
<h4 id="takeaways">Takeaways<a hidden class="anchor" aria-hidden="true" href="#takeaways">#</a></h4>
<ul>
<li>RSA treats speech acts as probabilistic goal inferences.</li>
<li>Indirect speech acts are explained as <strong>intentional ambiguity</strong> for reasons like politeness or plausible deniability.</li>
<li>By using recursive reasoning about goals, RSA naturally derives indirect meaning as an emergent property of rational communication.</li>
</ul>
<hr>
<hr>
<h3 id="extra">Extra<a hidden class="anchor" aria-hidden="true" href="#extra">#</a></h3>
<h3 id="extra-1-lambda-lambda">Extra #1 Lambda $\lambda$<a hidden class="anchor" aria-hidden="true" href="#extra-1-lambda-lambda">#</a></h3>
<p>In the Rational Speech Act (RSA) model, <strong>λ (lambda)</strong> appears in the <strong>Pragmatic Speaker equation</strong>, where it controls how strongly the speaker favors higher-utility utterances.</p>
<h4 id="core-intuition">Core Intuition:<a hidden class="anchor" aria-hidden="true" href="#core-intuition">#</a></h4>
<blockquote>
<p>λ determines how <strong>deterministic or probabilistic</strong> the speaker’s utterance choices are.</p></blockquote>
<h4 id="where-does-λ-appear">Where Does λ Appear?<a hidden class="anchor" aria-hidden="true" href="#where-does-λ-appear">#</a></h4>
<p>In <strong>Franke &amp; Jäger (2016)</strong>, the Pragmatic Speaker is modeled as:</p>
<p>$$
P_{prod}(p \mid r; \lambda, f) = \frac{\exp\left( \lambda \cdot EU(r, p; f) \right)}{\sum_{p&rsquo;} \exp\left( \lambda \cdot EU(r, p&rsquo;; f) \right)}
$$</p>
<ul>
<li>$p$: utterance (property)</li>
<li>$r$: intended referent</li>
<li>$f(p)$: cost or bias of utterance</li>
<li>$EU(r, p; f)$: utility of utterance $p$ for referent $r$</li>
<li><strong>λ</strong>: rationality parameter</li>
</ul>
<h4 id="what-does-λ-do">What Does λ Do?<a hidden class="anchor" aria-hidden="true" href="#what-does-λ-do">#</a></h4>
<ul>
<li>λ <strong>scales</strong> the utility before exponentiation.</li>
<li>It determines <strong>how much more likely</strong> the speaker is to choose high-utility utterances over others.</li>
</ul>
<h4 id="effects-of-different-λ-values">Effects of Different λ Values<a hidden class="anchor" aria-hidden="true" href="#effects-of-different-λ-values">#</a></h4>
<table>
  <thead>
      <tr>
          <th>λ Value</th>
          <th>Speaker Behavior</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>0</td>
          <td>Completely random (uniform choice)</td>
      </tr>
      <tr>
          <td>~0.5</td>
          <td>Weak preference for better options</td>
      </tr>
      <tr>
          <td>1</td>
          <td>Moderate, probabilistic choice</td>
      </tr>
      <tr>
          <td>&gt;5</td>
          <td>Strong preference for best option</td>
      </tr>
      <tr>
          <td>→ ∞</td>
          <td>Always chooses highest-utility utterance</td>
      </tr>
  </tbody>
</table>
<h4 id="mathematical-background">Mathematical Background<a hidden class="anchor" aria-hidden="true" href="#mathematical-background">#</a></h4>
<h4 id="softmax-function">Softmax Function<a hidden class="anchor" aria-hidden="true" href="#softmax-function">#</a></h4>
<p>The softmax is a smooth version of the <strong>argmax</strong>:</p>
<p>$$
\text{softmax}_i(x) = \frac{\exp(\lambda x_i)}{\sum_j \exp(\lambda x_j)}
$$</p>
<ul>
<li>λ controls the <strong>sharpness</strong> of the preference.</li>
<li>High λ → output resembles a hard decision (argmax).</li>
<li>Low λ → output is closer to uniform probability.</li>
</ul>
<h4 id="connection-to-statistical-physics">Connection to Statistical Physics<a hidden class="anchor" aria-hidden="true" href="#connection-to-statistical-physics">#</a></h4>
<p>In the <strong>Boltzmann distribution</strong>:</p>
<p>$$
P_i = \frac{\exp(-E_i / kT)}{\sum_j \exp(-E_j / kT)}
$$</p>
<ul>
<li>$T$ is temperature.</li>
<li>RSA’s λ is analogous to <strong>inverse temperature</strong>:</li>
</ul>
<p>$$
\lambda = \frac{1}{T}
$$</p>
<ul>
<li>Low temperature (high λ) → more deterministic.</li>
<li>High temperature (low λ) → more randomness.</li>
</ul>
<h4 id="link-to-decision-theory">Link to Decision Theory<a hidden class="anchor" aria-hidden="true" href="#link-to-decision-theory">#</a></h4>
<ul>
<li>Softmax choice reflects <strong>bounded rationality</strong>.</li>
<li>Agents aren’t fully deterministic, but biased toward better options.</li>
<li>λ represents <strong>decision sharpness</strong> or <strong>confidence</strong>.</li>
</ul>
<h4 id="numerical-example">Numerical Example<a hidden class="anchor" aria-hidden="true" href="#numerical-example">#</a></h4>
<p>Suppose:</p>
<ul>
<li>$EU(\text{&ldquo;square&rdquo;}) = 1.0$</li>
<li>$EU(\text{&ldquo;green&rdquo;}) = 0.5$</li>
</ul>
<p>Then:</p>
<ul>
<li>
<p>With λ = 1:</p>
<p>$$
\exp(1 \cdot 1.0) = 2.718,\quad \exp(1 \cdot 0.5) = 1.649
$$</p>
</li>
<li>
<p>Normalize:</p>
<p>$$
\text{Total} = 2.718 + 1.649 = 4.367
$$</p>
<p>$$
P(\text{&ldquo;square&rdquo;}) = \frac{2.718}{4.367} \approx 0.622,\quad P(\text{&ldquo;green&rdquo;}) = \frac{1.649}{4.367} \approx 0.378
$$</p>
</li>
</ul>
<p>The speaker prefers &ldquo;square&rdquo; but still sometimes says &ldquo;green&rdquo;.</p>
<h4 id="summary-table-1">Summary Table<a hidden class="anchor" aria-hidden="true" href="#summary-table-1">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Concept</th>
          <th>Interpretation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>λ (lambda)</td>
          <td>Rationality or sensitivity to utility</td>
      </tr>
      <tr>
          <td>Low λ</td>
          <td>Speaker behaves more randomly</td>
      </tr>
      <tr>
          <td>High λ</td>
          <td>Speaker consistently chooses best utterance</td>
      </tr>
      <tr>
          <td>λ = 0</td>
          <td>Speaker chooses uniformly</td>
      </tr>
      <tr>
          <td>λ → ∞</td>
          <td>Speaker chooses deterministically</td>
      </tr>
      <tr>
          <td>Mathematical Function</td>
          <td>Appears inside exponential in softmax</td>
      </tr>
  </tbody>
</table>
<h4 id="final-takeaway">Final Takeaway<a hidden class="anchor" aria-hidden="true" href="#final-takeaway">#</a></h4>
<blockquote>
<p>λ gives the RSA model <strong>flexibility</strong> to represent real human speakers who are sometimes decisive and sometimes probabilistic in their choices. It controls the <strong>softness of rationality</strong> in utterance production.</p></blockquote>
<hr>
<h3 id="extra-2-softmax-function">Extra #2 Softmax Function<a hidden class="anchor" aria-hidden="true" href="#extra-2-softmax-function">#</a></h3>
<h4 id="why-is-the-softmax-function-used-in-the-pragmatic-speaker-formula">Why Is the Softmax Function Used in the Pragmatic Speaker Formula?<a hidden class="anchor" aria-hidden="true" href="#why-is-the-softmax-function-used-in-the-pragmatic-speaker-formula">#</a></h4>
<p>In the RSA model, the <strong>Pragmatic Speaker</strong> chooses among possible utterances to convey an intended referent.<br>
The model assumes the speaker is <strong>rational but probabilistic</strong>.</p>
<h4 id="the-formula-franke--jäger-2016">The Formula (Franke &amp; Jäger, 2016)<a hidden class="anchor" aria-hidden="true" href="#the-formula-franke--jäger-2016">#</a></h4>
<p>The speaker&rsquo;s production probability is defined as:</p>
<p>$$
P_{prod}(p \mid r; \lambda, f) = \frac{\exp\left( \lambda \cdot EU(r, p; f) \right)}{\sum_{p&rsquo;} \exp\left( \lambda \cdot EU(r, p&rsquo;; f) \right)}
$$</p>
<ul>
<li>$p$: property/utterance</li>
<li>$r$: intended referent</li>
<li>$\lambda$: rationality parameter</li>
<li>$f(p)$: utterance bias or cost</li>
<li>$EU(r, p; f)$: expected utility of utterance $p$ for referent $r$</li>
</ul>
<h4 id="what-is-the-softmax-function-doing-here">What Is the Softmax Function Doing Here?<a hidden class="anchor" aria-hidden="true" href="#what-is-the-softmax-function-doing-here">#</a></h4>
<p>The softmax function:</p>
<p>$$
\text{softmax}_i(x) = \frac{\exp(\lambda x_i)}{\sum_j \exp(\lambda x_j)}
$$</p>
<p>is used to <strong>turn utility scores into probabilities</strong>. This is crucial in RSA because:</p>
<h4 id="1-it-maps-utility-to-probability">1. It Maps Utility to Probability<a hidden class="anchor" aria-hidden="true" href="#1-it-maps-utility-to-probability">#</a></h4>
<ul>
<li>Higher utility → higher probability</li>
<li>But all utterances still have <strong>some chance</strong> of being chosen</li>
<li>This models <strong>graded speaker behavior</strong></li>
</ul>
<h4 id="2-it-reflects-real-human-behavior">2. It Reflects Real Human Behavior<a hidden class="anchor" aria-hidden="true" href="#2-it-reflects-real-human-behavior">#</a></h4>
<p>Humans don’t always choose the &ldquo;best&rdquo; utterance. They might:</p>
<ul>
<li>Be uncertain</li>
<li>Prefer shorter or easier words</li>
<li>Make probabilistic rather than deterministic decisions</li>
</ul>
<p>Softmax <strong>captures this variation</strong>.</p>
<h4 id="3-it-controls-rationality-via-λ">3. It Controls Rationality via λ<a hidden class="anchor" aria-hidden="true" href="#3-it-controls-rationality-via-λ">#</a></h4>
<table>
  <thead>
      <tr>
          <th>λ Value</th>
          <th>Speaker Behavior</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>λ = 0</td>
          <td>Completely random</td>
      </tr>
      <tr>
          <td>λ = 1</td>
          <td>Matches utility proportionality</td>
      </tr>
      <tr>
          <td>λ → ∞</td>
          <td>Always chooses best option</td>
      </tr>
  </tbody>
</table>
<ul>
<li>λ allows the model to simulate <strong>different levels of speaker precision</strong></li>
<li>This is sometimes called <strong>bounded rationality</strong></li>
</ul>
<h4 id="4-it-enables-listener-inference">4. It Enables Listener Inference<a hidden class="anchor" aria-hidden="true" href="#4-it-enables-listener-inference">#</a></h4>
<p>Listeners in RSA models <strong>reverse-engineer speaker choices</strong>.<br>
Softmax:</p>
<ul>
<li>Makes speaker behavior <strong>invertible</strong> using Bayes’ rule</li>
<li>Lets listeners reason: <em>“Why would the speaker have said that?”</em></li>
</ul>
<h4 id="5-its-a-standard-tool-in-decision-and-learning-models">5. It’s a Standard Tool in Decision and Learning Models<a hidden class="anchor" aria-hidden="true" href="#5-its-a-standard-tool-in-decision-and-learning-models">#</a></h4>
<ul>
<li>Used in machine learning (e.g., neural networks)</li>
<li>Used in economics (e.g., quantal response models)</li>
<li>Used in reinforcement learning (for action selection)</li>
</ul>
<p>Softmax in RSA makes the model mathematically <strong>standard, general, and interpretable</strong>.</p>
<h4 id="intuition-1">Intuition<a hidden class="anchor" aria-hidden="true" href="#intuition-1">#</a></h4>
<blockquote>
<p>Softmax lets the speaker be smart—but not rigid.<br>
Better utterances are more likely, but nothing is guaranteed.</p></blockquote>
<h4 id="summary-table-2">Summary Table<a hidden class="anchor" aria-hidden="true" href="#summary-table-2">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Feature</th>
          <th>Why Softmax Helps in RSA</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Maps utility → probability</td>
          <td>Valid probability distribution</td>
      </tr>
      <tr>
          <td>Captures human behavior</td>
          <td>Models graded, non-deterministic choices</td>
      </tr>
      <tr>
          <td>Flexible rationality</td>
          <td>λ controls how sharp or flat the distribution is</td>
      </tr>
      <tr>
          <td>Bayesian inference support</td>
          <td>Listener can reason backwards</td>
      </tr>
      <tr>
          <td>Standard modeling tool</td>
          <td>Widely used in cognitive science and AI</td>
      </tr>
  </tbody>
</table>
<h3 id="extra-3-lambda-softmax-visualizer">Extra #3 Lambda Softmax Visualizer<a hidden class="anchor" aria-hidden="true" href="#extra-3-lambda-softmax-visualizer">#</a></h3>
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Softmax Visualization</title>
  <script src="https://cdn.jsdelivr.net/npm/mathjs@11.11.0/lib/browser/math.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js" defer></script>
  <style>
    body {
      font-family: sans-serif;
      padding: 1rem;
    }
    .bar {
      height: 24px;
      margin: 4px 0;
      background: #e0e0e0;
      position: relative;
    }
    .fill {
      height: 100%;
      background: steelblue;
      color: white;
      text-align: right;
      padding-right: 8px;
      white-space: nowrap;
      overflow: hidden;
      border-radius: 4px;
    }
  </style>
</head>
<body x-data="softmaxViz()">
  <h4>Effect of Lambda (λ) on Speaker Choice</h4>
  <p>Utility of "square": <strong>1.0</strong><br>
     Utility of "green": <strong>0.5</strong></p>
<p><label for="lambda">λ (Rationality parameter): <strong x-text="lambda"></strong></label>
<input id="lambda" type="range" min="0" max="10" step="0.1" x-model.number="lambda"></p>
  <div class="bar">
    <div class="fill" :style="{ width: (pSquare * 100).toFixed(1) + '%' }">
      <span x-text="'square: ' + (pSquare * 100).toFixed(1) + '%' "></span>
    </div>
  </div>
  <div class="bar">
    <div class="fill" :style="{ width: (pGreen * 100).toFixed(1) + '%' }">
      <span x-text="'green: ' + (pGreen * 100).toFixed(1) + '%' "></span>
    </div>
  </div>
  <script>
    function softmaxViz() {
      return {
        lambda: 1,
        uSquare: 1.0,
        uGreen: 0.5,
        get pSquare() {
          const num = Math.exp(this.lambda * this.uSquare);
          const denom = num + Math.exp(this.lambda * this.uGreen);
          return num / denom;
        },
        get pGreen() {
          const num = Math.exp(this.lambda * this.uGreen);
          const denom = Math.exp(this.lambda * this.uSquare) + num;
          return num / denom;
        }
      }
    }
  </script>
</body>
</html>
<hr>
<h3 id="extra-4-bayesian-theorem">Extra 4 Bayesian Theorem<a hidden class="anchor" aria-hidden="true" href="#extra-4-bayesian-theorem">#</a></h3>
<p><strong>Theorem: A Beginner’s Guide</strong></p>
<p>Bayes’ Theorem is a fundamental concept in probability theory and statistical reasoning. It describes how to <strong>update our beliefs</strong> in light of new evidence. This principle is central to many fields, including linguistics, cognitive science, machine learning, and especially <strong>pragmatic inference</strong> in models like the Rational Speech Act (RSA) framework.</p>
<p><strong>What is Bayes’ Theorem?</strong></p>
<p>At its core, Bayes’ Theorem answers the question:</p>
<blockquote>
<p><strong>&ldquo;Given some new evidence, how should I update my belief about a hypothesis?&rdquo;</strong></p></blockquote>
<p><strong>The Formula</strong></p>
<p>$$
P(H \mid E) = \frac{P(E \mid H) \cdot P(H)}{P(E)}
$$</p>
<p><strong>What the symbols mean:</strong></p>
<ul>
<li>
<p><strong>$P(H \mid E)$</strong>: Posterior<br>
The probability of hypothesis $H$ given the new evidence $E$. <em>(What we want to know.)</em></p>
</li>
<li>
<p><strong>$P(E \mid H)$</strong>: Likelihood<br>
The probability of seeing the evidence $E$ if the hypothesis $H$ were true.</p>
</li>
<li>
<p><strong>$P(H)$</strong>: Prior<br>
The initial belief about hypothesis $H$, before seeing the new evidence.</p>
</li>
<li>
<p><strong>$P(E)$</strong>: Marginal likelihood or Evidence<br>
The total probability of the evidence under all possible hypotheses.</p>
</li>
</ul>
<p><strong>Simple Example</strong></p>
<p>Suppose:</p>
<ul>
<li>A rare disease affects <strong>1%</strong> of a population.</li>
<li>A medical test detects the disease with <strong>99% accuracy</strong> (true positive rate).</li>
<li>The false positive rate is also <strong>1%</strong>.</li>
</ul>
<p>If a person tests positive, what is the probability they actually have the disease?</p>
<p><em><strong>Step 1: Define Events</strong></em></p>
<ul>
<li>$D$: Person has the disease</li>
<li>$T$: Person tests positive</li>
</ul>
<p><em><strong>Step 2: Plug in the numbers</strong></em></p>
<ul>
<li>$P(D) = 0.01$</li>
<li>$P(\neg D) = 0.99$</li>
<li>$P(T \mid D) = 0.99$</li>
<li>$P(T \mid \neg D) = 0.01$</li>
</ul>
<p>$$
P(T) = P(T \mid D) \cdot P(D) + P(T \mid \neg D) \cdot P(\neg D)
= 0.99 \cdot 0.01 + 0.01 \cdot 0.99 = 0.0198
$$</p>
<p>$$
P(D \mid T) = \frac{0.99 \cdot 0.01}{0.0198} \approx 0.50
$$</p>
<p><strong>Conclusion</strong>:
Even after testing positive, the probability of actually having the disease is <strong>only 50%</strong>!<br>
This counterintuitive result shows the importance of prior probability in updating beliefs.</p>
<p><strong>Visual Breakdown</strong></p>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">flowchart TD
    A[Start: Prior P(H)] --&gt; B[New Evidence P(E | H)]
    B --&gt; C[Compute Joint: P(E | H) * P(H)]
    C --&gt; D[Compute P(E)]
    D --&gt; E[Posterior: P(H | E)]
</code></pre><hr>
<h3 id="sources">Sources<a hidden class="anchor" aria-hidden="true" href="#sources">#</a></h3>
<p>Degen (2023), <a href="https://www.annualreviews.org/content/journals/10.1146/annurev-linguistics-031220-010811"><em>The Rational Speech Act Framework</em></a></p>
<p>Franke, M., &amp; Jäger, G. (2016). <a href="(https://www.degruyter.com/document/doi/10.1515/zfs-2016-0002/html?lang=en&amp;srsltid=AfmBOorsU5Gn-tQd3xJ7ka_Cl8OKh5eujrjBwscNJHTEkX9JFG2WF9rR)">Probabilistic pragmatics, or why Bayes’ rule is probably important for pragmatics</a>. <em>Zeitschrift für Sprachwissenschaft</em>, 35(1), 3 - 44.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://zhangjunfelix.github.io/">Jun Zhang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
